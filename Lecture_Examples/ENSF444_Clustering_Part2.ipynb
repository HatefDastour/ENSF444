{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5a2KYZqOsihn"
   },
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mw5NQLp4OkVK"
   },
   "source": [
    "## Clustering Models: Agglomerative\n",
    "\n",
    "Agglomerative clustering is a type of hierarchical clustering that builds clusters from the bottom up. It works as follows:\n",
    "\n",
    "- Initially, each data point is assigned to its own cluster.\n",
    "- Then, the two clusters that are closest to each other are merged into one.\n",
    "- This process is repeated until the desired number of clusters is reached or another stopping criterion is met.\n",
    "\n",
    "The most common stopping criterion is the number of clusters, which can be specified by the user. Scikit-learn implements this option in its agglomerative clustering module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUEyykZqPb5J"
   },
   "source": [
    "### Linkage Criteria\n",
    "\n",
    "Linkage criteria measure the similarity between two clusters and determine which ones to merge. There are four common options:\n",
    "\n",
    "- **ward**: This is the default option. It merges the two clusters that cause the least increase in variance within all clusters. This results in clusters of similar size.\n",
    "- **average**: This option calculates the average distance between all points in two clusters and merges the ones with the smallest value.\n",
    "- **complete**: This option (also called maximum linkage) finds the maximum distance between any two points in two clusters and merges the ones with the smallest value.\n",
    "- **single**: This option (also called minimum linkage) finds the minimum distance between any two points in two clusters and merges the ones with the smallest value.\n",
    "\n",
    "Ward works well for most datasets, but the other options might be better if the clusters have unequal sizes. For example, if one cluster is much larger than the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mr4EM_xng4Hp"
   },
   "source": [
    "Some of the hyperparameters for AgglomerativeClustering from the sklearn:\n",
    "\n",
    "- **n_clusters**: The number of clusters to find. It must be an integer or None (if `distance_threshold` is not None).\n",
    "\n",
    "- **metric**: The metric used to compute the linkage. Can be \"euclidean\", \"l1\", \"l2\", \"manhattan\", \"cosine\", or \"precomputed\". If the linkage is \"ward\", only \"euclidean\" is accepted.\n",
    "\n",
    "- **memory**: Used to cache the output of the computation of the tree. It can be a path to the caching directory or an object with the joblib.Memory interface.\n",
    "\n",
    "- **connectivity**: Defines the neighboring samples following a given structure of the data. It can be a connectivity matrix or a callable that transforms the data into a connectivity matrix.\n",
    "\n",
    "- **compute_full_tree**: Whether to stop early the construction of the tree at `n_clusters` or not. It's useful to decrease computation time and must be True if `distance_threshold` is not None.\n",
    "\n",
    "- **linkage**: The linkage criterion to use, which determines the distance between sets of observations. Options are \"ward\", \"complete\", \"average\", or \"single\".\n",
    "\n",
    "- **distance_threshold**: The linkage distance threshold above which clusters will not be merged. If not None, `n_clusters` must be None, and `compute_full_tree` must be True].\n",
    "\n",
    "- **compute_distances**: Whether to compute distances between clusters for dendrogram visualization, even if `distance_threshold` is not used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NcbpUWZ_j72N"
   },
   "source": [
    "### Implementation Notes\n",
    "\n",
    "- Agglomerative clustering does not have a predict method.\n",
    "- You need to use the fit_predict method to assign cluster labels to new data.\n",
    "- You can visualize the cluster hierarchy with a dendrogram and choose the optimal number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpomk_5kgxDL"
   },
   "source": [
    "<font color='Blue'><b>Example:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cS_fxobOoDAr"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use a custom style from a specified URL\n",
    "plt.style.use('https://raw.githubusercontent.com/HatefDastour/ENSF444/main/Files/mystyle.mplstyle')\n",
    "\n",
    "# Generate synthetic two-dimensional data\n",
    "X, y = make_blobs(random_state = 2)\n",
    "\n",
    "df = pd.DataFrame(X, columns=['Feature 1', 'Feature 2'])\n",
    "display(df)\n",
    "\n",
    "# Create subplots for original data and clustered data\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "\n",
    "ax.scatter(df['Feature 1'], df['Feature 2'],\n",
    "           c='gray', edgecolor='k', linewidth=1, alpha=0.7, s=40, zorder = 2)\n",
    "ax.set(xlabel='Feature 1', ylabel='Feature 2',\n",
    "        xlim = [-6, 8], ylim = [-12, 2], aspect = 'equal')\n",
    "ax.grid(True)\n",
    "# Adjust layout for better presentation\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BmWfgmq_1ydO"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.metrics import silhouette_score\n",
    "import seaborn as sns\n",
    "\n",
    "n_clusters = 3\n",
    "# Perform Agglomerative Clustering\n",
    "clustering = AgglomerativeClustering(n_clusters=n_clusters,\n",
    "                                      linkage='ward')\n",
    "clustering.fit(df)\n",
    "\n",
    "# Define a custom color map for the clusters\n",
    "colors = [\"#f44336\", \"#4e9130\", \"#2986cc\"]\n",
    "cmap = ListedColormap(colors)\n",
    "\n",
    "# Create subplots for original data and clustered data\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 6))\n",
    "\n",
    "# Scatter plot for the original data\n",
    "axs[0].scatter(df['Feature 1'], df['Feature 2'], c='gray', edgecolor='k',\n",
    "                linewidth=1, alpha=0.7, s=40, zorder = 2)\n",
    "axs[0].set_title('Original Data', fontsize=14)\n",
    "\n",
    "# Scatter plot for the clustered data\n",
    "sns.scatterplot(x = df['Feature 1'], y = df['Feature 2'], ax = axs[1],\n",
    "                hue=clustering.labels_, palette=colors,\n",
    "                edgecolor='k', linewidth=0.5, alpha=0.7, s=50)\n",
    "axs[1].legend(loc='best', title = 'Cluster')\n",
    "\n",
    "axs[1].set_title(f'AgglomerativeClustering\\n(n_clusters = {n_clusters}, linkage = ward)', fontsize=14)\n",
    "\n",
    "axs[1].text(0.52, 0.04, f'Silhouette = {silhouette_score(df, clustering.labels_):.3f}',\n",
    "            transform=axs[1].transAxes, fontsize=12, weight='bold',\n",
    "            bbox=dict(facecolor='Whitesmoke', alpha=0.7))\n",
    "\n",
    "# Set labels for axes\n",
    "for ax in axs:\n",
    "    ax.set(xlabel='Feature 1', ylabel='Feature 2',\n",
    "            xlim = [-5, 7.5], ylim = [-12, 2], aspect = 'equal')\n",
    "    ax.grid(True)\n",
    "\n",
    "# Adjust layout for better presentation\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tPVJSocSm6RM"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def visualize_agglomerative_clustering(df_inp, n_clusters, linkages=['ward', 'complete', 'average', 'single']):\n",
    "    \"\"\"\n",
    "    Visualizes Agglomerative Clustering results with different linkages on a 2x2 subplot.\n",
    "\n",
    "    Parameters:\n",
    "    - df_inp: DataFrame, input data with features.\n",
    "    - n_clusters: int, number of clusters for Agglomerative Clustering.\n",
    "    - linkages: list of str, linkage criteria for Agglomerative Clustering.\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    # Define a custom color map for the clusters\n",
    "    colors = [\"#f44336\", \"#4e9130\", \"#2986cc\"]\n",
    "    cmap = ListedColormap(colors)\n",
    "\n",
    "    for i, linkage in enumerate(linkages):\n",
    "        # Perform Agglomerative Clustering\n",
    "        clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage=linkage)\n",
    "        clustering.fit(df_inp)\n",
    "\n",
    "        # Scatter plot for the clustered data\n",
    "        sns.scatterplot(x = df_inp['Feature 1'],\n",
    "                        y = df_inp['Feature 2'],\n",
    "                        ax = axs[i],\n",
    "                        hue=clustering.labels_, palette=colors,\n",
    "                        edgecolor='k', linewidth=0.5, alpha=0.7, s=50)\n",
    "        axs[i].legend(loc='best', title = 'Cluster')\n",
    "        axs[i].set_title(f'Linkage = {linkage}', fontsize=14)\n",
    "        axs[i].set(xlabel='Feature 1', ylabel='Feature 2',\n",
    "                   xlim = [-5, 7.5], ylim = [-12, 2], aspect = 'equal')\n",
    "        axs[i].text(0.48, 0.04,\n",
    "                    f'Silhouette = {silhouette_score(df, clustering.labels_):.3f}',\n",
    "                    transform=axs[i].transAxes, fontsize=12, weight='bold',\n",
    "                    bbox=dict(facecolor='Whitesmoke', alpha=0.7))\n",
    "\n",
    "    # Adjust layout for better presentation\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Example usage:\n",
    "visualize_agglomerative_clustering(df, n_clusters=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lU0V_nD0vMnO"
   },
   "source": [
    "## Dendrogram to visualize cluster formation\n",
    "\n",
    "A dendrogram is a tree-like diagram that shows how data points are grouped into clusters based on their similarity or distance. For example, suppose we have a synthetic two-dimensional data set with 15 points, as shown in the left plot below. We can apply agglomerative clustering, which is a bottom-up approach that starts with each point as a single cluster and then merges the closest clusters until a desired number of clusters is reached. The ward linkage method is one way to measure the distance between clusters, which minimizes the variance within each cluster and tends to produce compact and spherical clusters. The right plot below shows the result of agglomerative clustering with three clusters, indicated by different colors. We can also use the silhouette score to evaluate how well each point fits its cluster, which ranges from -1 to 1. A higher silhouette score means a better clustering. The silhouette score for this example is 0.659, which is relatively high.\n",
    "\n",
    "The dendrogram can help to visualize the hierarchical structure of the data and to determine the optimal number of clusters. The dendrogram below shows how the clusters are formed and the distance between them. Each node represents a cluster, and the height of the node indicates the distance between the clusters that are merged. The lower the node, the more similar the clusters are. We can also use horizontal lines to cut the dendrogram at different levels and obtain different numbers of clusters. For example, if we cut the dendrogram at the dashed line near the top, we get two clusters. If we cut the dendrogram at the dashed line near the bottom, we get three clusters. We can also add annotations to indicate the number of clusters at each level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EryFvgigvNdy",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.metrics import silhouette_score\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs\n",
    "from scipy.cluster import hierarchy\n",
    "\n",
    "# Generate synthetic two-dimensional data\n",
    "X, y = make_blobs(random_state=0, n_samples=15)\n",
    "df = pd.DataFrame(X, columns=['Feature 1', 'Feature 2'])\n",
    "display(df)\n",
    "\n",
    "n_clusters = 3\n",
    "\n",
    "# Perform Agglomerative Clustering\n",
    "clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward').fit(df)\n",
    "\n",
    "# Define a custom color map for the clusters\n",
    "colors = [\"#FF7C4C\", \"#4e9130\", \"#c90076\"]\n",
    "\n",
    "# Create subplots for original data and clustered data\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Scatter plot for the original data\n",
    "axs[0].scatter(df['Feature 1'], df['Feature 2'], c='gray', edgecolor='k', linewidth=1, alpha=0.7, s=40, zorder=2)\n",
    "axs[0].set_title('Original Data', fontsize=14)\n",
    "\n",
    "# Scatter plot for the clustered data\n",
    "sns.scatterplot(x=df['Feature 1'], y=df['Feature 2'], ax=axs[1], hue=clustering.labels_, palette=colors,\n",
    "                edgecolor='k', linewidth=0.5, alpha=0.7, s=50)\n",
    "axs[1].set_title(f'Agglomerative Clustering\\n(n_clusters = {n_clusters}, linkage = ward)', fontsize=14)\n",
    "\n",
    "# Annotate points with their index\n",
    "for i, row in df.iterrows():\n",
    "    axs[1].text(row['Feature 1'], row['Feature 2'], str(i),\n",
    "               fontdict={'weight': 'bold', 'size': 12})\n",
    "\n",
    "axs[1].text(0.53, 0.04, f'Silhouette = {silhouette_score(df, clustering.labels_):.3f}',\n",
    "            transform=axs[1].transAxes, fontsize=12, weight='bold', bbox=dict(facecolor='Whitesmoke', alpha=0.7))\n",
    "axs[1].legend(loc='best', title = 'Cluster')\n",
    "\n",
    "# Set labels for axes\n",
    "for ax in axs:\n",
    "    ax.set(xlabel='Feature 1', ylabel='Feature 2', xlim=[-4, 6], ylim=[-2, 8], aspect='equal')\n",
    "    ax.grid(True)\n",
    "\n",
    "# Adjust layout for better presentation\n",
    "plt.tight_layout()\n",
    "\n",
    "# Dendrogram plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "ax.grid(False)\n",
    "z = hierarchy.complete(df)\n",
    "hierarchy.set_link_color_palette([\"#4e9130\", \"#FF7C4C\", \"#c90076\"])\n",
    "_ = hierarchy.dendrogram(z, ax=ax, count_sort=True, distance_sort=True)\n",
    "\n",
    "# Plot horizontal lines with cluster annotations\n",
    "bounds = ax.get_xbound()\n",
    "for hline, annotation in zip([6.5, 4], [' Two Clusters', ' Three Clusters']):\n",
    "    ax.plot(bounds, [hline] * 2, '--', lw=2, c='k')\n",
    "    ax.text(bounds[1], hline, annotation, va='center', fontdict={'size': 15})\n",
    "\n",
    "ax.set(xlabel=\"Sample index\", ylabel=\"Cluster Distance\")\n",
    "\n",
    "# Adjust layout for better presentation\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3wJFgt7wE_Uc"
   },
   "source": [
    "## Clustering Models: DBSCAN\n",
    "\n",
    "DBSCAN is a clustering algorithm that stands for **D**ensity-**B**ased **S**patial **C**lustering of **A**pplications with **N**oise. It has the following advantages:\n",
    "\n",
    "- It does not require specifying the number of clusters in advance\n",
    "- It can discover clusters with arbitrary shapes\n",
    "- It can detect outliers or noise points that do not belong to any cluster\n",
    "\n",
    "DBSCAN is based on the concept of density in the feature space, which measures how close data points are to each other. DBSCAN identifies clusters as regions of high density, separated by regions of low density. DBSCAN is slower than agglomerative and k-means algorithms, but it can handle larger datasets and more complex data structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ej1ZznhLFpuD"
   },
   "source": [
    "### DBSCAN Hyperparameters\n",
    "\n",
    "**DBSCAN** is a clustering algorithm that groups data points based on their **density**. It requires the user to specify two hyperparameters that control the density of a region:\n",
    "\n",
    "- **min_samples**: This is the minimum number of data points that must be within a certain distance to form a dense region. A data point is considered a **core sample** (or core point) if it has at least min_samples neighbors, including itself, within a given radius.\n",
    "- **eps**: This is the radius that defines the distance between data points. Two data points are considered neighbors if they are within eps distance of each other. DBSCAN assigns the same cluster label to core samples that are neighbors, and to any other data point that is reachable from a core sample by a chain of neighbors.\n",
    "\n",
    "DBSCAN identifies core samples as data points that belong to a dense region, and clusters them based on their proximity. The choice of min_samples and eps can affect the number and size of the clusters that DBSCAN finds.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://cdn-images-1.medium.com/v2/resize:fit:1600/1*tc8UF-h0nQqUfLC8-0uInQ.gif\" alt=\"picture\" width=\"700\">\n",
    "<br>\n",
    "<b>Figure</b>: An example of using DBSCAN. Image source: https://www.digitalvidya.com/blog/the-top-5-clustering-algorithms-data-scientists-should-know/\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxZN1PqZNHKo"
   },
   "source": [
    "### DBSCAN Methodology\n",
    "\n",
    "1. It randomly selects a data point that has not been visited yet.\n",
    "1. It checks how many data points are within a distance of `eps` from the selected point. This is called the **neighborhood** of the point.\n",
    "1. If the neighborhood has fewer than `min_samples` data points, the selected point is marked as **noise**, meaning it does not belong to any cluster.\n",
    "1. If the neighborhood has at least `min_samples` data points, the selected point is marked as a **core points** and assigned a new cluster label. Then, the algorithm visits all the data points in the neighborhood and assigns them the same cluster label. If any of these data points are also core points, their neighborhoods are visited and labeled as well, and so on. This process continues until all the data points that are reachable from the selected point by a chain of neighbors are clustered.\n",
    "1. The algorithm repeats the above steps until all the data points have been visited and labeled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7XbOTjvyUGjV"
   },
   "source": [
    "### Types of Points\n",
    "\n",
    "DBSCAN distinguishes between three types of data points based on their density and distance:\n",
    "\n",
    "- **Core points** are data points that have at least `min_samples` data points within a distance of `eps` from them. They are the main drivers of the clustering process.\n",
    "\n",
    "- **Boundary points** are data points that are within a distance of `eps` from a core point, but have fewer than `min_samples` data points in their neighborhood. They are assigned to the same cluster as the nearest core point, but they do not expand the cluster.\n",
    "\n",
    "- **Noise points** are data points that are neither core nor boundary points. They are considered outliers and do not belong to any cluster.\n",
    "\n",
    "The clustering of the core points is **deterministic**, meaning that it does not depend on the order of the data points or the random selection of the starting point. The same core points will always be clustered together, and the same points will always be labeled as noise, regardless of how many times the DBSCAN algorithm is run on the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QlCpXSlKVBKN"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "# Generate synthetic two-dimensional data\n",
    "X, _ = make_blobs(n_samples= 100, n_features=2,\n",
    "                  centers = 4,\n",
    "                  cluster_std= [0.8, 0.5, 0.8, 0.6],\n",
    "                  random_state=0)\n",
    "\n",
    "df = pd.DataFrame(X[:, ::-1], columns=['Feature 1', 'Feature 2'])\n",
    "display(df)\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 9))\n",
    "axs = axs.flatten()\n",
    "\n",
    "sns.scatterplot(x = df['Feature 1'], y = df['Feature 2'], ax = axs[0],\n",
    "                color = 'gray',\n",
    "                edgecolor='k', linewidth=0.5, alpha=0.7, s=50)\n",
    "axs[0].set_title('Original Data', fontsize=14)\n",
    "\n",
    "for i, min_samples in enumerate([3, 4, 5], 1):\n",
    "    clustering = DBSCAN(eps = 0.5, min_samples = min_samples)\n",
    "    clustering.fit(df)\n",
    "    sns.scatterplot(x = df['Feature 1'], y = df['Feature 2'], ax = axs[i],\n",
    "                    hue=clustering.labels_, palette= 'Set1',\n",
    "                    edgecolor='k', linewidth=0.5, alpha=0.7, s=50)\n",
    "    axs[i].legend(loc='best', title = 'Cluster')\n",
    "    axs[i].set_title(f'min_samples = {min_samples}', fontsize=14)\n",
    "    axs[i].text(0.52, 0.04,\n",
    "                f'Silhouette = {silhouette_score(df, clustering.labels_):.3f}',\n",
    "                transform=axs[i].transAxes, fontsize=12, weight='bold',\n",
    "                bbox=dict(facecolor='Whitesmoke', alpha=0.7))\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set(xlabel='Feature 1', ylabel='Feature 2',\n",
    "           ylim = [-5, 5], xlim = [-1, 10],\n",
    "           aspect = 'equal')\n",
    "# Adjust layout for better presentation\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9o97U4Boe-0E"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 9))\n",
    "axs = axs.flatten()\n",
    "\n",
    "sns.scatterplot(x = df['Feature 1'], y = df['Feature 2'], ax = axs[0],\n",
    "                color = 'gray',\n",
    "                edgecolor='k', linewidth=0.5, alpha=0.7, s=50)\n",
    "axs[0].set_title('Original Data', fontsize=14)\n",
    "\n",
    "for i, eps in enumerate([0.3, 0.7, 1], 1):\n",
    "    clustering = DBSCAN(eps = eps, min_samples = 5)\n",
    "    clustering.fit(df)\n",
    "\n",
    "    sns.scatterplot(x = df['Feature 1'], y = df['Feature 2'], ax = axs[i],\n",
    "                    hue=clustering.labels_, palette= 'Set1',\n",
    "                    edgecolor='k', linewidth=0.5, alpha=0.7, s=50)\n",
    "    axs[i].legend(loc='best', title = 'Cluster')\n",
    "    axs[i].set_title(f'eps = {eps:.1f}', fontsize=14)\n",
    "    axs[i].text(0.52, 0.04,\n",
    "                f'Silhouette = {silhouette_score(df, clustering.labels_):.3f}',\n",
    "                transform=axs[i].transAxes, fontsize=12, weight='bold',\n",
    "                bbox=dict(facecolor='Whitesmoke', alpha=0.7))\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set(xlabel='Feature 1', ylabel='Feature 2',\n",
    "           ylim = [-5, 5], xlim = [-1, 10],\n",
    "           aspect = 'equal')\n",
    "# Adjust layout for better presentation\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmYfqgVm962N"
   },
   "source": [
    "## Clustering Models: Gaussian Mixture Models (GMM)\n",
    "\n",
    "**K-means clustering** is a popular method due to its simplicity and efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 970
    },
    "id": "EV72tuTk959p",
    "outputId": "86fb27d5-2e10-4439-8d84-1a78642ba01d"
   },
   "outputs": [],
   "source": [
    "#@title K-Means Example\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Use a custom style from a specified URL\n",
    "plt.style.use('https://raw.githubusercontent.com/HatefDastour/ENSF444/main/Files/mystyle.mplstyle')\n",
    "\n",
    "# Generate data\n",
    "X, y_true = make_blobs(n_samples=400, centers=3, cluster_std=0.60, random_state=0)\n",
    "X = X[:, ::-1]\n",
    "\n",
    "df = pd.DataFrame(X, columns=['Feature 1', 'Feature 2'])\n",
    "display(df)\n",
    "\n",
    "# Set colors for clusters\n",
    "colors = [\"#f44336\", \"#4e9130\", \"#2986cc\"]\n",
    "cmap = ListedColormap(colors)\n",
    "\n",
    "def plot_circles(kmeans, X, ax, colors, alpha=0.2):\n",
    "    \"\"\"\n",
    "    Plot circles representing clusters determined by k-means algorithm.\n",
    "\n",
    "    Parameters:\n",
    "    - kmeans: KMeans object\n",
    "        Instance of KMeans clustering algorithm.\n",
    "    - X: array-like or pd.DataFrame\n",
    "        Input data used for clustering.\n",
    "    - ax: matplotlib.axes._axes.Axes\n",
    "        Axes on which circles will be plotted.\n",
    "    - colors: list of str\n",
    "        List of colors for the clusters.\n",
    "    - alpha: float, optional (default=0.2)\n",
    "        Opacity of the circles.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    labels = kmeans.labels_\n",
    "    centers = kmeans.cluster_centers_\n",
    "    radii = [cdist(X[labels == i], [center]).max() for i, center in enumerate(centers)]\n",
    "    for c, r, color in zip(centers, radii, colors):\n",
    "        ax.add_patch(plt.Circle(c, r, fc=color,\n",
    "                                edgecolor=color, lw=3,\n",
    "                                alpha=alpha, zorder=1))\n",
    "\n",
    "\n",
    "# Initialize KMeans model with desired number of clusters\n",
    "kmeans = KMeans(n_clusters=3, random_state=0, n_init=10)\n",
    "kmeans.fit(X)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "sns.scatterplot(x=df['Feature 1'], y=df['Feature 2'], ax=ax,\n",
    "                hue=kmeans.labels_, palette=colors,\n",
    "                edgecolor='k', linewidth=0.5, alpha=0.7, s=50)\n",
    "plot_circles(kmeans, X, ax, colors, alpha=0.1)\n",
    "\n",
    "ax.set(xlabel='Feature 1', ylabel='Feature 2', title='K-Means (K = 3)',\n",
    "       xlim=[-2, 7], ylim=[-4, 4], aspect='equal')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UWNALGho_M4t"
   },
   "source": [
    " However, it struggles with complex cluster shapes and structures because it assigns points to the nearest cluster center, leading to spherical clusters of equal size. This limitation becomes apparent with data that exhibit elongated, overlapping, or nested clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ukpfKd3MP5VO",
    "outputId": "693d5a5f-33ad-4db0-c19a-0e845c045ace"
   },
   "outputs": [],
   "source": [
    "#@title K-Means Example (elongated)\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.RandomState(13)\n",
    "X_stretched = np.dot(X, rng.randn(2, 2))\n",
    "\n",
    "df = pd.DataFrame(X_stretched, columns=['Feature 1', 'Feature 2'])\n",
    "display(df)\n",
    "\n",
    "# Set colors for clusters\n",
    "colors = [\"#f44336\", \"#4e9130\", \"#2986cc\"]\n",
    "cmap = ListedColormap(colors)\n",
    "\n",
    "# Initialize KMeans model with desired number of clusters\n",
    "kmeans = KMeans(n_clusters=3, random_state=0, n_init=10)\n",
    "kmeans.fit(X_stretched)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "sns.scatterplot(x=df['Feature 1'], y=df['Feature 2'], ax=ax,\n",
    "                hue=kmeans.labels_, palette=colors,\n",
    "                edgecolor='k', linewidth=0.5, alpha=0.7, s=50)\n",
    "plot_circles(kmeans, X_stretched, ax, colors, alpha=0.1)\n",
    "\n",
    "ax.set(xlabel='Feature 1', ylabel='Feature 2', title='K-Means (K = 3)',\n",
    "       xlim = [-5, 1], ylim = [-1, 6],\n",
    "       aspect='equal')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDBfRBIV_OM0"
   },
   "source": [
    "In contrast, **Gaussian Mixture Models (GMMs)** offer a more sophisticated approach. They are adept at modeling clusters with diverse shapes, sizes, and orientations, making them a superior alternative to K-means for complex datasets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "gxoZoFiLAM0k"
   },
   "source": [
    "### How GMM Works:\n",
    "\n",
    "- GMMs posit that data points are generated from a combination of multiple Gaussian distributions (aka normal distributions), each characterized by its mean, covariance, and mixture weight.\n",
    "\n",
    "- The objective of GMMs is to infer these parameters and categorize each data point into the Gaussian distribution it most likely belongs to.\n",
    "\n",
    "- Additionally, GMMs can quantify the uncertainty of cluster assignments by calculating the posterior probability for each Gaussian component given the data. This feature is particularly useful for pinpointing ambiguous data points or outliers.\n",
    "\n",
    "\n",
    "In a Gaussian Mixture Model, posterior probability tells us how likely a data point is to belong to each group, updated by the model's evidence. It's the model's best guess of a point's group based on all the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2yXseDW-LcAR"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "### Understanding GMM Cluster Shapes:\n",
    "\n",
    "In Gaussian Mixture Models (GMMs), the shape of each cluster is determined by its covariance matrix, which captures the spread and directionality of data points. Scikit-learn's `covariance_type` parameter offers four options, each affecting cluster geometry differently:\n",
    "\n",
    "- **`\"spherical\"`**: This option assumes that clusters are spherical, with equal variance in all directions—similar to the clusters formed by K-means. However, unlike K-means, GMMs with spherical covariance can have clusters of different sizes due to varying component weights.\n",
    "\n",
    "- **`\"diag\"`**: Here, clusters can stretch along the feature axes since each feature has its own variance, but features are considered uncorrelated. This results in elongated, axis-aligned clusters.\n",
    "\n",
    "- **`\"tied\"`**: With a tied covariance matrix, all clusters share the same shape and orientation but can differ in size and position. This leads to elliptical clusters that have the same orientation across the feature space.\n",
    "\n",
    "- **`\"full\"`**: The most flexible option, where each cluster has its own covariance matrix, allowing for elliptical clusters of any orientation and shape. This flexibility comes at the cost of increased computational complexity due to the larger number of parameters to estimate.\n",
    "\n",
    "Each `covariance_type` influences the flexibility and constraints of cluster shapes, with implications for model complexity and the nature of the data being modeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lp4_z4bSnqMQ"
   },
   "outputs": [],
   "source": [
    "from matplotlib.patches import Ellipse\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Set colors for clusters\n",
    "colors = [\"#f44336\", \"#4e9130\", \"#2986cc\"]\n",
    "cmap = ListedColormap(colors)\n",
    "\n",
    "def draw_ellipse(position, covariance, ax=None, **kwargs):\n",
    "    \"\"\"Draw an ellipse with a given position and covariance\"\"\"\n",
    "    ax = ax or plt.gca()\n",
    "\n",
    "    # Convert covariance to principal axes\n",
    "    if covariance.shape == (2, 2):\n",
    "        U, s, Vt = np.linalg.svd(covariance)\n",
    "        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n",
    "        width, height = 2 * np.sqrt(s)\n",
    "    else:\n",
    "        angle = 0\n",
    "        width, height = 2 * np.sqrt(covariance)\n",
    "\n",
    "    # Draw the ellipse with angle as a keyword parameter\n",
    "    for nsig in range(1, 4):\n",
    "        ax.add_patch(Ellipse(position, nsig * width, nsig * height, angle=angle,\n",
    "                             fc='#999999', zorder=1,\n",
    "                             **kwargs))\n",
    "def plot_gmm(gmm, X, label=True, ax=None, cmap=cmap):\n",
    "    ax = ax or plt.gca()\n",
    "    labels = gmm.fit(X).predict(X)\n",
    "\n",
    "    # Scatter plot with or without labels\n",
    "    if label:\n",
    "        ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap=cmap, zorder=2)\n",
    "    else:\n",
    "        ax.scatter(X[:, 0], X[:, 1], s=40, zorder=2)\n",
    "\n",
    "    ax.axis('equal')\n",
    "\n",
    "    # Set the factor for ellipse alpha\n",
    "    w_factor = 0.2 / gmm.weights_.max()\n",
    "\n",
    "    # Plot ellipses for each Gaussian component\n",
    "    for pos, covar, w in zip(gmm.means_, gmm.covariances_, gmm.weights_):\n",
    "        draw_ellipse(pos, covar, alpha=w * w_factor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "h8BbVOhKnrkX",
    "outputId": "8ffcaa85-2147-48bc-a467-368e856f9b73"
   },
   "outputs": [],
   "source": [
    "#@title GMM (Diag)\n",
    "cov_type = 'diag'\n",
    "gmm = GaussianMixture(n_components=3, covariance_type = cov_type, random_state=0)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "plot_gmm(gmm, X_stretched, ax = ax)\n",
    "ax.set(xlabel='Feature 1', ylabel='Feature 2', title = f'GMM (covariance_type = {cov_type})',\n",
    "        xlim = [-5, 1], ylim = [-1, 6],\n",
    "       aspect = 'equal')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "tWcF2y2Bn0OQ",
    "outputId": "96d4d4b2-2580-47ad-a3d0-05ec62d0855d"
   },
   "outputs": [],
   "source": [
    "#@title GMM (full)\n",
    "cov_type = 'full'\n",
    "gmm = GaussianMixture(n_components=3, covariance_type = cov_type, random_state=0)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "plot_gmm(gmm, X_stretched, ax = ax)\n",
    "ax.set(xlabel='Feature 1', ylabel='Feature 2', title = f'GMM (covariance_type = {cov_type})',\n",
    "        xlim = [-5, 1], ylim = [-1, 6],\n",
    "       aspect = 'equal')\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
