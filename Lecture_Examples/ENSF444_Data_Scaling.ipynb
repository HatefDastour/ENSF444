{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing Methods"
      ],
      "metadata": {
        "id": "V3H--6Y2Qdty"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## StandardScaler\n",
        "Standard scaler is a technique for transforming numerical data to have a mean of zero and a standard deviation of one. It is useful for machine learning algorithms that perform better when the input variables are scaled to a standard range.\n",
        "\n",
        "The formula for standard scaling is:\n",
        "\n",
        "$$z = \\frac{x - \\mu}{\\sigma}$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $z$ is the scaled data.\n",
        "- $x$ is the original data.\n",
        "- $\\mu$ is the mean of the data.\n",
        "- $\\sigma$ is the standard deviation of the data."
      ],
      "metadata": {
        "id": "8WLZTgmXQE7A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='Blue'><b>Example:</b></font> This following example demonstrates how to use the `StandardScaler` from `sklearn.preprocessing` to standardize features of a dataset by removing the mean and scaling to unit variance. The original and scaled data are displayed for comparison. The purpose is to prepare the data for machine learning algorithms that perform better with standardized input features."
      ],
      "metadata": {
        "id": "QKwrDuHO_uC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the StandardScaler class\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Create some sample data\n",
        "data = {\n",
        "    \"x1\": [1, 2, 3, 4],\n",
        "    \"x2\": [5, 6, 7, 8]\n",
        "}\n",
        "\n",
        "# Create a DataFrame from the data\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Print the original DataFrame\n",
        "print(\"Original DataFrame:\")\n",
        "display(df)\n",
        "\n",
        "print('\\nMean and Standard Deviation:')\n",
        "df_temp = pd.concat([df.mean(axis = 0).to_frame('Mean'), df.std(axis = 0).to_frame('STD')], axis = 1)\n",
        "display(df_temp)\n",
        "\n",
        "# Initialize the scaler object\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the DataFrame\n",
        "df_scaled = scaler.fit_transform(df)\n",
        "\n",
        "# Convert the scaled array to a DataFrame\n",
        "df_scaled = pd.DataFrame(df_scaled, columns=[\"x1\", \"x2\"])\n",
        "\n",
        "# Print the scaled DataFrame\n",
        "print(\"Scaled DataFrame:\")\n",
        "display(df_scaled)"
      ],
      "metadata": {
        "id": "N4vlWZnRQGUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that the scaled data has a mean of zero and a standard deviation of one for each feature. This makes the data more suitable for some machine learning algorithms, such as linear regression, k-nearest neighbors, and neural networks."
      ],
      "metadata": {
        "id": "LA8M4EQUNuJk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kernel Density Estimation (KDE) plot\n",
        "\n",
        "A **Kernel Density Estimation (KDE) plot** is a method for visualizing the distribution of observations in a dataset. It's used to estimate the probability density function of continuous or non-parametric data. In other words, it shows the probability density at different values in a continuous variable.\n",
        "\n",
        "A **univariate KDE plot** represents the probability distribution of a single variable. The area under the plotted curve represents the probability distribution of the data values.\n",
        "\n",
        "A univariate KDE plot is a way to visualize this data. It's like a smooth, continuous version of a histogram. Instead of bars, you get a curve (or a 'hill') that shows where most of the data points (or 'ages') fall. The higher the hill, the more data points in that area.\n",
        "\n",
        "The total area under a **Kernel Density Estimation (KDE) plot** is 1. This is because a KDE plot represents a probability distribution, and the total probability of all outcomes in a probability distribution always adds up to 1. This property makes KDE plots a useful tool for visualizing data distributions."
      ],
      "metadata": {
        "id": "bP8bJN4yWIjt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**FYI Only:**\n",
        "\n",
        "A PDF, denoted as $f(x)$, describes the likelihood of a continuous random variable $X$ taking on a certain value $x$. However, for a continuous random variable, the probability that $X$ takes on any exact value is essentially zero because there are infinite possible values it can take. Therefore, we use the PDF to calculate the probability that $X$ falls within a certain range of values. This is done by integrating the PDF over that range.\n",
        "\n",
        "Mathematically, the probability that $X$ lies in the interval $[a, b]$ is given by the integral of the PDF from $a$ to $b$, i.e.,\n",
        "\n",
        "\\begin{equation}P(a \\leq X \\leq b) = \\int_{a}^{b} f(x) dx\\end{equation}\n",
        "\n",
        "This integral represents the area under the curve of the PDF from $a$ to $b$, which gives the probability that $X$ takes a value in this range.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "d2pWkRiK2JJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "plt.style.use('https://raw.githubusercontent.com/HatefDastour/ENSF444/main/Files/mystyle.mplstyle')\n",
        "# Set the seed for the random number generator\n",
        "np.random.seed(0)\n",
        "\n",
        "# Let's say we have ages of 300 people\n",
        "ages = np.random.randint(1, 80, 300)\n",
        "\n",
        "# Create a new figure\n",
        "plt.figure(figsize=(6, 5))\n",
        "\n",
        "# Plot the histogram with KDE\n",
        "sns.histplot(ages, stat = 'density', kde=True, color='ForestGreen', alpha =0.3)\n",
        "\n",
        "# Plot the KDE separately with a wider range\n",
        "sns.kdeplot(ages, color='red', lw = 3)\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "VlQX7Bcl1dXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This plot gives you a 'smoothed' view of where the ages fall, showing you the distribution of ages in your data. The higher the curve at any point, the more people of that age you have in your data.\n",
        "\n",
        "\n",
        "The KDE plot in this example might appear to stretch from -20 to 100. This is because the KDE plot uses a technique called **kernel smoothing**.\n",
        "\n",
        "In kernel smoothing, each data point is replaced with a curve (or 'kernel'). The curves of nearby data points overlap, and where they overlap the most is where you see the 'hills' in your KDE plot.\n",
        "\n",
        "However, these curves can also extend beyond the range of your data, which is why you see the plot stretching from -20 to 100, even though your data only ranges from 1 to 80."
      ],
      "metadata": {
        "id": "SWKtu6t7W4cv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Boxplots"
      ],
      "metadata": {
        "id": "ZV89Nhah4QSl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A box plot, also referred to as a box-and-whisker diagram, is a graphical tool that visually depicts the distribution of a dataset. The components of a box plot include:\n",
        "\n",
        "1. **Minimum Score**: This is the smallest value in the dataset, excluding outliers, and is represented by the left whisker's end.\n",
        "2. **Lower Quartile ($Q_1$)**: This is the value below which 25% of the data falls, also known as the first quartile.\n",
        "3. **Median ($Q_2$)**: This is the middle value of the dataset, also referred to as the second quartile. Half of the data points are equal to or greater than this value, and the other half are less.\n",
        "4. **Upper Quartile ($Q3$)**: This is the value below which 75% of the data falls, also known as the third quartile. Consequently, 25% of the data is above this value.\n",
        "5. **Maximum Score**: This is the largest value in the dataset, excluding outliers, and is represented by the right whisker's end.\n",
        "6. **Whiskers**: These are the lines extending from the box indicating variability outside the upper and lower quartiles, hence they capture the data within the interquartile range.\n",
        "7. **Interquartile Range (IQR)**: This is the range between the first quartile (Q1) and the third quartile (Q3), and is represented by the box:\n",
        "\n",
        "$$IQR = Q_3 - Q1$$\n",
        "\n",
        "Outliers, which are data points that deviate significantly from the overall distribution pattern, are also depicted in a box plot. They are typically shown as individual points outside the whiskers. A data point is considered an outlier if it satisfies one of the following conditions:\n",
        "\n",
        "- The data point is less than $Q_1 - 1.5 \\times IQR$\n",
        "- The data point is greater than $Q_3 + 1.5 \\times IQR$\n",
        "\n",
        "These outliers are typically marked with a small dot outside the whiskers' range in the box plot. When outliers are present, the \"minimum\" and \"maximum\" values in the box plot are simply set to $Q_1 - 1.5 \\times IQR$ and $Q_3 + 1.5 \\times IQR$, respectively."
      ],
      "metadata": {
        "id": "kADuKZCt4A4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the seed for the random number generator\n",
        "np.random.seed(0)\n",
        "\n",
        "# Create 1D dataset with some outliers\n",
        "data_1d = np.concatenate([np.random.rand(100, 1) * 10,\n",
        "                          np.array([[20], [25]])])\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 3))\n",
        "sns.boxplot(x=data_1d.flatten(), ax=ax, color='#9fc5e8', linewidth = 2, linecolor = 'k')\n",
        "_ = ax.set_title('Box Plot - Original 1D Data')\n",
        "plt.tight_layout()\n",
        "display(pd.DataFrame(data = data_1d, columns = ['Original 1D Data']))"
      ],
      "metadata": {
        "id": "ALQzagbO4cfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='Blue'><b>Example:</b></font>"
      ],
      "metadata": {
        "id": "iO0JcB35AET6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_data(ax, data, color, title):\n",
        "    sns.kdeplot(data = data.flatten(),\n",
        "                ax = ax, fill = True,\n",
        "                color = color,\n",
        "                lw = 2,\n",
        "                legend = False)\n",
        "    ax.set_title(title, weight='bold')\n",
        "    ax.grid(axis = 'x')\n",
        "\n",
        "# Set the seed for the random number generator\n",
        "np.random.seed(0)\n",
        "\n",
        "# Create 1D dataset with some outliers\n",
        "data_1d = np.concatenate([np.random.rand(100, 1) * 10,\n",
        "                          np.array([[20], [25]])])\n",
        "\n",
        "# Initialize the standard scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the 1D dataset\n",
        "scaled_data_1d = scaler.fit_transform(data_1d)\n",
        "\n",
        "# Create figure with additional row for box plots\n",
        "fig, axs = plt.subplots(2, 2, figsize=(12, 6),\n",
        "                        sharex =  'col', sharey = False,\n",
        "                        height_ratios=[0.2, 0.8])\n",
        "\n",
        "# Box plots\n",
        "sns.boxplot(x=data_1d.flatten(), ax=axs[0, 0], color='#9fc5e8', linewidth = 2, linecolor = 'k')\n",
        "axs[0, 0].set_title('Box Plot - Original 1D Data')\n",
        "sns.boxplot(x=scaled_data_1d.flatten(), ax=axs[0, 1], color= '#b6d7a8', linewidth = 2, linecolor = 'k')\n",
        "axs[0, 1].set_title('Box Plot - Scaled 1D Data')\n",
        "\n",
        "# Density plots\n",
        "plot_data(axs[1, 0], data_1d, '#0b5394', 'Density Plot - Original 1D Data')\n",
        "plot_data(axs[1, 1], scaled_data_1d, '#38761d', 'Density Plot - Scaled 1D Data')\n",
        "# axs[1, 0].set(ylim = [0,.4])\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the original and scaled data (sorted)\n",
        "display(pd.DataFrame(data = np.hstack([data_1d, scaled_data_1d]), columns = ['Original 1D Data', 'Scaled 1D Data']).sort_values(by = ['Original 1D Data']))"
      ],
      "metadata": {
        "id": "KTzHiAIxWq0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='Blue'><b>Example:</b></font>"
      ],
      "metadata": {
        "id": "rfbHd8GlAFSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import gaussian_kde\n",
        "\n",
        "def plot_data(ax, data, facecolor, edgecolor, title):\n",
        "    ax.scatter(data[:, 0], data[:, 1], fc=facecolor, alpha=alpha, ec=edgecolor)\n",
        "    ax.set_title(title, weight='bold')\n",
        "    ax.set(xlim=[-6, 6], ylim=[-6, 6],\n",
        "           xlabel = 'Feature 1', ylabel = 'Feature 2',\n",
        "           aspect='equal')\n",
        "\n",
        "# Set the seed for the random number generator\n",
        "np.random.seed(0)\n",
        "\n",
        "# Create 2D dataset\n",
        "data_2d = np.random.rand(100, 2)\n",
        "data_2d[:,0] = data_2d[:,0] * 10 - 5\n",
        "data_2d[:,1] = data_2d[:,1] * 8 - 4\n",
        "\n",
        "# Initialize the standard scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the 2D dataset\n",
        "scaled_data_2d = scaler.fit_transform(data_2d)\n",
        "\n",
        "# Define color and alpha\n",
        "facecolor_orig = '#cc0000'\n",
        "edgecolor_orig = '#990000'\n",
        "facecolor_scaled = '#6aa84f'\n",
        "edgecolor_scaled = '#38761d'\n",
        "alpha = 0.7\n",
        "\n",
        "# Visualize the original and scaled 2D dataset\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "plot_data(axs[0], data_2d, facecolor_orig, edgecolor_orig, 'Original 2D Data')\n",
        "plot_data(axs[1], scaled_data_2d, facecolor_scaled, edgecolor_scaled, 'Scaled 2D Data')\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "WDohuiiKzup-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='Blue'><b>Example:</b></font>"
      ],
      "metadata": {
        "id": "lKbjtWMFAGLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function for data visualization\n",
        "def plot_data(ax, data, color, title):\n",
        "    sns.kdeplot(data = data, ax = ax, fill = True, color = color, legend = False)\n",
        "    ax.set_title(title, weight='bold')\n",
        "    ax.grid(axis = 'x')\n",
        "    ax.set(ylim = [0, 1])\n",
        "\n",
        "# Set the seed for the random number generator\n",
        "np.random.seed(0)\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Create DataFrame for visualization\n",
        "data = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "\n",
        "# StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "scaled_data = pd.DataFrame(data=scaled_data, columns=iris.feature_names)\n",
        "# Create figure and axes\n",
        "fig, axs = plt.subplots(len(data.columns), 2, figsize=(12, 8), sharex=True, sharey = True)\n",
        "\n",
        "# Subplots for each feature\n",
        "for i, feature in enumerate(data.columns):\n",
        "    plot_data(axs[i, 0], data[feature], '#0b5394', f'Original Data - {feature}')\n",
        "    plot_data(axs[i, 1], scaled_data[feature], '#6aa84f', f'Scaled Data - {feature}')\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "cX_75MMNUZ0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stacked visualization of the same plot"
      ],
      "metadata": {
        "id": "Yzq6-_VQHFzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# Create figure and axes\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 5), sharey = True)\n",
        "\n",
        "sns.kdeplot(ax = axs[0], data=data, fill=True, common_norm=False, palette=\"tab10\", alpha=.5, linewidth= 2)\n",
        "axs[0].set_title('Iris Data', weight = 'bold')\n",
        "sns.kdeplot(ax = axs[1], data=scaled_data, fill=True, common_norm=False, palette=\"tab10\", alpha=.5, linewidth=2)\n",
        "axs[1].set_title('Iris Data (Scaled)', weight = 'bold')\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "7a00wrHd6MR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Robust Scaler Method\n",
        "\n",
        "Given a dataset, for each feature, the `RobustScaler` adjusts the values using the following formula:\n",
        "\n",
        "$$x_{scaled} = \\frac{x - median(x)}{IQR(x)}$$\n",
        "\n",
        "Where:\n",
        "- $x$ is the original feature vector\n",
        "- $median(x)$ is the median of the feature vector\n",
        "- $IQR(x)$ is the interquartile range of the feature vector, which is the difference between the third quartile (75th percentile) and the first quartile (25th percentile)\n",
        "- $x_{scaled}$ is the scaled feature vector\n",
        "\n",
        "This formula essentially subtracts the median and then divides by the interquartile range. The interquartile range is the range between the first quartile (25th quantile) and the third quartile (75th quantile). This makes `RobustScaler` less prone to outliers. This method is particularly useful when you have data that has outliers. It's a way to standardize your data that is robust to outliers."
      ],
      "metadata": {
        "id": "Kw9OjvrHMrKo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='Blue'><b>Example:</b></font>"
      ],
      "metadata": {
        "id": "WNOOYdZzRK5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the RobustScaler class\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Create some sample data\n",
        "data = {\n",
        "    \"x1\": [1, 2, 3, 4],\n",
        "    \"x2\": [5, 6, 7, 8]\n",
        "}\n",
        "\n",
        "# Create a DataFrame from the data\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Print the original DataFrame\n",
        "print(\"Original DataFrame:\")\n",
        "display(df)\n",
        "\n",
        "# Calculate Q1, Q2 (median), and Q3\n",
        "Q1 = df.quantile(0.25)\n",
        "Q2 = df.median()\n",
        "Q3 = df.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "df_temp = pd.concat([Q1.to_frame('Q1'), Q2.to_frame('Q2'), Q3.to_frame('Q3'), IQR.to_frame('IQR')], axis = 1)\n",
        "print('\\nQ1, Median, Q3, and IQR:')\n",
        "display(df_temp)\n",
        "\n",
        "# Initialize the scaler object\n",
        "scaler = RobustScaler()\n",
        "\n",
        "# Fit and transform the DataFrame\n",
        "df_scaled = scaler.fit_transform(df)\n",
        "\n",
        "# Convert the scaled array to a DataFrame\n",
        "df_scaled = pd.DataFrame(df_scaled, columns=[\"x1\", \"x2\"])\n",
        "\n",
        "# Print the scaled DataFrame\n",
        "print(\"Scaled DataFrame:\")\n",
        "display(df_scaled)\n"
      ],
      "metadata": {
        "id": "FLsENFgNO0ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='Blue'><b>Example:</b></font>"
      ],
      "metadata": {
        "id": "-7qg8M3dRMEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set the style for the plot\n",
        "plt.style.use('https://raw.githubusercontent.com/HatefDastour/ENSF444/main/Files/mystyle.mplstyle')\n",
        "\n",
        "# Define a function to plot the data\n",
        "def plot_data(ax, data, color, title):\n",
        "    sns.kdeplot(data = data.flatten(),\n",
        "                ax = ax, fill = True,\n",
        "                color = color,\n",
        "                lw = 2,\n",
        "                legend = False)\n",
        "    ax.set_title(title, weight='bold')\n",
        "    ax.grid(axis = 'x')\n",
        "\n",
        "# Set the seed for the random number generator\n",
        "np.random.seed(0)\n",
        "\n",
        "# Create 1D dataset with some outliers\n",
        "data_1d = np.concatenate([np.random.rand(100, 1) * 10,\n",
        "                          np.array([[20], [25]])])\n",
        "\n",
        "# Initialize the robust scaler\n",
        "scaler = RobustScaler()\n",
        "\n",
        "# Fit and transform the 1D dataset\n",
        "scaled_data_1d = scaler.fit_transform(data_1d)\n",
        "\n",
        "# Create figure with additional row for box plots\n",
        "fig, axs = plt.subplots(2, 2, figsize=(12, 6),\n",
        "                        sharex =  'col', sharey = False,\n",
        "                        height_ratios=[0.2, 0.8])\n",
        "\n",
        "# Box plots\n",
        "sns.boxplot(x=data_1d.flatten(), ax=axs[0, 0], color='#9fc5e8', linewidth = 2, linecolor = 'k')\n",
        "axs[0, 0].set_title('Box Plot - Original 1D Data')\n",
        "sns.boxplot(x=scaled_data_1d.flatten(), ax=axs[0, 1], color= '#b6d7a8', linewidth = 2, linecolor = 'k')\n",
        "axs[0, 1].set_title('Box Plot - Scaled 1D Data')\n",
        "\n",
        "# Density plots\n",
        "plot_data(axs[1, 0], data_1d, '#0b5394', 'Density Plot - Original 1D Data')\n",
        "plot_data(axs[1, 1], scaled_data_1d, '#38761d', 'Density Plot - Scaled 1D Data')\n",
        "# axs[1, 0].set(ylim = [0,.6])\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Display the original and scaled data (sorted)\n",
        "display(pd.DataFrame(data = np.hstack([data_1d, scaled_data_1d]), columns = ['Original 1D Data', 'Scaled 1D Data']).sort_values(by = ['Original 1D Data']))"
      ],
      "metadata": {
        "id": "ksuiu1fwRLM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `RobustScaler` doesn't remove the outliers; it only reduces their impact when scaling the data. The outliers are still present in the scaled data, but they have less influence on the overall distribution of the scaled data compared to other scaling methods like `StandardScaler` which uses the mean and standard deviation.\n",
        "\n",
        "The goal of `RobustScaler` is not to remove outliers but to ensure that they have less influence on the scaled data. This can be particularly useful in machine learning models that are sensitive to the range of the input features. It helps to make these models more robust to outliers."
      ],
      "metadata": {
        "id": "tKFlvg_TTn_E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='Blue'><b>Example:</b></font>"
      ],
      "metadata": {
        "id": "RYTiJaPEVCh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function for data visualization\n",
        "def plot_data(ax, data, color, title):\n",
        "    sns.kdeplot(data = data, ax = ax, fill = True, color = color, legend = False)\n",
        "    ax.set_title(title, weight='bold')\n",
        "    ax.grid(axis = 'x')\n",
        "    ax.set(ylim = [0, 1])\n",
        "\n",
        "# Set the seed for the random number generator\n",
        "np.random.seed(0)\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Create DataFrame for visualization\n",
        "data = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "\n",
        "# Initialize the robust scaler\n",
        "scaler = RobustScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "scaled_data = pd.DataFrame(data=scaled_data, columns=iris.feature_names)\n",
        "# Create figure and axes\n",
        "fig, axs = plt.subplots(len(data.columns), 2, figsize=(12, 8), sharex=True, sharey = True)\n",
        "\n",
        "# Subplots for each feature\n",
        "for i, feature in enumerate(data.columns):\n",
        "    plot_data(axs[i, 0], data[feature], '#0b5394', f'Original Data - {feature}')\n",
        "    plot_data(axs[i, 1], scaled_data[feature], '#6aa84f', f'Scaled Data - {feature}')\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "Wsz2hnnwRUI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stacked visualization of the same plot"
      ],
      "metadata": {
        "id": "kX6pptXuVMyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# Create figure and axes\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 5), sharey = True)\n",
        "\n",
        "sns.kdeplot(ax = axs[0], data=data, fill=True, common_norm=False, palette=\"tab10\", alpha=.5, linewidth= 2)\n",
        "axs[0].set_title('Iris Data', weight = 'bold')\n",
        "sns.kdeplot(ax = axs[1], data=scaled_data, fill=True, common_norm=False, palette=\"tab10\", alpha=.5, linewidth=2)\n",
        "axs[1].set_title('Iris Data (Scaled)', weight = 'bold')\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "tcDP-UkEVIuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MinMax Scaler Method\n",
        "\n",
        "The `MinMaxScaler` is a data normalization technique used in machine learning preprocessing. It scales each feature to a given range, usually between 0 and 1.\n",
        "\n",
        "- It subtracts the minimum value in the feature from each value in the feature.\n",
        "- It then divides the result by the range of that feature (i.e., the difference between the maximum and minimum value).\n",
        "- The result is that the minimum value of the feature becomes 0, the maximum value becomes 1, and all other values lie in between on a relative scale.\n",
        "\n",
        "Mathematical representation of the `MinMaxScaler`:\n",
        "\n",
        "Given a dataset, for each feature, the `MinMaxScaler` adjusts the values using the following formula:\n",
        "\n",
        "$$x_{scaled} = \\frac{x - min(x)}{max(x) - min(x)}$$\n",
        "\n",
        "where:\n",
        "- $x$ is the original feature vector\n",
        "- $min(x)$ is the minimum value of the feature vector\n",
        "- $max(x)$ is the maximum value of the feature vector\n",
        "- $x_{scaled}$ is the scaled feature vector\n",
        "\n",
        "This scaling method is beneficial when you want your data to be bounded within a certain range. However, it's important to note that `MinMaxScaler` does not reduce the impact of outliers. If your data contains significant outliers, you might want to consider using a method more robust to outliers, such as the `RobustScaler`."
      ],
      "metadata": {
        "id": "otNCFs9nbEXA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='Blue'><b>Example:</b></font>"
      ],
      "metadata": {
        "id": "qgXaW70oOdxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set the style for the plot\n",
        "plt.style.use('https://raw.githubusercontent.com/HatefDastour/ENSF444/main/Files/mystyle.mplstyle')\n",
        "\n",
        "# Define a function to plot the data\n",
        "def plot_data(ax, data, color, title):\n",
        "    sns.kdeplot(data = data.flatten(),\n",
        "                ax = ax, fill = True,\n",
        "                color = color,\n",
        "                lw = 2,\n",
        "                legend = False)\n",
        "    ax.set_title(title, weight='bold')\n",
        "    ax.grid(axis = 'x')\n",
        "\n",
        "# Set the seed for the random number generator\n",
        "np.random.seed(0)\n",
        "\n",
        "# Create 1D dataset with some outliers\n",
        "data_1d = np.concatenate([np.random.rand(100, 1) * 10,\n",
        "                          np.array([[20], [25]])])\n",
        "\n",
        "# Initialize the MinMax scaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit and transform the 1D dataset\n",
        "scaled_data_1d = scaler.fit_transform(data_1d)\n",
        "\n",
        "# Create figure with additional row for box plots\n",
        "fig, axs = plt.subplots(2, 2, figsize=(12, 6),\n",
        "                        sharex =  'col', sharey = False,\n",
        "                        height_ratios=[0.2, 0.8])\n",
        "\n",
        "# Box plots\n",
        "sns.boxplot(x=data_1d.flatten(), ax=axs[0, 0], color='#9fc5e8', linewidth = 2, linecolor = 'k')\n",
        "axs[0, 0].set_title('Box Plot - Original 1D Data')\n",
        "sns.boxplot(x=scaled_data_1d.flatten(), ax=axs[0, 1], color= '#b6d7a8', linewidth = 2, linecolor = 'k')\n",
        "axs[0, 1].set_title('Box Plot - Scaled 1D Data')\n",
        "\n",
        "# Density plots\n",
        "plot_data(axs[1, 0], data_1d, '#0b5394', 'Density Plot - Original 1D Data')\n",
        "plot_data(axs[1, 1], scaled_data_1d, '#38761d', 'Density Plot - Scaled 1D Data')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Display the original and scaled data (sorted)\n",
        "display(pd.DataFrame(data = np.hstack([data_1d, scaled_data_1d]), columns = ['Original 1D Data', 'Scaled 1D Data']).sort_values(by = ['Original 1D Data']))"
      ],
      "metadata": {
        "id": "GKF9Ms9tVQna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalizer method\n",
        "\n",
        "The Normalizer method is a function that scales each non zero row of a data matrix to unit norm. This means that each row vector is divided by its length, resulting in a vector with magnitude one. This can be useful for text classification or clustering, where the cosine similarity between two vectors can be computed as their dot product. The Normalizer method can use different norms, such as l1, l2, or inf, to measure the length of a vector.\n",
        "\n",
        "1. **$\\ell_1$ norm (Manhattan norm)**: The l1 norm of a vector `x` is the sum of the absolute values of its elements. If we use the l1 norm, the normalized vector $x_{scaled}$ is given by:\n",
        "\n",
        "   $$ x_{scaled} = \\frac{x}{||x||_1} $$\n",
        "\n",
        "   where $||x||_1$ is the l1 norm of the vector `x`, calculated as $||x||_1 = \\sum |x_i|$.\n",
        "\n",
        "2. **$\\ell_2$ norm (Euclidean norm)**: The l2 norm of a vector `x` is the square root of the sum of the squares of its elements. If we use the l2 norm, the normalized vector $x_{scaled}$ is given by:\n",
        "\n",
        "   $$ x_{scaled} = \\frac{x}{||x||_2} $$\n",
        "\n",
        "   where $||x||_2$ is the l2 norm of the vector `x`, calculated as $||x||_2 = \\sqrt{ \\sum x_i^2}$.\n",
        "\n",
        "3. **$\\ell_\\inf$ norm (Maximum norm)**: The inf norm of a vector `x` is the maximum absolute value among its elements. If we use the inf norm, the normalized vector $x_{scaled}$ is given by:\n",
        "\n",
        "   $$ x_{scaled} = \\frac{x}{||x||_{inf}} $$\n",
        "\n",
        "   where $||x||_{inf}$ is the inf norm of the vector `x`, calculated as $||x||_{inf} = \\max(|x_i|)$.\n",
        "\n",
        "In all cases, the transformed vectors have a unit norm, which means the sum of their elements' absolute values (for $\\ell_1$), the square root of the sum of their elements' squares (for $\\ell_2$), or the maximum absolute value among their elements (for $\\ell_\\inf$) equals to 1. This transformation can be useful when the cosine similarity between two vectors is computed as their dot product, as it is often the case in text classification or clustering."
      ],
      "metadata": {
        "id": "-3Qb_PCcIcQ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The current version of `Normalizer` (scikit-learn 1.4.0) in `sklearn.preprocessing` scales each **row** of the data to unit norm. This means that for each row in your data, it calculates the norm (based on the type of norm you specify - 'l1', 'l2', or 'max'), and then divides each element in the row by this norm. The result is that the norm of each row will be 1 according to the type of norm used. This can be particularly useful in certain applications like text classification or clustering where cosine similarity is used. Please note that this normalization is applied independently to each sample (i.e., each row of your data), not to the features (i.e., not column-wise)."
      ],
      "metadata": {
        "id": "450LSmrSLhtf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='Blue'><b>Example:</b></font>"
      ],
      "metadata": {
        "id": "vmyAIYTQOcsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Normalizer class\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Create some sample data\n",
        "data = {\n",
        "    \"x1\": [1, 2, 3, 4],\n",
        "    \"x2\": [5, 6, 7, 8]\n",
        "}\n",
        "\n",
        "# Create a DataFrame from the data\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Print the original DataFrame\n",
        "print(\"Original Data:\")\n",
        "display(df)\n",
        "\n",
        "# List of norms\n",
        "norms = ['l1', 'l2', 'max']\n",
        "\n",
        "# Loop over the norms\n",
        "for norm in norms:\n",
        "    # Initialize the Normalizer object\n",
        "    normalizer = Normalizer(norm=norm)\n",
        "\n",
        "    # Fit and transform the DataFrame\n",
        "    df_normalized = normalizer.fit_transform(df)\n",
        "\n",
        "    # Convert the normalized array to a DataFrame\n",
        "    df_normalized = pd.DataFrame(df_normalized, columns=[\"x1\", \"x2\"])\n",
        "\n",
        "    # Print the normalized DataFrame\n",
        "    print(f\"\\nNormalized Data by {norm} norm:\")\n",
        "    display(df_normalized)"
      ],
      "metadata": {
        "id": "n2KZknTEDwet"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}