{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to SVM\n",
        "\n",
        "A hyperplane is a plane with one less dimension than the dimension of its ambient space. For example, if space is 3-dimensional, then its hyperplanes are 2-dimensional planes. Moreover, if the space is 2-dimensional, its hyperplanes are the 1-dimensional lines.\n"
      ],
      "metadata": {
        "id": "BI6XlHcUvo9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "plt.style.use('https://raw.githubusercontent.com/HatefDastour/ENSF444/main/Files/mystyle.mplstyle')\n",
        "\n",
        "# Create a figure\n",
        "fig = plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Define the grid\n",
        "gs = gridspec.GridSpec(1, 2, width_ratios=[3, 4])\n",
        "\n",
        "# Left Subplot - 2D hyperplane plot\n",
        "ax1 = plt.subplot(gs[0])\n",
        "X1 = np.linspace(-1.5, 1.5, 1000)\n",
        "X2 = -(1 + 2*X1)/3\n",
        "ax1.plot(X1, X2)\n",
        "ax1.set(xlabel=r'$X_1$', ylabel=r'$X_2$', xlim=[-1.5, 1.5], ylim=[-1.5, 1.5], aspect=1)\n",
        "ax1.fill_between(X1, -1.5, -(1 + 2*X1)/3, color='LimeGreen', alpha=0.1)\n",
        "ax1.annotate(r'$1 + 2X_1 + 3X_2 > 0$', xy=(-0.5, 0.5), fontsize='xx-large')\n",
        "ax1.fill_between(X1, -(1 + 2*X1)/3, 1.5, color='Orange', alpha=0.1)\n",
        "ax1.annotate(r'$1 + 2X_1 + 3X_2 < 0$', xy=(-0.9, -1), fontsize='xx-large')\n",
        "ax1.grid(True)\n",
        "ax1.set_title('A 2D Hyperplane Plot')\n",
        "\n",
        "# Right Subplot - 3D-like representation\n",
        "ax2 = plt.subplot(gs[1], projection='3d')\n",
        "X1_3d, X2_3d = np.meshgrid(np.linspace(-1.5, 1.5, 100), np.linspace(-1.5, 1.5, 100))\n",
        "# Calculate corresponding values for X3 based on the hyperplane equation -(1 + 2*X1 + 3*X2)/4\n",
        "X3_3d = -(1 + 2*X1_3d + 3*X2_3d)/4\n",
        "# Plot the 3D hyperplane\n",
        "ax2.plot_surface(X1_3d, X2_3d, X3_3d, alpha=0.5, rstride=100, cstride=100, color='b')\n",
        "# Set labels for X, Y, and Z axes\n",
        "ax2.set(xlabel=r'$X_1$', ylabel=r'$X_2$', zlabel=r'$X_3$')\n",
        "# Set limits for X, Y, and Z axes\n",
        "ax2.set_xlim([-1.5, 1.5])\n",
        "ax2.set_ylim([-1.5, 1.5])\n",
        "ax2.set_zlim([-1.5, 1.5])\n",
        "# Enable the grid\n",
        "ax2.grid(True)\n",
        "# Annotate the region above the hyperplane\n",
        "ax2.text(-0.5, 0, 0.5, r'$1 + 2X_1 + 3X_2 + 4X_3 > 0$', fontsize='xx-large')\n",
        "# Annotate the region below the hyperplane\n",
        "ax2.text(-2.5, 0, -3, r'$1 + 2X_1 + 3X_2 + 4X_3 < 0$', fontsize='xx-large')\n",
        "ax2.set_title('A 3D Hyperplane Plot')\n",
        "\n",
        "# Ensure a tight layout\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "b6PtHlBsv6jS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finding the Maximal Margin Hyperplane\n",
        "\n",
        "We have *n* points *$x_1, ..., x_n$* in a *p*-dimensional space, each with a label *$y_1, ..., y_n$* of either -1 or 1. We want to find the best line that splits the points by their labels.\n",
        "\n",
        "The best boundary is the one that has the largest distance to the nearest points from each class. This distance is the margin (*$M$*), and it shows how well the boundary splits the classes. The bigger the margin, the less likely we make mistakes.\n",
        "\n",
        "To find the best boundary, we need to solve a math problem that finds the values *$b, w_1, ..., w_p$* that define the boundary equation:\n",
        "\n",
        "\\begin{equation}\n",
        "b + w^T x = 0\n",
        "\\end{equation}\n",
        "\n",
        "Where:\n",
        "- *$x$* is any data point.\n",
        "- *$b$* is the intercept, and *$w$* is the slope vector.\n",
        "\n",
        "The margin, also known as the geometric margin, is the distance between the hyperplane and the closest data point on either side of the hyperplane. The margin can be calculated as:\n",
        "\n",
        "$$\\text{Margin} = \\frac{2}{\\|w\\|}$$\n",
        "\n",
        "where $\\|w\\|$ is the $\\ell_2$ norm of the weight vector. The $\\ell_2$ norm measures the length or magnitude of the vector.\n",
        "\n",
        "The math problem has some rules for each data point (*$i$*) [James et al., 2023]:\n",
        "\n",
        "\\begin{equation}\n",
        "y_i (b + w^T x_i) \\geq M (1 - \\epsilon_i)\n",
        "\\end{equation}\n",
        "\n",
        "Where:\n",
        "- *$y_i$* is the class label of the *$i$*-th point, either -1 or 1.\n",
        "- *$x_i$* is the *$i$*-th data point.\n",
        "- *$\\epsilon_i$* is a variable that lets us be flexible in the separation. It is zero for points that are outside the margin and correctly classified, positive for points that are inside the margin or wrong, and negative for points that are on the boundary.\n",
        "\n",
        "These rules make sure that the boundary separates the classes as well as we can, while allowing some errors for noisy or overlapping data.\n",
        "\n",
        "Another rule that keeps the values normal is:\n",
        "\n",
        "\\begin{equation}\n",
        "\\| w \\|^2_2 = \\sum w_j^2 = 1\n",
        "\\end{equation}\n",
        "\n",
        "This rule stops the values from getting too big, which would change the margin.\n",
        "\n",
        "Also, the parameter *$C$* balances between maximizing the margin *$M$* and minimizing the errors *$\\epsilon_i$*:\n",
        "\n",
        "\\begin{equation}\n",
        "\\sum_{i=1}^{n} \\epsilon_i \\leq C\n",
        "\\end{equation}\n",
        "\n",
        "The parameter *$C$* limits how much error we can have in the separation. A small *$C$* means a large margin and a strict separation, while a large *$C$* means a small margin and a flexible separation.\n",
        "\n",
        "The math problem tries to find the values *$b, w_1, ..., w_p$* and the variables *$\\epsilon_i$* that follow these rules and maximize the margin *$M$*. By solving this problem, we get the best boundary that separates the classes in the data space. The points that are closest to the boundary and decide its position and margin are called support vectors. They have the most effect on the separation result."
      ],
      "metadata": {
        "id": "bEd1kTjeC3l9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  Support vectors are the data points closest to the decision boundary (hyperplane) that define its position and orientation. They are crucial for SVM (Support Vector Machine) algorithms because they determine the margin, which is the distance between the decision boundary and the nearest data points. The support_vectors variable contains the coordinates of these data points in your feature space.\n",
        "\n",
        "*  Coefficients are the weights of each feature in the decision boundary equation. They represent the importance and direction of each feature in the feature space. For a linear SVC model, the decision boundary is a hyperplane, and the coefficients are the normal vector to this hyperplane. The coefficients variable contains the coefficients for each feature dimension.\n",
        "\n",
        "*  Intercept is the offset of the decision boundary from the origin in the feature space. It shifts the decision boundary along the axis that is orthogonal to the hyperplane. A positive intercept means the decision boundary is shifted in one direction, and a negative intercept means it is shifted in the opposite direction. The intercept variable contains the intercept value for the decision boundary.\n",
        "\n",
        "*  Margin is the distance between the decision boundary and the nearest support vectors. It measures how well the model can separate the classes. The margin variable calculates the margin using the formula:\n",
        "\n",
        "*  margin = 2 / np.linalg.norm(coefficients)\n",
        "Slack variables are the distances of the data points from the margin. They allow some data points to be on the wrong side of the decision boundary while still satisfying the margin constraints. They are used in soft-margin SVMs. The slack_variables variable calculates the slack variables using the formula:\n",
        "\n",
        "*  slack_variables = 1 - dual_coefs\n",
        "where dual_coefs are the Lagrange multipliers associated with the support vectors. They indicate the importance of each support vector in determining the decision boundary. The dual_coefs variable retrieves the dual coefficients for the first class of the binary classification problem."
      ],
      "metadata": {
        "id": "nLyNr4Jz_bWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Sample data (features and class labels)\n",
        "X = np.array([[2, 1], [3, 3], [4, 3], [5, 4], [6, 5], [7, 5], [8, 6], [9, 7]])\n",
        "y = np.array([-1, -1, -1, -1, 1, 1, 1, 1])\n",
        "\n",
        "# Create a Support Vector Classifier\n",
        "svc = SVC(kernel='linear')\n",
        "\n",
        "# Fit the model to the data\n",
        "svc.fit(X, y)\n",
        "\n",
        "# Get the support vectors\n",
        "support_vectors = svc.support_vectors_\n",
        "print(\"Support Vectors:\")\n",
        "for i in range(support_vectors.shape[1]):\n",
        "    print(f'\\t\\t{support_vectors[i, :]}')\n",
        "\n",
        "# Get the coefficients (w values) and the intercept (b value)\n",
        "# The hyperplane is defined by w^T x + b = 0\n",
        "coefficients = svc.coef_\n",
        "intercept = svc.intercept_\n",
        "print(\"Coefficients (w values):\\t\\t\", coefficients)\n",
        "print(\"Intercept (b value):\\t\\t\\t\", intercept)\n",
        "\n",
        "# Get the margin (M)\n",
        "# The margin is defined by 2 / ||w||\n",
        "margin = 2 / np.linalg.norm(coefficients)\n",
        "print(f'Margin (M):\\t\\t\\t\\t {margin:+.3f}')"
      ],
      "metadata": {
        "id": "tZYP2ay_v6yQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "# Hyperplane equation\n",
        "w1, w2 = svc.coef_[0]\n",
        "b = svc.intercept_[0]\n",
        "equation = f'${w1:+.2f} \\cdot x_1 {w2:+.2f} \\cdot x_2 {b:+.2f} = 0$'\n",
        "\n",
        "# Display the equation using LaTeX\n",
        "from IPython.display import Latex, display\n",
        "display(Latex(equation))\n",
        "\n",
        "\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "colors = [\"#f5645a\", \"#b781ea\"]\n",
        "edge_colors = ['#8A0002', '#3C1F8B']\n",
        "# Create a figure and axis\n",
        "fig, ax = plt.subplots(figsize=(7, 6))\n",
        "\n",
        "# Decision boundary display\n",
        "DecisionBoundaryDisplay.from_estimator(svc, X, cmap= ListedColormap(colors), ax=ax,\n",
        "                                       grid_resolution = 300,\n",
        "                                       response_method=\"predict\",\n",
        "                                       plot_method=\"pcolormesh\",\n",
        "                                       alpha=0.5,\n",
        "                                       xlabel='Feature 1', ylabel='Feature 2', shading=\"auto\")\n",
        "\n",
        "# Define labels and markers for different classes\n",
        "class_info = [(1, 'o', colors[1]), (-1, 's', colors[0])]\n",
        "\n",
        "for label, marker, color in class_info:\n",
        "    class_data = X[y == label]\n",
        "    ax.scatter(class_data[:, 0], class_data[:, 1], fc=color, ec=edge_colors[label == 1],\n",
        "               label=str(label), marker=marker)\n",
        "\n",
        "# Plot support vectors\n",
        "support_vectors = svc.support_vectors_\n",
        "ax.scatter(support_vectors[:, 0], support_vectors[:, 1], s=200, facecolors='none',\n",
        "           edgecolors='k', lw=2, label='Support Vectors')\n",
        "\n",
        "# Decision boundary line\n",
        "w1, w2 = svc.coef_[0]\n",
        "b = svc.intercept_[0]\n",
        "line_x = np.linspace(ax.get_xlim()[0], ax.get_xlim()[1], 100)\n",
        "line_y = (-w1 / w2) * line_x - (b / w2)\n",
        "ax.plot(line_x, line_y, color='black', linestyle='--', label= f'Decision Boundary: {equation}')\n",
        "\n",
        "# Plot settings\n",
        "ax.legend(loc = 'lower right')\n",
        "ax.set_title(\"\"\"Support Vector Classifier\"\"\", fontweight='bold', fontsize=16)\n",
        "ax.grid(False)\n",
        "ax.set_ylim(0, 8)\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "jCxCWO896WCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kernel Methods\n",
        "\n",
        "However, not all data sets are linearly separable or have a linear relationship with the target variable. In such cases, SVM can use a technique called kernel methods to map the data points into a higher-dimensional feature space, where a linear hyperplane can be found. Kernel methods are based on the observation that many machine learning algorithms can be written in terms of inner products between data points, such as $x^T y$. By replacing these inner products with a kernel function $K(x, y)$, we can implicitly compute the inner products in the feature space without explicitly transforming the data points. This is known as the kernel trick, and it allows us to use SVM with nonlinear and complex data sets. The kernel function, also known as the similarity function, can be any function that satisfies some mathematical properties, such as symmetry and positive definiteness. Some common examples of kernel functions are:\n",
        "\n",
        "To use support vector machines, you can map your data to a higher-dimensional space with different methods, such as:\n",
        "- The **linear kernel**, which uses a simple dot product\n",
        "- The **polynomial kernel**, which raises the dot product to a power\n",
        "- The **RBF kernel** or **Gaussian kernel**, which measures the similarity between points\n",
        "- The **Sigmond kernel**, which applies a sigmoid function to the dot product\n",
        "\n",
        "The choice of kernel and its parameters, also known as the hyperparameters of the SVM model, depends on the characteristics of the data set and the problem at hand. It is important to evaluate the performance of different kernels and tune their parameters using various methods, such as cross-validation, grid search, random search, or Bayesian optimization. By using kernel methods, SVM can learn complex and nonlinear patterns from the data and achieve high accuracy and generalization."
      ],
      "metadata": {
        "id": "g3TExJkW4e-9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM Examples\n",
        "\n",
        "## Example: Iris Flower Dataset"
      ],
      "metadata": {
        "id": "rI_6ibgbPsmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets, svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "# Load data\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data[:, :2]\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create SVM models\n",
        "models = [svm.SVC(kernel=\"linear\", C=1.0),\n",
        "          svm.SVC(kernel=\"rbf\", gamma=0.7, C=1.0),\n",
        "          svm.SVC(kernel=\"poly\", degree=3, gamma=\"auto\", C=1.0),\n",
        "          svm.SVC(kernel=\"poly\", degree=5, gamma=\"auto\", C=1.0)]\n",
        "\n",
        "# Feature labels\n",
        "xlabel, ylabel = [x.title().replace('Cm','CM') for x in iris.feature_names[:2]]\n",
        "\n",
        "# Fit models to data\n",
        "models = [clf.fit(X_train, y_train) for clf in models]\n",
        "\n",
        "# Titles for the plots\n",
        "titles = [\"LinearSVC (linear kernel)\",\n",
        "          \"SVC with RBF kernel\",\n",
        "          \"SVC with polynomial (degree 3) kernel\",\n",
        "          \"SVC with polynomial (degree 5) kernel\"]\n",
        "\n",
        "# Define colors and markers\n",
        "colors = [\"#f44336\", \"#40a347\", '#0086ff']\n",
        "edge_colors = [\"#cc180b\", \"#16791d\", '#11548f']\n",
        "markers = ['o', 's', 'd']\n",
        "cmap_ = ListedColormap(colors)\n",
        "\n",
        "# Create 2x2 grid for plotting\n",
        "fig, axes = plt.subplots(2, 2, figsize=(11, 9))\n",
        "\n",
        "# Create a dictionary to map target names to numbers\n",
        "label_map = dict(zip( np.arange(len(iris.target_names)), [x.title() for x in iris.target_names]))\n",
        "\n",
        "# Plot decision boundaries\n",
        "for clf, title, ax in zip(models, titles, axes.flatten()):\n",
        "    disp = DecisionBoundaryDisplay.from_estimator(clf, X_train,\n",
        "                                                  response_method=\"predict\", cmap=cmap_,\n",
        "                                                  grid_resolution = 300,\n",
        "                                                  alpha=0.2, ax=ax,\n",
        "                                                  xlabel=xlabel,\n",
        "                                                  ylabel=ylabel)\n",
        "\n",
        "    # Scatter plot of data points with target names\n",
        "    for num in np.unique(y):\n",
        "        ax.scatter(X_train[:, 0][y_train == num], X_train[:, 1][y_train == num],\n",
        "                   c=colors[num],\n",
        "                   s=40, ec=edge_colors[num],\n",
        "                   marker=markers[num], label=label_map[num])\n",
        "\n",
        "    ax.set_title(title, weight='bold')\n",
        "    ax.grid(False)\n",
        "    ax.legend(title = 'Flower Type', fontsize = 11, loc = 'best')\n",
        "\n",
        "# Add a title\n",
        "plt.suptitle(\"SVM Classifier Comparison (Train Dataset)\", fontsize=16, fontweight='bold')\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Fit models to data and calculate accuracy scores\n",
        "for clf, title in zip(models, titles):\n",
        "    clf.fit(X_train, y_train)\n",
        "    train_preds = clf.predict(X_train)\n",
        "    test_preds = clf.predict(X_test)\n",
        "    train_acc = accuracy_score(y_train, train_preds)\n",
        "    test_acc = accuracy_score(y_test, test_preds)\n",
        "    print(f\"{title}\\nTrain Accuracy: {train_acc:.3f}\\nTest Accuracy: {test_acc:.3f}\\n\")"
      ],
      "metadata": {
        "id": "9ZpW8FlS9TyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example: SVR"
      ],
      "metadata": {
        "id": "ZSOYl59jYeTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.style.use('https://raw.githubusercontent.com/HatefDastour/ENSF444/main/Files/mystyle.mplstyle')\n",
        "\n",
        "# Generate sample data\n",
        "rng = np.random.RandomState(42)\n",
        "X = 5 * rng.rand(200, 1)\n",
        "y = np.cos(X).ravel()\n",
        "\n",
        "# Add noise to targets\n",
        "y[::5] += 3 * (0.5 - rng.rand(X.shape[0] // 5))\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_plot = np.linspace(0, 5, 1000)[:, None]\n",
        "\n",
        "# Specify the kernel types\n",
        "kernels = ['linear', 'rbf']\n",
        "\n",
        "# Initialize the plot\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 5), sharex=True, sharey=True)\n",
        "\n",
        "for ax, kernel in zip(axes.ravel(), kernels):\n",
        "    # Fit regression model\n",
        "    svr = SVR(kernel=kernel, C=1.0, epsilon=0.1)\n",
        "    svr.fit(X_train, y_train)\n",
        "    y_train_pred = svr.predict(X_train)\n",
        "    y_test_pred = svr.predict(X_test)\n",
        "    y_plot = svr.predict(X_plot)\n",
        "\n",
        "    # Calculate MSE\n",
        "    mse_train = mean_squared_error(y_train, y_train_pred)\n",
        "    mse_test = mean_squared_error(y_test, y_test_pred)\n",
        "    print(f'MSE for {kernel} kernel: Train = {mse_train:.3f}, Test = {mse_test:.3f}')\n",
        "\n",
        "    # Plot\n",
        "    ax.plot(X_plot, y_plot, color='k', lw=2, label=f'{kernel} model')\n",
        "    ax.scatter(X_train[svr.support_], y_train[svr.support_], facecolor=\"green\",\n",
        "               edgecolor='DarkGreen', s=50, label=f'{kernel} support vectors')\n",
        "    ax.scatter(X_train[np.setdiff1d(np.arange(len(X_train)), svr.support_)], y_train[np.setdiff1d(np.arange(len(X_train)),\n",
        "                                                                                svr.support_)], facecolor=\"none\",\n",
        "               edgecolor='red', s=50, label='other training data')\n",
        "    ax.legend(loc='best', ncol=1, fancybox=True, shadow=True)\n",
        "    ax.set_title(f'SVR with a {kernel.title()} Kernel', weight = 'bold')\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "7AY907Gua-E1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}