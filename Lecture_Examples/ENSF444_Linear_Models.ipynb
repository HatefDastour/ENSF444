{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFa5WyrsJasB"
   },
   "source": [
    "# Linear Models: Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dB_kFkh_50ru"
   },
   "source": [
    "## Linear Regression: Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GGnyITjtzMe3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set custom style\n",
    "plt.style.use('https://raw.githubusercontent.com/HatefDastour/ENSF444/main/Files/mystyle.mplstyle')\n",
    "\n",
    "# Generate random data for demonstration\n",
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "print(f'N = {len(y)}')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a linear regression linr_reg\n",
    "linr_reg = LinearRegression()\n",
    "\n",
    "# Fit the linr_reg to the training data\n",
    "linr_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training and test data\n",
    "y_train_pred = linr_reg.predict(X_train)\n",
    "y_test_pred = linr_reg.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error for training and testing sets\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "# Create subplots for training and testing sets\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Label for linear regression equation\n",
    "label = r'$\\hat{y}$' + f' = {linr_reg.coef_.ravel()[0]:.3f}x' + f' + {linr_reg.intercept_.ravel()[0]:.3f}'\n",
    "\n",
    "# Plot for the training set\n",
    "axs[0].scatter(X_train, y_train, label=f'Training Data', fc='aqua', ec='blue')\n",
    "axs[0].plot(X_train, y_train_pred, 'r-', label=label)\n",
    "axs[0].set_title(f'Training Set (MSE = {mse_train:.3f})')\n",
    "\n",
    "# Plot for the testing set\n",
    "axs[1].scatter(X_test, y_test, label=f'Testing Data', fc='aqua', ec='blue')\n",
    "axs[1].plot(X_test, y_test_pred, 'r-', label=label)\n",
    "axs[1].set_title(f'Testing Set (MSE = {mse_test:.3f})')\n",
    "\n",
    "for ax in axs:\n",
    "  ax.set_xlabel('x')\n",
    "  ax.set_ylabel('y')\n",
    "  ax.legend(loc = 'upper left')\n",
    "  ax.set(xlim = [-.1, 2.1], ylim = [2.9,12.1])\n",
    "\n",
    "# Adjust layout and display the plots\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8CbH0cO7bOY"
   },
   "source": [
    "## Ridge Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52tAE25h1wvH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate random data for demonstration\n",
    "np.random.seed(42)\n",
    "X, y = 2 * np.random.rand(100, 1), 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "alpha_values = [1e-3, 1e-2, 3e-2, 0.5, 1, 2, 4, 6, 10]\n",
    "train_mse_scores, test_mse_scores = [], []\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    # Create and fit a Ridge regression model\n",
    "    ridge = Ridge(alpha=alpha).fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the training and test data\n",
    "    y_train_pred, y_test_pred = ridge.predict(X_train), ridge.predict(X_test)\n",
    "\n",
    "    # Calculate Mean Squared Error for training and testing sets\n",
    "    mse_train, mse_test = mean_squared_error(y_train, y_train_pred), mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "    # Append MSE scores to lists for plotting\n",
    "    train_mse_scores.append(mse_train)\n",
    "    test_mse_scores.append(mse_test)\n",
    "    # Print training and test MSE scores\n",
    "    print(f'alpha = {alpha}')\n",
    "    print(\"Training MSE: {:.3f}\".format(mse_train))\n",
    "    print(\"Test MSE: {:.3f}\".format(mse_test))\n",
    "    print('\\n')\n",
    "\n",
    "# Plotting using fig and ax\n",
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "ax.plot(alpha_values, train_mse_scores, label='Training MSE', marker='o', color='Green')\n",
    "ax.plot(alpha_values, test_mse_scores, label='Test MSE', marker='o', color='Blue')\n",
    "ax.set_xscale('log')  # Using log scale for better visualization\n",
    "ax.set_xlabel(r'$\\alpha$ Values (log scale)')\n",
    "ax.set_ylabel('Mean Squared Error (MSE)')\n",
    "ax.set_title('Ridge Regression: Training and Test MSE Scores for Various Alpha Values')\n",
    "ax.legend(loc = 'upper left')\n",
    "\n",
    "# Adjust layout and display the plots\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9JSBr9wAcjf"
   },
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LZIOPml3AcvL"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate random data for demonstration\n",
    "np.random.seed(42)\n",
    "X, y = 2 * np.random.rand(100, 1), 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "alpha_values = [1e-3, 1e-2, 3e-2, 0.5, 1, 2, 4, 6, 10]\n",
    "train_mse_scores, test_mse_scores = [], []\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    # Create and fit a Lasso regression model\n",
    "    lasso = Lasso(alpha=alpha).fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the training and test data\n",
    "    y_train_pred, y_test_pred = lasso.predict(X_train), lasso.predict(X_test)\n",
    "\n",
    "    # Calculate Mean Squared Error for training and testing sets\n",
    "    mse_train, mse_test = mean_squared_error(y_train, y_train_pred), mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "    # Append MSE scores to lists for plotting\n",
    "    train_mse_scores.append(mse_train)\n",
    "    test_mse_scores.append(mse_test)\n",
    "\n",
    "    # Print training and test MSE scores\n",
    "    print(f'alpha = {alpha}')\n",
    "    print(\"Training MSE: {:.3f}\".format(mse_train))\n",
    "    print(\"Test MSE: {:.3f}\".format(mse_test))\n",
    "    print('\\n')\n",
    "\n",
    "# Plotting using fig and ax\n",
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "ax.plot(alpha_values, train_mse_scores, label='Training MSE', marker='o', color='Green')\n",
    "ax.plot(alpha_values, test_mse_scores, label='Test MSE', marker='o', color='Blue')\n",
    "ax.set_xscale('log')  # Using log scale for better visualization\n",
    "ax.set_xlabel(r'$\\alpha$ Values (log scale)')\n",
    "ax.set_ylabel('Mean Squared Error (MSE)')\n",
    "ax.set_title('Lasso Regression: Training and Test MSE Scores for Various Alpha Values')\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "# Adjust layout and display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IzZSFdvG5ESu"
   },
   "source": [
    "## Regression Accuracy Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A-IWUMe55V_x"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "\n",
    "# Linear Regression\n",
    "linr_reg = LinearRegression().fit(X_train, y_train)\n",
    "y_train_pred_lr = linr_reg.predict(X_train)\n",
    "y_test_pred_lr = linr_reg.predict(X_test)\n",
    "\n",
    "# Ridge Regression\n",
    "ridge = Ridge(alpha=1).fit(X_train, y_train)\n",
    "y_train_pred_ridge = ridge.predict(X_train)\n",
    "y_test_pred_ridge = ridge.predict(X_test)\n",
    "\n",
    "# Lasso Regression\n",
    "lasso = Lasso(alpha=1e-3).fit(X_train, y_train)\n",
    "y_train_pred_lasso = lasso.predict(X_train)\n",
    "y_test_pred_lasso = lasso.predict(X_test)\n",
    "\n",
    "# Calculate the R-squared for each model on test set\n",
    "r2_lr_test = r2_score(y_test, y_test_pred_lr)\n",
    "r2_ridge_test = r2_score(y_test, y_test_pred_ridge)\n",
    "r2_lasso_test = r2_score(y_test, y_test_pred_lasso)\n",
    "\n",
    "# Calculate the R-squared for each model on train set\n",
    "r2_lr_train = r2_score(y_train, y_train_pred_lr)\n",
    "r2_ridge_train = r2_score(y_train, y_train_pred_ridge)\n",
    "r2_lasso_train = r2_score(y_train, y_train_pred_lasso)\n",
    "\n",
    "# Print the combined R-squared values for each model\n",
    "print(f\"R-squared for linear regression:\\n  Train Set: {r2_lr_train:.3f}\\n  Test Set: {r2_lr_test:.3f}\\n\")\n",
    "print(f\"R-squared for ridge regression:\\n  Train Set: {r2_ridge_train:.3f}\\n  Test Set: {r2_ridge_test:.3f}\\n\")\n",
    "print(f\"R-squared for lasso regression:\\n  Train Set: {r2_lasso_train:.3f}\\n  Test Set: {r2_lasso_test:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RSbOOLSJQtZi"
   },
   "source": [
    "## R-Squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l70BaY0wRMrd"
   },
   "source": [
    "In a linear regression context, the relationship between the coefficient of determination (R-squared) and the square of the correlation coefficient (correlation$^2$) can be illustrated through a Python example. When there is a single independent variable (X) and a dependent variable (y), this relationship holds true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j9X42O83QvuV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Generate random data\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1)\n",
    "y = 2 * X.squeeze() + 1 + 0.1 * np.random.randn(100)\n",
    "\n",
    "# Fit linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Calculate R-squared\n",
    "r_squared = r2_score(y, model.predict(X))\n",
    "\n",
    "# Calculate correlation coefficient\n",
    "correlation = np.corrcoef(X.squeeze(), y)[0, 1]\n",
    "\n",
    "# Verify the relationship: R-squared = correlation^2\n",
    "assert np.isclose(r_squared, correlation**2)\n",
    "\n",
    "# Plotting the regression line\n",
    "plt.scatter(X, y, label='Data points')\n",
    "plt.plot(X, model.predict(X), color='red', label='Regression line')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Linear Regression Example')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f'R-squared: {r_squared}')\n",
    "print(f'Correlation coefficient: {correlation}')\n",
    "print(f'Correlation coefficient^2: {correlation**2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TeRdfqz4RQek"
   },
   "source": [
    "In this example, a linear regression model is fitted to randomly generated data. The R-squared value is calculated using the `r2_score` function from scikit-learn, and the correlation coefficient is calculated using `np.corrcoef`. The assertion confirms the relationship between R-squared and the square of the correlation coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHxS3oWIVCNV"
   },
   "source": [
    "## Another Example\n",
    "\n",
    "It's important to note that the benefits of Ridge and Lasso are often more pronounced in scenarios with high multicollinearity, where these regularization techniques can help prevent overfitting and improve generalization to new data. Further experimentation with different datasets and tuning of regularization parameters may provide additional insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f2H9b2lVU_ov"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Create a synthetic dataset with multicollinearity\n",
    "X, y = make_regression(n_samples=500, n_features=4, noise=5, random_state=42)\n",
    "X[:, 2] = 5 * X[:, 1]  # Introducing multicollinearity\n",
    "X[:, 3] = 7 * X[:, 1]  # Further multicollinearity\n",
    "\n",
    "# Split the synthetic data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to evaluate regression model performance\n",
    "def evaluate_performance(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of a regression model using mean squared error (MSE) and R-squared.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: array-like, true target values\n",
    "    - y_pred: array-like, predicted target values\n",
    "\n",
    "    Returns:\n",
    "    - mse: Mean Squared Error\n",
    "    - r2: R-squared\n",
    "    \"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YVf3NVtQVKVG"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "import numpy as np\n",
    "\n",
    "# Specify a range of alpha values to test\n",
    "alphas = np.logspace(-6, 6, 13)  # Set up a logarithmic range of alpha values for regularization\n",
    "\n",
    "# Create RidgeCV model with specified alphas and perform cross-validation\n",
    "ridge_cv = RidgeCV(alphas=alphas, store_cv_values=True)\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "\n",
    "# Create LassoCV model with specified alphas and perform cross-validation\n",
    "lasso_cv = LassoCV(alphas=alphas)\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "\n",
    "# Get the best alpha determined by cross-validation\n",
    "best_alpha_ridge = ridge_cv.alpha_\n",
    "best_alpha_lasso = lasso_cv.alpha_\n",
    "\n",
    "# Use the best alpha to train the final Ridge Regression model\n",
    "ridge_reg_best = Ridge(alpha=best_alpha_ridge)\n",
    "ridge_reg_best.fit(X_train, y_train)\n",
    "\n",
    "# Use the best alpha to train the final Lasso Regression model\n",
    "lasso_reg_best = Lasso(alpha=best_alpha_lasso)\n",
    "lasso_reg_best.fit(X_train, y_train)\n",
    "\n",
    "# Print results for Ridge Regression\n",
    "print(f'Best Alpha (Ridge) = {best_alpha_ridge}')\n",
    "print(\"\\nRidge Regression with Best Alpha - Training set:\")\n",
    "print(\"MSE:\", f\"{evaluate_performance(y_train, ridge_reg_best.predict(X_train))[0]:.3f}\")\n",
    "print(\"R-squared:\", f\"{evaluate_performance(y_train, ridge_reg_best.predict(X_train))[1]:.3f}\")\n",
    "print(\"\\nRidge Regression with Best Alpha - Test set:\")\n",
    "print(\"MSE:\", f\"{evaluate_performance(y_test, ridge_reg_best.predict(X_test))[0]:.3f}\")\n",
    "print(\"R-squared:\", f\"{evaluate_performance(y_test, ridge_reg_best.predict(X_test))[1]:.3f}\")\n",
    "print(30*'_')\n",
    "\n",
    "# Print results for Lasso Regression\n",
    "print(f'Best Alpha (Lasso) = {best_alpha_lasso}')\n",
    "print(\"\\nLasso Regression with Best Alpha - Training set:\")\n",
    "print(\"MSE:\", f\"{evaluate_performance(y_train, lasso_reg_best.predict(X_train))[0]:.3f}\")\n",
    "print(\"R-squared:\", f\"{evaluate_performance(y_train, lasso_reg_best.predict(X_train))[1]:.3f}\")\n",
    "print(\"\\nLasso Regression with Best Alpha - Test set:\")\n",
    "print(\"MSE:\", f\"{evaluate_performance(y_test, lasso_reg_best.predict(X_test))[0]:.3f}\")\n",
    "print(\"R-squared:\", f\"{evaluate_performance(y_test, lasso_reg_best.predict(X_test))[1]:.3f}\")\n",
    "print(30*'_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PhAb7MLNHRgJ"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Initialize lists for storing Ridge and Lasso MSE scores\n",
    "ridge_train_mse_scores, ridge_test_mse_scores = [], []\n",
    "lasso_train_mse_scores, lasso_test_mse_scores = [], []\n",
    "\n",
    "# Iterate over different alpha values\n",
    "for alpha in alphas:\n",
    "    # Create and fit Ridge regression model\n",
    "    ridge = Ridge(alpha=alpha).fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on training and test data\n",
    "    y_train_pred, y_test_pred = ridge.predict(X_train), ridge.predict(X_test)\n",
    "\n",
    "    # Calculate Mean Squared Error for training and testing sets\n",
    "    mse_train, mse_test = mean_squared_error(y_train, y_train_pred), mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "    # Append MSE scores for Ridge to lists\n",
    "    ridge_train_mse_scores.append(mse_train)\n",
    "    ridge_test_mse_scores.append(mse_test)\n",
    "\n",
    "    # Create and fit Lasso regression model\n",
    "    lasso = Lasso(alpha=alpha).fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on training and test data\n",
    "    y_train_pred, y_test_pred = lasso.predict(X_train), lasso.predict(X_test)\n",
    "\n",
    "    # Calculate Mean Squared Error for training and testing sets\n",
    "    mse_train, mse_test = mean_squared_error(y_train, y_train_pred), mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "    # Append MSE scores for Lasso to lists\n",
    "    lasso_train_mse_scores.append(mse_train)\n",
    "    lasso_test_mse_scores.append(mse_test)\n",
    "\n",
    "# Create subplots with shared x-axis\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 6), sharex=True)\n",
    "\n",
    "# Plot Ridge results\n",
    "ax1.semilogx(alphas, ridge_train_mse_scores, '-o', label='Training MSE', color='green')\n",
    "ax1.semilogx(alphas, ridge_test_mse_scores, '-o', label='Test MSE', color='blue')\n",
    "ax1.set(title='Ridge Cross-Validation Values', ylabel='MSE', ylim=[1400, 4000])\n",
    "\n",
    "# Highlight the best alpha for Ridge\n",
    "ax1.axvline(best_alpha_ridge, color='red', lw=2, linestyle='--', label=f'Best Alpha (Ridge) = {best_alpha_ridge}')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot Lasso results\n",
    "ax2.semilogx(alphas, lasso_train_mse_scores, '-o', label='Training MSE', color='green')\n",
    "ax2.semilogx(alphas, lasso_test_mse_scores, '-o', label='Test MSE', color='blue')\n",
    "ax2.set(title='Lasso Cross-Validation Values', xlabel='Alpha', ylabel='MSE', ylim=[1400, 4000])\n",
    "\n",
    "# Highlight the best alpha for Lasso\n",
    "ax2.axvline(best_alpha_lasso, color='red', lw=2, linestyle='--', label=f'Best Alpha (Lasso) = {best_alpha_lasso}')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6pg1ZrIcV449"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "import numpy as np\n",
    "\n",
    "# Linear Regression\n",
    "linear_reg = LinearRegression()\n",
    "linear_reg.fit(X_train, y_train)\n",
    "linear_train_pred = linear_reg.predict(X_train)\n",
    "linear_test_pred = linear_reg.predict(X_test)\n",
    "\n",
    "# Ridge Regression\n",
    "ridge_reg = Ridge(alpha=1)  # Alpha parameter controls regularization strength - adjust for tuning\n",
    "ridge_reg.fit(X_train, y_train)\n",
    "ridge_train_pred = ridge_reg.predict(X_train)\n",
    "ridge_test_pred = ridge_reg.predict(X_test)\n",
    "\n",
    "# Lasso Regression\n",
    "lasso_reg = Lasso(alpha=1)  # Alpha parameter controls regularization strength - adjust for tuning\n",
    "lasso_reg.fit(X_train, y_train)\n",
    "lasso_train_pred = lasso_reg.predict(X_train)\n",
    "lasso_test_pred = lasso_reg.predict(X_test)\n",
    "\n",
    "# Print results without coefficients for brevity\n",
    "print(\"Linear Regression - Training set:\")\n",
    "print(\"Mean Squared Error (MSE):\", f\"{evaluate_performance(y_train, linear_train_pred)[0]:.3f}\")\n",
    "print(\"R-squared:\", f\"{evaluate_performance(y_train, linear_train_pred)[1]:.3f}\")\n",
    "print(\"\\nLinear Regression - Test set:\")\n",
    "print(\"Mean Squared Error (MSE):\", f\"{evaluate_performance(y_test, linear_test_pred)[0]:.3f}\")\n",
    "print(\"R-squared:\", f\"{evaluate_performance(y_test, linear_test_pred)[1]:.3f}\")\n",
    "print(30*'_')\n",
    "\n",
    "print(\"\\nRidge Regression - Training set:\")\n",
    "print(\"Mean Squared Error (MSE):\", f\"{evaluate_performance(y_train, ridge_train_pred)[0]:.3f}\")\n",
    "print(\"R-squared:\", f\"{evaluate_performance(y_train, ridge_train_pred)[1]:.3f}\")\n",
    "print(\"\\nRidge Regression - Test set:\")\n",
    "print(\"Mean Squared Error (MSE):\", f\"{evaluate_performance(y_test, ridge_test_pred)[0]:.3f}\")\n",
    "print(\"R-squared:\", f\"{evaluate_performance(y_test, ridge_test_pred)[1]:.3f}\")\n",
    "print(30*'_')\n",
    "\n",
    "print(\"\\nLasso Regression - Training set:\")\n",
    "print(\"Mean Squared Error (MSE):\", f\"{evaluate_performance(y_train, lasso_train_pred)[0]:.3f}\")\n",
    "print(\"R-squared:\", f\"{evaluate_performance(y_train, lasso_train_pred)[1]:.3f}\")\n",
    "print(\"\\nLasso Regression - Test set:\")\n",
    "print(\"Mean Squared Error (MSE):\", f\"{evaluate_performance(y_test, lasso_test_pred)[0]:.3f}\")\n",
    "print(\"R-squared:\", f\"{evaluate_performance(y_test, lasso_test_pred)[1]:.3f}\")\n",
    "print(30*'_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2bMMJilhWZCo"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a DataFrame for coefficients\n",
    "coefficients_data = {\n",
    "    'Linear Regression': linear_reg.coef_,\n",
    "    'Ridge Regression': ridge_reg.coef_,\n",
    "    'Lasso Regression': lasso_reg.coef_\n",
    "}\n",
    "\n",
    "coefficients_df = pd.DataFrame(coefficients_data, index=[f'Feature {i}' for i in range(1, 5)])\n",
    "display(coefficients_df)\n",
    "\n",
    "# We also could see the intercepts:\n",
    "# print(linear_reg.intercept_)\n",
    "# print(ridge_reg.intercept_)\n",
    "# print(lasso_reg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J6hwRGF3X9KM"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Evaluate performance for both training and test sets\n",
    "models = ['Linear Regression', 'Ridge Regression', 'Lasso Regression']\n",
    "train_mse = [evaluate_performance(y_train, linear_train_pred)[0],\n",
    "             evaluate_performance(y_train, ridge_train_pred)[0],\n",
    "             evaluate_performance(y_train, lasso_train_pred)[0]]\n",
    "\n",
    "test_mse = [evaluate_performance(y_test, linear_test_pred)[0],\n",
    "            evaluate_performance(y_test, ridge_test_pred)[0],\n",
    "            evaluate_performance(y_test, lasso_test_pred)[0]]\n",
    "\n",
    "# Create a DataFrame\n",
    "mse_df = pd.DataFrame({\n",
    "    'Model': models * 2,\n",
    "    'MSE': train_mse + test_mse,\n",
    "    'Set': ['Train'] * len(models) + ['Test'] * len(models)\n",
    "})\n",
    "display(mse_df.set_index('Model').round(3))\n",
    "\n",
    "# Bar plot for MSE on both training and test sets\n",
    "# Create a figure and axes\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "# Bar plot for MSE on both training and test sets\n",
    "sns.barplot(x='Model', y='MSE', hue='Set', data=mse_df, ec = 'k',\n",
    "            palette={'Train': '#329932', 'Test': '#3232FF'}, ax=ax)\n",
    "ax.set_title('Mean Squared Error (MSE) on Training and Test Sets', weight = 'bold')\n",
    "ax.set_ylabel('MSE')\n",
    "\n",
    "# Move legend outside the right side of the plot\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1, 0.99))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FlsHGQaJkbw"
   },
   "source": [
    "# Linear Models: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2kqy0vzQxla"
   },
   "source": [
    "## Logistic Regression vs Linear Regression: Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wQ3KL2vdAhkl"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# from scipy.special import expit\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "\n",
    "mu, sigma = 10, 2  # mean and standard deviation\n",
    "np.random.seed(0)\n",
    "X = np.random.normal(mu, sigma, 100)\n",
    "y = np.where(X > mu, 1, 0)\n",
    "X = X.reshape(-1, 1)\n",
    "X_gen = np.linspace(X.min(), X.max(), 300)\n",
    "\n",
    "# Create the models\n",
    "models = [LinearRegression(), LogisticRegression()]\n",
    "\n",
    "# Titles for the plots\n",
    "titles = [ \"Linear Regression Model\", \"Logistic Regression Model\"]\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(9.5, 7), sharex=True, gridspec_kw={'hspace': 0})\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    model = models[i]\n",
    "    model.fit(X, y)\n",
    "\n",
    "    if i == 1:\n",
    "        Px = (1/(1 + np.exp(- (X_gen * model.coef_ + model.intercept_)))).flatten()\n",
    "        # Or we could utilize scipy.special.expit\n",
    "        # Px = expit(X_gen * model.coef_ + model.intercept_).ravel()\n",
    "    else:\n",
    "        Px = model.coef_ * X_gen + model.intercept_\n",
    "\n",
    "    ax.scatter(X, y, color='#b496ff', ec='#602ce5', alpha=0.3, label=\"Sample Data\")\n",
    "    ax.plot(X_gen, Px, color='#C60004', lw=2, label=titles[i])\n",
    "    ax.set_xlim([4, 16])\n",
    "    ax.hlines([0, 1], *ax.get_xlim(), linestyles='dashed', color = '#26912d', lw=1.5)\n",
    "    ax.set(xlabel='' if i == 0 else 'X', ylabel='Probability(X)')\n",
    "    ax.grid(False)\n",
    "    ax.legend(loc = 'right')\n",
    "    ax.set_ylim(-0.1, 1.1)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gE0gtLzuTN-J"
   },
   "source": [
    "## Logistic Regression: Example\n",
    "\n",
    "\n",
    "<font color='Blue'><b>Example</b></font>: The synthetic dataset is generated using the `make_blobs` function from scikit-learn. This particular dataset consists of the following characteristics:\n",
    "\n",
    "- **Number of Samples:** 1000\n",
    "- **Number of Features:** 2\n",
    "- **Number of Classes:** 2\n",
    "- **Random Seed (random_state):** 0\n",
    "- **Cluster Standard Deviation (cluster_std):** 1.0\n",
    "\n",
    "**Features:**\n",
    "- The dataset contains 1000 data points, each described by a pair of feature values. These features are represented as 'Feature 1' and 'Feature 2'.\n",
    "\n",
    "**Outcome (Target Variable):**\n",
    "- The dataset also includes a target variable called 'Outcome.' This variable assigns each data point to one of two distinct classes, identified as 'Class 0' and 'Class 1'.\n",
    "\n",
    "The dataset has been designed to simulate a scenario with two well-separated clusters, making it suitable for binary classification tasks. Each data point in this dataset is associated with one of the two classes, and it can be used for practicing and evaluating machine learning algorithms that deal with binary classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RdAgY9neTNaa"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_blobs(n_samples=1000, centers=2, random_state=0, cluster_std=1.0)\n",
    "\n",
    "# Create a scatter plot using Seaborn\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "\n",
    "colors = [\"#f5645a\", \"#b781ea\"]\n",
    "markers = ['o', 's']\n",
    "\n",
    "# Scatter plot of data points\n",
    "for num in np.unique(y):\n",
    "    ax.scatter(X[:, 0][y == num], X[:, 1][y == num], c=colors[num],\n",
    "                s=40, edgecolors=\"k\", marker=markers[num], label=str(num))\n",
    "\n",
    "ax.set(xlim=[-2, 6], ylim=[-2, 8])\n",
    "ax.legend(title = 'Outcome')\n",
    "ax.set(xlabel='Feature 1', ylabel='Feature 2')\n",
    "ax.set_title('Synthetic Dataset', weight = 'bold', fontsize = 16)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwtialhigXBS"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.25, random_state=0)\n",
    "\n",
    "print(f'Shape of X_train = {X_train.shape}')\n",
    "print(f'Shape of y_train = {y_train.shape}')\n",
    "print(f'Shape of X_test = {X_test.shape}')\n",
    "print(f'Shape of y_test = {y_test.shape}')\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "_ = log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ED2GV5QJTSok"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "colors = [\"#f5645a\", \"#b781ea\"]\n",
    "edge_colors = ['#8A0002', '#3C1F8B']\n",
    "cmap_light = ListedColormap(['#f7dfdf', '#e3d3f2'])\n",
    "markers = ['o', 's']\n",
    "\n",
    "# Plot decision boundaries\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 7))\n",
    "log_reg = LogisticRegression(max_iter = 200, solver = 'lbfgs')\n",
    "log_reg.fit(X, y)\n",
    "DecisionBoundaryDisplay.from_estimator(log_reg, X, cmap=cmap_light, ax=ax,\n",
    "                                   response_method=\"predict\",\n",
    "                                   plot_method=\"pcolormesh\",\n",
    "                                   xlabel= 'Feature 1', ylabel='Feature 2',\n",
    "                                   shading=\"auto\")\n",
    "# Scatter plot of data points\n",
    "for num in np.unique(y):\n",
    "    ax.scatter(X[:, 0][y == num], X[:, 1][y == num], c=colors[num],\n",
    "                s=40, edgecolors=\"k\", marker=markers[num], label=str(num))\n",
    "\n",
    "ax.set(xlim=[-2, 6], ylim=[-2, 8])\n",
    "ax.legend(title = 'Outcome')\n",
    "ax.set(xlim=[-2, 6], ylim=[-2, 8])\n",
    "ax.set_title(f'Logistic Regression', fontweight='bold', fontsize = 16)\n",
    "ax.grid(False)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LQIVndDrTWLR"
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(9.5, 4.5))\n",
    "\n",
    "# Create a loop for train and test sets\n",
    "for i, (X_set, y_set, title) in enumerate([(X_train, y_train, 'Train Set'), (X_test, y_test, 'Test Set')]):\n",
    "    # Plot decision boundaries\n",
    "    DecisionBoundaryDisplay.from_estimator(log_reg, X_set, cmap=cmap_light, ax=axes[i],\n",
    "                                           response_method=\"predict\",\n",
    "                                           plot_method=\"pcolormesh\",\n",
    "                                           xlabel='Feature 1', ylabel='Feature 2',\n",
    "                                           shading=\"auto\")\n",
    "    for num in np.unique(y):\n",
    "        axes[i].scatter(X_set[:, 0][y_set == num],\n",
    "                     X_set[:, 1][y_set == num], c=colors[num],\n",
    "                    s=40, edgecolors=\"k\", marker=markers[num], label= f'Outcome {num}')\n",
    "\n",
    "    # Plot data points where y_set and log_reg(X_set) differ in color\n",
    "    axes[i].scatter(X_set[:, 0][y_set != log_reg.predict(X_set)],\n",
    "                    X_set[:, 1][y_set != log_reg.predict(X_set)],\n",
    "                    fc='Yellow', ec='black', s=40, marker= 'h', label= 'Misclassified')\n",
    "\n",
    "    axes[i].set_title(f'{title} - Logistic Regression', fontweight='bold', fontsize=16)\n",
    "    axes[i].grid(False)\n",
    "    # Remove the legend for each panel\n",
    "    axes[i].legend()\n",
    "    axes[i].get_legend().remove()\n",
    "\n",
    "# Create a single legend for both subplots at the top\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper center', ncol=3, borderaxespad= -0.2)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZdYkbPsq52Y"
   },
   "source": [
    "## Classification Accuracy Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T5OYPugKq6cv"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # Import matplotlib for plotting\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix  # Import necessary functions/classes\n",
    "\n",
    "def format_confusion_matrix(cm, title):\n",
    "    true_neg, false_pos, false_neg, true_pos = cm.ravel()\n",
    "    result = f\"\\033[1m{title} Set Confusion Matrix\\033[0m:\\n\"\n",
    "    result += f\"- {true_neg} instances were correctly predicted as class 0.\\n\"\n",
    "    result += f\"- {true_pos} instances were correctly predicted as class 1.\\n\"\n",
    "    result += f\"- {false_pos} instance was incorrectly predicted as class 1 when it was actually class 0.\\n\"\n",
    "    result += f\"- {false_neg} instances were incorrectly predicted as class 0 when they were actually class 1.\\n\"\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def plot_cm(model, X_train, X_test, y_train, y_test, class_names, figsize=(7, 4)):\n",
    "    # Create a figure and axes for displaying confusion matrices side by side\n",
    "    fig, ax = plt.subplots(1, 2, figsize=figsize)\n",
    "\n",
    "    datasets = [(X_train, y_train, 'Train'), (X_test, y_test, 'Test')]\n",
    "\n",
    "    for i in range(2):\n",
    "        X, y, dataset_name = datasets[i]\n",
    "\n",
    "        # Compute confusion matrix for the dataset predictions\n",
    "        cm = confusion_matrix(y, model.predict(X))\n",
    "\n",
    "        result = format_confusion_matrix(cm, dataset_name)\n",
    "        print(result)\n",
    "\n",
    "        # Create a ConfusionMatrixDisplay and plot it on the respective axis\n",
    "        cm_display = ConfusionMatrixDisplay(cm, display_labels=class_names)\\\n",
    "                        .plot(ax=ax[i],\n",
    "                              im_kw=dict(cmap='Greens' if dataset_name == 'Train' else 'Blues'),\n",
    "                              text_kw={\"size\": 16}, colorbar=False)\n",
    "        ax[i].set_title(f'{dataset_name} Data')\n",
    "        ax[i].grid(False)\n",
    "\n",
    "    # Add a super title for the entire figure\n",
    "    fig.suptitle('Confusion Matrices', fontsize=16, weight = 'bold')\n",
    "\n",
    "    # Adjust the layout for better spacing\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lrAGTC42rHt_"
   },
   "outputs": [],
   "source": [
    "plot_cm(log_reg, X_train, X_test, y_train, y_test, ['0', '1'], figsize=(6, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RzlC2uJZq6tN"
   },
   "outputs": [],
   "source": [
    "def _gen_cr(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    Results = pd.DataFrame(classification_report(y, y_pred,\n",
    "                                             output_dict=True)).T\n",
    "    display(Results.style.format(precision = 3))\n",
    "\n",
    "print('\\nTrain Data:')\n",
    "_gen_cr(log_reg, X_train, y_train)\n",
    "\n",
    "print('\\nTest Data:')\n",
    "_gen_cr(log_reg, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-fb0SsulsZFL"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have X and y defined earlier\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.25, random_state=0)\n",
    "\n",
    "print(f'Shape of X_train = {X_train.shape}')\n",
    "print(f'Shape of y_train = {y_train.shape}')\n",
    "print(f'Shape of X_test = {X_test.shape}')\n",
    "print(f'Shape of y_test = {y_test.shape}')\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "_ = log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities on train and test sets\n",
    "y_train_probs = log_reg.predict_proba(X_train)[:, 1]\n",
    "y_test_probs = log_reg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate ROC curve and AUC for train set\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_probs)\n",
    "roc_auc_train = auc(fpr_train, tpr_train)\n",
    "\n",
    "# Calculate ROC curve and AUC for test set\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, y_test_probs)\n",
    "roc_auc_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "# Plot ROC curves using fig and ax\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot ROC curve for train set\n",
    "axs[0].plot(fpr_train, tpr_train, label=f'Train ROC Curve (AUC = {roc_auc_train:.3f})', color='green', lw = 3)\n",
    "axs[0].set_title('Receiver Operating Characteristic (ROC) - Train Set')\n",
    "\n",
    "# Plot ROC curve for test set\n",
    "axs[1].plot(fpr_test, tpr_test, label=f'Test ROC Curve (AUC = {roc_auc_test:.3f})', color='blue', lw = 3)\n",
    "axs[1].set_title('Receiver Operating Characteristic (ROC) - Test Set')\n",
    "\n",
    "for ax in axs:\n",
    "  ax.plot([0, 1], [0, 1], color='red', linestyle='--', lw = 2)\n",
    "  ax.set_xlabel('False Positive Rate')\n",
    "  ax.set_ylabel('True Positive Rate')\n",
    "  ax.legend()\n",
    "\n",
    "# Adjust layout and display the plots\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
