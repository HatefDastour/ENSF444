{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install scikit-learn --upgrade --force-reinstall"
      ],
      "metadata": {
        "id": "vd9oiSPfCsOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning\n"
      ],
      "metadata": {
        "id": "bVg6G22aB1eK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting a Dataset into Train and Test sets\n",
        "\n",
        "In the context of Machine Learning, there are two main approaches for splitting a dataset into train and test sets.\n",
        "\n",
        "1. The first one is to use a simple train-test split, where you randomly divide the data into two subsets: one for training the model and one for evaluating its performance.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/HatefDastour/ENSF444/12326debd66856cf557346ba47e221f0a43aa55a/Images/TrainTest_Split.png\" alt=\"picture\" width=\"900\">\n",
        "<br>\n",
        "<b>Figure</b>: Train and Test splitting of a dataset.\n",
        "</center>\n",
        "\n",
        "2. The second one is to use a train-validation-test split, where you further split the training set into two subsets: one for training the model and one for tuning its hyperparameters. The test set is only used for the final evaluation of the model.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/HatefDastour/ENSF444/12326debd66856cf557346ba47e221f0a43aa55a/Images/TrainValTest_Split.png\" alt=\"picture\" width=\"900\">\n",
        "<br>\n",
        "<b>Figure</b>: Train, Validation and Test splitting of a dataset.\n",
        "</center>\n",
        "\n",
        "The advantage of the second approach is that it allows you to optimize the model's parameters without using the test set, which reduces the risk of overfitting and improves the generalization ability of the model. The disadvantage is that it reduces the amount of data available for training the model, which may affect its accuracy."
      ],
      "metadata": {
        "id": "sfkBvQKf4FQS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Searching for Optimal Hyperparameters\n",
        "\n",
        "<font color='Blue'><b>Example:</b></font>"
      ],
      "metadata": {
        "id": "TZYYDr3W-EeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_blobs\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "colors = [\"#f5645a\", \"#0096ff\", '#B2FF66']\n",
        "edge_colors = ['#8A0002', '#2e658c', '#6A993D']\n",
        "markers = ['o', '*', 's']\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_class_distribution(ax, y,\n",
        "                            colors = colors,\n",
        "                            edge_colors = edge_colors,\n",
        "                            title = 'Class Distribution'):\n",
        "\n",
        "    # Calculate bar heights and labels\n",
        "    bar_heights, bar_labels = np.unique(y, return_counts=True)\n",
        "\n",
        "    # Create bars\n",
        "    bars = ax.bar(bar_heights, bar_labels, color=colors, edgecolor=edge_colors)\n",
        "\n",
        "    # Add xticks with labels\n",
        "    ax.set_xticks(bar_heights)\n",
        "    ax.set_xticklabels(map(str, bar_heights))\n",
        "\n",
        "    # Add grid and title\n",
        "    ax.grid(which='major', axis='y')\n",
        "    ax.set_title(title, weight='bold', fontsize=16, y=1.02)\n",
        "\n",
        "    # Add labels for bar heights inside each bar\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        percentage = 100 * height / len(y)\n",
        "        ax.annotate(f'{percentage:.1f}%', xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                    xytext=(0, 3), textcoords='offset points', ha='center', fontsize=12)\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y = make_blobs(n_samples = [500, 300, 200],\n",
        "                  centers=[[0, 0], [2, 2], [4, 4]],\n",
        "                  n_features=2,\n",
        "                  random_state=0, cluster_std=[1.0, 1, .6])\n",
        "\n",
        "# Create a scatter plot using Seaborn\n",
        "fig, ax = plt.subplots(1, 2, figsize=(10, 7), gridspec_kw={'width_ratios': [8, 2]})\n",
        "\n",
        "for num in np.unique(y):\n",
        "    ax[0].scatter(X[:, 0][y == num], X[:, 1][y == num], c=colors[num],\n",
        "               s=40, ec=edge_colors[num], marker=markers[num], label=str(num))\n",
        "\n",
        "ax[0].grid(True)\n",
        "ax[0].legend(title='Class', fontsize=14)\n",
        "ax[0].set(xlim = [-4, 6], ylim = [-4, 6], xlabel = 'Feature 1', ylabel = 'Feature 2')\n",
        "ax[0].set_title('Synthetic Dataset', weight = 'bold', fontsize = 16, y = 1.02)\n",
        "\n",
        "plot_class_distribution(ax[1], y, colors = colors, edge_colors = edge_colors)\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "ssMQgqto2UFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the dataset into train+validation set and test set\n",
        "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, stratify=y, random_state=0)\n",
        "\n",
        "# Further split the train+validation set into training and validation sets\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_trainval, y_trainval, stratify=y_trainval, random_state=0)\n",
        "\n",
        "# Print the sizes of the training, validation, and test sets\n",
        "print(\"Size of training set: {}   size of validation set: {}   size of test set: {}\"\n",
        "      \"\\n\".format(X_train.shape[0], X_valid.shape[0], X_test.shape[0]))\n",
        "\n",
        "# Create subplots for visualizing class distribution\n",
        "fig, ax = plt.subplots(1, 3, figsize=(10, 5), sharey=True)\n",
        "\n",
        "# Iterate over train, validation, and test sets for plotting class distribution\n",
        "for ax, y_set, title, textcolor in zip(ax, [y_train, y_valid, y_test], ['Train', 'Validation', 'Test'],\n",
        "                                       ['#0f4539', '#ff9317', '#2986cc']):\n",
        "    # Plot the class distribution on the current subplot\n",
        "    plot_class_distribution(ax=ax, y=y_set, title=title)\n",
        "\n",
        "    # Set y-axis limits for better visualization\n",
        "    ax.set_ylim([0, 300])\n",
        "\n",
        "    # Set subplot title with specified font properties\n",
        "    ax.set_title(title, fontdict={'size': 12, 'weight': 'bold', 'color': textcolor})\n",
        "\n",
        "# Set the overall title for the entire figure\n",
        "fig.suptitle('Class Distribution', weight='bold', fontsize=12)\n",
        "\n",
        "# Adjust layout for better visualization\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "8Wn0JlZW-sW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from pprint import pprint\n",
        "\n",
        "# Instantiate a RandomForestClassifier with a specified random seed\n",
        "rfc = RandomForestClassifier(random_state=0)\n",
        "\n",
        "# Train the model on the combined Train and Validation Sets\n",
        "rfc.fit(X_trainval, y_trainval)\n",
        "\n",
        "# Calculate accuracy scores for the combined Train and Validation Sets and the Test Set\n",
        "train_score = rfc.score(X_trainval, y_trainval)\n",
        "test_score = rfc.score(X_test, y_test)\n",
        "\n",
        "# Print accuracy scores\n",
        "print(f\"Accuracy Score (Train and Validation Sets combined): {train_score:.4f}\")\n",
        "print(f\"Accuracy Score (Test Set): {test_score:.4f}\")\n",
        "\n",
        "# Display default parameters of the RandomForestClassifier\n",
        "print('\\nDefault Parameters:')\n",
        "pprint(rfc.get_params(deep=True))"
      ],
      "metadata": {
        "id": "NShsp3zlEoFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Initialize variables to track the best score and corresponding parameters\n",
        "best_score = 0\n",
        "best_parameters = {}\n",
        "\n",
        "# Define the parameter grid for Random Forest\n",
        "param_grid = dict(n_estimators=[100, 150, 200],\n",
        "                  max_depth=[3, 5, 7])\n",
        "\n",
        "# Iterate over combinations of n_estimators and max_depth\n",
        "for n_estimators in param_grid['n_estimators']:\n",
        "    for max_depth in param_grid['max_depth']:\n",
        "        # Instantiate a RandomForestClassifier with the current parameters\n",
        "        rfc = RandomForestClassifier(n_estimators=n_estimators,\n",
        "                                     max_depth=max_depth,\n",
        "                                     random_state=0)\n",
        "\n",
        "        # Train the model on the training set\n",
        "        rfc.fit(X_train, y_train)\n",
        "\n",
        "        # Evaluate the model on the validation set\n",
        "        score = rfc.score(X_valid, y_valid)\n",
        "\n",
        "        # If the current score is better than the best score, update best score and parameters\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_parameters = {'n_estimators': n_estimators, 'max_depth': max_depth}\n",
        "\n",
        "# Print the best score and corresponding parameters on the validation set\n",
        "print(\"Best score on validation set: {:.4f}\".format(best_score))\n",
        "print(\"Best parameters: \", best_parameters)"
      ],
      "metadata": {
        "id": "_NQ-HYHQCfI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rebuild a Random Forest model on the combined training and validation set\n",
        "# using the best parameters obtained from the parameter search\n",
        "rfc = RandomForestClassifier(random_state=0, **best_parameters)\n",
        "\n",
        "# Train the model on the combined Train and Validation Sets\n",
        "rfc.fit(X_trainval, y_trainval)\n",
        "\n",
        "# Evaluate the model on the training and test sets\n",
        "train_score = rfc.score(X_trainval, y_trainval)\n",
        "test_score = rfc.score(X_test, y_test)\n",
        "\n",
        "# Print accuracy scores for the combined Train and Validation Sets and the Test Set\n",
        "print(f\"Accuracy Score (Train and Validation Sets combined): {train_score:.4f}\")\n",
        "print(f\"Accuracy Score (Test Set): {test_score:.4f}\")\n",
        "\n",
        "# Display the current parameters of the Random Forest model\n",
        "print('\\nCurrent Parameters:')\n",
        "pprint(rfc.get_params(deep=True))"
      ],
      "metadata": {
        "id": "aw7z0H_CELpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The “best” parameters may not actually be the best parameters. These are the best parameters that we have found based on the search. If we add more details to our search, we may find better hyperparameters (better in terms of overall accuracy)."
      ],
      "metadata": {
        "id": "ECggGv9NYGDR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross-validated Grid-search\n",
        "\n",
        "GridSearchCV is a function that performs **hyperparameter optimization** by training and evaluating a machine learning model using different combinations of hyperparameters. The best set of hyperparameters is then selected based on a specified performance metric.\n",
        "\n",
        "GridSearchCV has two main advantages:\n",
        "\n",
        "- It applies a **grid search** to an array of hyperparameters, which means it tries every possible combination of the values you specify for each hyperparameter.\n",
        "- It **cross-validates** your model using **k-fold cross-validation**, which means it splits your data into k subsets and uses one subset as the test set and the rest as the training set. It repeats this process k times, each time using a different subset as the test set. This could help to avoid overfitting and gives a more reliable estimate of the model's performance.\n",
        "\n",
        "To use GridSearchCV, you need to pass the following parameters:\n",
        "\n",
        "- **estimator**: the machine learning model you want to tune, such as a classifier or a regressor.\n",
        "- **param_grid**: a dictionary or a list of dictionaries with the hyperparameter's names as keys and the values to try as values.\n",
        "- **scoring**: a string, a callable, a list, or a dictionary that defines the metric or metrics to use to evaluate the model's performance.\n",
        "- **n_jobs**: an integer that specifies the number of parallel jobs to run. -1 means using all processors.\n",
        "- **refit**: a boolean, a string, or a callable that determines whether to refit the model with the best-found parameters on the whole dataset and make it available as the **best_estimator_** attribute.\n",
        "\n",
        "<font color='Blue'><b>Example:</b></font>"
      ],
      "metadata": {
        "id": "okEtV98JdtoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=0)\n",
        "\n",
        "# Create Gradient Boosting Classifier instance\n",
        "rfc = RandomForestClassifier(random_state=0)\n",
        "\n",
        "# Define the parameter grid for Random Forest\n",
        "param_grid = dict(n_estimators=[100, 150, 200],\n",
        "                  max_depth=[3, 5, 7])\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator = rfc,\n",
        "                           param_grid = param_grid,\n",
        "                           cv=5,\n",
        "                           scoring='accuracy')\n",
        "\n",
        "# Perform grid search on the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Display the best parameters and corresponding accuracy score\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(f\"Best Accuracy Score on Training Data: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_accuracy = grid_search.best_estimator_.score(X_test, y_test)\n",
        "print(f\"Accuracy Score on Test Data: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "Qs0YU4smg-WD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# convert to DataFrame\n",
        "results = pd.DataFrame(grid_search.cv_results_).sort_values(by = 'rank_test_score').reset_index(drop = True)\n",
        "display(results)"
      ],
      "metadata": {
        "id": "-Zldupo3mNr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rebuild a Random Forest model using the best parameters obtained from the parameter search\n",
        "rfc = RandomForestClassifier(random_state=0, **grid_search.best_params_)\n",
        "\n",
        "# Train the model\n",
        "rfc.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the training and test sets\n",
        "train_score = rfc.score(X_train, y_train)\n",
        "test_score = rfc.score(X_test, y_test)\n",
        "\n",
        "# Print accuracy scores for the combined Train and Validation Sets and the Test Set\n",
        "print(f\"Accuracy Score (Train Set): {train_score:.4f}\")\n",
        "print(f\"Accuracy Score (Test Set): {test_score:.4f}\")\n",
        "\n",
        "# Display the current parameters of the Random Forest model\n",
        "print('\\nCurrent Parameters:')\n",
        "pprint(rfc.get_params(deep=True))"
      ],
      "metadata": {
        "id": "w5YxFIcEFP0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Halving Grid Search CV\n",
        "\n",
        "HalvingGridSearchCV is a scikit-learn estimator that performs a grid search over specified parameter values with successive halving. Successive halving is a search strategy that starts evaluating all the candidates with a small amount of resources and iteratively selects the best candidates, using more and more resources. This can be much faster than a regular grid search, especially when the number of candidates is large.\n",
        "\n",
        "The hyperparameters of HalvingGridSearchCV are:\n",
        "\n",
        "- `estimator`: the estimator object that implements the scikit-learn estimator interface. It must have a score function or a scoring parameter must be provided.\n",
        "- `param_grid`: a dictionary or a list of dictionaries that define the parameter names and values to try. Each dictionary corresponds to a grid of parameter combinations to explore.\n",
        "- `factor`: the halving parameter that determines the proportion of candidates that are selected for each subsequent iteration. For example, factor=3 means that only one third of the candidates are selected.\n",
        "- `resource`: the resource that increases with each iteration. By default, it is the number of samples, but it can also be any parameter of the base estimator that accepts positive integer values, such as `n_iterations` or `n_estimators`.\n",
        "- `max_resources`: the maximum amount of resource that any candidate is allowed to use for a given iteration. By default, it is set to the number of samples when resource='n_samples', otherwise it must be specified explicitly.\n",
        "- `min_resources`: the minimum amount of resource that any candidate is allowed to use for a given iteration. It can be either 'exhaust', which sets it such that the last iteration uses as much resources as possible, or 'smallest', which sets it to a small heuristic value based on the problem type and the number of splits, or an integer value.\n",
        "- `aggressive_elimination`: a boolean flag that determines whether to eliminate candidates that do not meet the required rate of improvement. If True, it can speed up the search, but it can also introduce some variability in the results.\n",
        "- `cv`: the cross-validation scheme to use for splitting the data. It can be either an integer, a cross-validation generator, or an iterable of train/test splits.\n",
        "- `scoring`: the scoring function or a list of scoring functions to evaluate the candidates. It can be either a string, a callable, or None, in which case the estimator's score function is used.\n",
        "- `refit`: a boolean flag or a string that determines whether to refit the estimator using the best found parameters. If True, the whole dataset is used for refitting. If a string, it must be the name of a scoring function, and the best parameters are chosen based on that score. If False, no refitting is done and the estimator is not available at the end.\n",
        "- `error_score`: the value to assign to the score if an error occurs during fitting. It can be either 'raise', which will raise the error, or a numeric value, which will be used as the score.\n",
        "- `return_train_score`: a boolean flag that determines whether to return the training scores along with the test scores.\n",
        "- `random_state`: the random state or seed to use for reproducibility. It can be either an integer, a RandomState instance, or None, in which case the global random state is used.\n",
        "- `n_jobs`: the number of jobs to run in parallel. It can be either an integer, or None, in which case one job is used.\n",
        "- `verbose`: the verbosity level. It can be either an integer, or None, in which case the default verbosity is used."
      ],
      "metadata": {
        "id": "GIoui5tTeCIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# On Google Colab, recommend to upgrade sklearn to 1.4,1\n",
        "from sklearn.experimental import enable_halving_search_cv\n",
        "from sklearn.model_selection import HalvingGridSearchCV, train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=0)\n",
        "\n",
        "# Create Random Forest Classifier instance\n",
        "rfc = RandomForestClassifier(random_state=0)\n",
        "\n",
        "# Define the parameter grid for Random Forest\n",
        "param_grid = dict(n_estimators=[100, 150, 200],\n",
        "                  max_depth=[3, 5, 7])\n",
        "\n",
        "# Initialize HalvingGridSearchCV\n",
        "halving_grid_search = HalvingGridSearchCV(estimator = rfc,\n",
        "                                          param_grid = param_grid,\n",
        "                                          cv=5,\n",
        "                                          return_train_score = True,\n",
        "                                          scoring='accuracy',\n",
        "                                          random_state = 1)\n",
        "\n",
        "# Perform halving grid search on the training data\n",
        "halving_grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Display the best parameters and corresponding accuracy score\n",
        "print(\"Best Parameters:\", halving_grid_search.best_params_)\n",
        "print(f\"Best Accuracy Score on Training Data: {halving_grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_accuracy = halving_grid_search.best_estimator_.score(X_test, y_test)\n",
        "print(f\"Accuracy Score on Test Data: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "NMAthPolCOga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = pd.DataFrame(halving_grid_search.cv_results_)\n",
        "display(results)"
      ],
      "metadata": {
        "id": "l1qVS9igQ08x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rebuild a Random Forest model using the best parameters obtained from the parameter search\n",
        "rfc = RandomForestClassifier(random_state=0, **halving_grid_search.best_params_)\n",
        "\n",
        "# Train the model\n",
        "rfc.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the training and test sets\n",
        "train_score = rfc.score(X_train, y_train)\n",
        "test_score = rfc.score(X_test, y_test)\n",
        "\n",
        "# Print accuracy scores for the combined Train and Validation Sets and the Test Set\n",
        "print(f\"Accuracy Score (Train Set): {train_score:.4f}\")\n",
        "print(f\"Accuracy Score (Test Set): {test_score:.4f}\")\n",
        "\n",
        "# Display the current parameters of the Random Forest model\n",
        "print('\\nCurrent Parameters:')\n",
        "pprint(rfc.get_params(deep=True))"
      ],
      "metadata": {
        "id": "IAEUxpPzFDjM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}