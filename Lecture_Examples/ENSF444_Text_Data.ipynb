{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WMfms9RqCv9"
   },
   "source": [
    "# Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0KhlTUN0loL"
   },
   "source": [
    "## Types of Datasets\n",
    "   - **Numerical Dataset:** Contains measurable quantities and can be analyzed mathematically. Examples include temperature, humidity, and test scores.\n",
    "   - **Categorical Dataset:** Comprises a set of categories or groups. Examples are colors, product categories, and yes/no responses.\n",
    "   - **Time Series Dataset:** Captures data points at successive time intervals. Useful for analyzing trends over time.\n",
    "   - **Image Dataset:** Consists of image files, often used in computer vision tasks to identify patterns or objects.\n",
    "   - **Text Dataset:** Includes collections of words, sentences, or documents, typically analyzed for linguistic patterns or content.\n",
    "\n",
    "   and many more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGv7gQunOqRW"
   },
   "source": [
    "## Text Data\n",
    "\n",
    "1. **Understanding Text Data:**\n",
    "   - Text data is composed of sequences of characters, forming words, sentences, or paragraphs.\n",
    "   - It varies in length and complexity, often containing nuanced linguistic features.\n",
    "\n",
    "2. **Text vs. Categorical Data:**\n",
    "   - Strings of characters can represent different types of data.\n",
    "   - Categorical data is derived from a predefined set of options, such as 'red' or 'blue', 'yes' or 'no'.\n",
    "   - Text data, however, is more fluid, often forming meaningful phrases or sentences that convey complex ideas.\n",
    "\n",
    "3. **Analyzing Text: Corpus and Documents:**\n",
    "   - Text analysis typically involves examining a large body of text, known as a [corpus](https://en.wikipedia.org/wiki/Text_corpus).\n",
    "   - Within this corpus, each individual text entry, whether an article, social media post, or review, is termed a **document**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q74ClDrxPQk1"
   },
   "source": [
    "## What is bag-of-words?\n",
    "\n",
    "Bag-of-words is a technique in natural language processing where we ignore the structure of input text and focus solely on word occurrences. Itâ€™s like mentally holding a bag of words and counting how many times each word appears in a document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7EOXWuoQPz2y"
   },
   "source": [
    "## Steps for bag-of-words representation\n",
    "\n",
    "1. **Tokenization**:\n",
    "   - **Tokenization** is the process of splitting each document (text) into individual words or tokens.\n",
    "   - We achieve this by breaking the text at whitespace, punctuation marks, or other delimiters.\n",
    "\n",
    "2. **Vocabulary Building**:\n",
    "   - Next, we create a **vocabulary** containing all unique words (tokens) that appear in any of the documents.\n",
    "   - Each word is assigned a unique **index** (usually in alphabetical order).\n",
    "\n",
    "3. **Encoding**:\n",
    "   - For each document, we count how often each word from the vocabulary appears in that document.\n",
    "   - The resulting vector represents the word frequencies (counts) for that document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1q3i1R-7q36"
   },
   "source": [
    "## Implementing Bag-of-Words\n",
    "\n",
    "In the scikit-learn library, the [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) is utilized to transform text data into a bag-of-words representation.\n",
    "\n",
    "The `CountVectorizer` method standardizes all text data to lowercase, ensuring that words with identical spellings are recognized as the same token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwaO-Qz2v1K_"
   },
   "source": [
    "<font color='Blue'><b>Example:</b></font> Let's create a simple example of the Bag-of-Words (BoW) model using text related to Calgary. We'll follow the steps mentioned earlier:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lfKx3Ih0zK5"
   },
   "source": [
    "Each Tokenization can be done as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mVgOiI_H0OVN"
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Step 0: Collect Data\n",
    "# Define the documents\n",
    "docs = [\"Calgary is known for its annual Stampede.\",\n",
    "        \"The Calgary Tower offers stunning views of the city.\",\n",
    "        \"Calgary's weather can be unpredictable.\"\n",
    "        ]\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "tokenizer = vectorizer.build_analyzer()\n",
    "for (i, doc) in enumerate(docs, 1):\n",
    "    print(f'\\nDoc {i}:')\n",
    "    print(\"Original document:\")\n",
    "    print(doc)\n",
    "    print(\"Tokenized document:\")\n",
    "    print(tokenizer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U0NUjcLFv2jx"
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Step 0: Collect Data\n",
    "# Define the documents\n",
    "docs = [\"Calgary is known for its annual Stampede.\",\n",
    "        \"The Calgary Tower offers stunning views of the city.\",\n",
    "        \"Calgary's weather can be unpredictable.\"\n",
    "        ]\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the documents\n",
    "vectorizer.fit(docs)\n",
    "\n",
    "# Transform the documents into a bag-of-words matrix\n",
    "bag_of_words = vectorizer.transform(docs)\n",
    "\n",
    "# Get the feature names (vocabulary)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print('Feature Names (Vocabulary): ' + ', '.join(feature_names))\n",
    "\n",
    "# Display the vocabulary size and content\n",
    "print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
    "print(f\"Vocabulary content: {vectorizer.vocabulary_}\")\n",
    "\n",
    "# Display the dense representation of the bag_of_words\n",
    "# print(\"Dense representation of bag_of_words:\\n{}\".format(bag_of_words.toarray()))\n",
    "\n",
    "# Create a DataFrame from the BoW matrix\n",
    "df_bow = pd.DataFrame(bag_of_words.toarray(), columns=feature_names)\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKkejZAs2JSL"
   },
   "source": [
    "<font color='Blue'><b>Example:</b></font> Example: Bag-of-Words (BoW) Model Using Text Related to Calgary with **repeating words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j45W0xAg2GTe"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Define a list of sentences with repeating words related to the city of Calgary\n",
    "# This list will be used to demonstrate how the Bag-of-Words model handles repetition\n",
    "repeating_words = [\"Calgary, Calgary, a city so vibrant, so vibrant.\",\n",
    "                   \"The Rockies, the Rockies, so majestic, so majestic.\"\n",
    "                   ]\n",
    "\n",
    "# Initialize a CountVectorizer object to convert the text data into a matrix of token counts\n",
    "# The CountVectorizer will tokenize the text and count the occurrences of each token\n",
    "vect = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the list of sentences to build the vocabulary\n",
    "# The vocabulary consists of all unique tokens in the text\n",
    "vect.fit(repeating_words)\n",
    "\n",
    "# Display the vocabulary that has been learned from the input documents\n",
    "# The vocabulary is a dictionary where keys are tokens and values are their corresponding indices\n",
    "print(f\"Vocabulary learned from the documents: {vect.vocabulary_}\")\n",
    "\n",
    "# Transform the list of sentences into a bag-of-words matrix\n",
    "# Each row in the matrix represents a document, and each column represents a token from the vocabulary\n",
    "# The values in the matrix are the frequency counts of the tokens in each document\n",
    "bag_of_words = vect.transform(repeating_words)\n",
    "\n",
    "# Convert the bag-of-words matrix into a pandas DataFrame for better visualization\n",
    "# The DataFrame makes it easier to see the token frequencies in tabular form\n",
    "df_bow = pd.DataFrame(bag_of_words.toarray(), columns=vect.get_feature_names_out())\n",
    "\n",
    "# Display the DataFrame that shows the frequency of each word in the given sentences\n",
    "# This DataFrame is useful for understanding the distribution of tokens across the documents\n",
    "print(\"DataFrame showing the Bag-of-Words matrix:\")\n",
    "display(df_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-zHJMcW3nVr"
   },
   "source": [
    "## Enhancing Bag-of-Words: Stopword Removal\n",
    "\n",
    "In the Bag-of-Words (BoW) model, certain words are so common that they carry minimal useful information about the actual content of the document. These words, known as 'stopwords', can be removed to improve the analysis. There are two primary methods to eliminate stopwords:\n",
    "\n",
    "1. Utilizing a predefined list of stopwords specific to a language.\n",
    "2. Excluding words that appear too frequently across the documents.\n",
    "\n",
    "The scikit-learn library provides a built-in English stopword list in the `feature_extraction.text` module. This list can be used to filter out stopwords from the text data during the vectorization process, resulting in a more meaningful BoW representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "__VObSvv2OH2"
   },
   "outputs": [],
   "source": [
    "# Import the set of English stop words from scikit-learn's feature_extraction.text module\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# Display the total number of stop words provided in the scikit-learn's list\n",
    "print(f\"Number of stop words: {len(ENGLISH_STOP_WORDS)}\")\n",
    "\n",
    "# To provide a sample of this list, print every 20th stop word\n",
    "# This gives an idea of what kind of words are considered as stop words\n",
    "print(\"Every 20th stopword:\")\n",
    "print(list(ENGLISH_STOP_WORDS)[::20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yknRhNKz3pnJ"
   },
   "outputs": [],
   "source": [
    "docs_ext = docs + repeating_words\n",
    "vect = CountVectorizer(stop_words = \"english\")\n",
    "vect.fit(docs_ext)\n",
    "bag_of_words = vect.transform(docs_ext)\n",
    "pd.DataFrame(bag_of_words.toarray(), columns=vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtWleuve4xSp"
   },
   "source": [
    "## Understanding TfidfVectorizer\n",
    "\n",
    "`TfidfVectorizer` is an advanced feature extraction tool from the scikit-learn library that converts text data into a numerical matrix of TF-IDF features, suitable for use in machine learning models.\n",
    "\n",
    "- **TF (Term Frequency)**: This quantifies the frequency of a term within a single document, giving higher weight to terms that occur more frequently.\n",
    "\n",
    "- **IDF (Inverse Document Frequency)**: This assesses the significance of a term across the entire document corpus. It diminishes the weight of terms that occur very commonly across documents, thereby amplifying the importance of rarer terms. The IDF for a term is calculated by taking the logarithm of the ratio of the total number of documents to the number of documents containing the term.\n",
    "\n",
    "- **TF-IDF**: This metric is the product of TF and IDF, reflecting the term's significance within a particular document relative to the entire collection of documents. A term's TF-IDF score increases with its frequency in a document but is balanced by its commonality across all documents.\n",
    "\n",
    "<font color='Blue'><b>Example:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00oGy2pW45ix"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample extended documents related to Calgary\n",
    "docs_ext = docs + repeating_words\n",
    "\n",
    "# Initialize the TfidfVectorizer with English stop words to be filtered out\n",
    "vectorizer = TfidfVectorizer(stop_words = \"english\")\n",
    "\n",
    "# Fit the vectorizer to the documents to learn the vocabulary\n",
    "vectorizer.fit(docs_ext)\n",
    "\n",
    "# Display the size and content of the learned vocabulary\n",
    "print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
    "print(\"Vocabulary content:\")\n",
    "print(vectorizer.vocabulary_)\n",
    "\n",
    "# Transform the documents into a TF-IDF-weighted term-document matrix\n",
    "tfidf_matrix = vectorizer.transform(docs_ext)\n",
    "\n",
    "# Retrieve and display the feature names (vocabulary)\n",
    "print(\"\\nFeature names:\")\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the TF-IDF matrix as a dense array\n",
    "# print(\"\\nTF-IDF matrix:\")\n",
    "# print(tfidf_matrix.toarray())\n",
    "\n",
    "# Convert the TF-IDF matrix into a DataFrame for better readability\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"\\nDataFrame representation of the TF-IDF matrix:\")\n",
    "display(tfidf_df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFBJVax6OWch"
   },
   "source": [
    "The numbers in the table represent the **TF-IDF (Term Frequency-Inverse Document Frequency) scores** for various terms across different documents. Here's what they indicate:\n",
    "\n",
    "- **Columns**: Each column header represents a unique term (e.g., 'annual', 'calgary', 'city', etc.).\n",
    "- **Rows**: Each row corresponds to a different document (e.g., Document 0, Document 1, etc.).\n",
    "- **Values**: The numerical values are the TF-IDF scores, which measure how important a term is to a document in a collection. A score of **0** means the term does not appear in the document, while higher scores indicate greater importance.\n",
    "\n",
    "For example, in Document 0, the terms 'annual', 'known', and 'stampede' have a score of **0.549**, suggesting they are significant in that document. Conversely, the term 'city' has a score of **0**, indicating it does not appear in Document 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the TF-IDF score for the word \"Calgary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the index of the word 'Calgary' in the feature names\n",
    "word_index = vectorizer.vocabulary_.get('calgary')\n",
    "\n",
    "# Get the TF-IDF scores for 'Calgary' across all documents\n",
    "tfidf_scores = tfidf_matrix[:, word_index].toarray().flatten()\n",
    "\n",
    "# Print the TF-IDF scores for 'Calgary'\n",
    "print(f\"TF-IDF scores for 'Calgary': {tfidf_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = vectorizer.build_analyzer()\n",
    "for (i, doc) in enumerate(docs_ext, 1):\n",
    "    print(f'\\nDoc {i}:')\n",
    "    print(\"Original document:\")\n",
    "    print(doc)\n",
    "    print(\"Tokenized document:\")\n",
    "    print(tokenizer(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could calculate TF-IDF score from  CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "\n",
    "# Sample documents\n",
    "documents = docs + repeating_words\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words = \"english\")\n",
    "\n",
    "# Transform documents into term frequency matrix\n",
    "tf_matrix = count_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Calculate IDF\n",
    "num_docs, num_terms = tf_matrix.shape\n",
    "print(f'\\nNumber of Docs = {num_docs} and Number of Terms = {num_terms}')\n",
    "terms_in_docs = (tf_matrix > 0).sum(axis=0)\n",
    "print(f'\\nterms_in_docs')\n",
    "display(pd.DataFrame(terms_in_docs,columns = vect.get_feature_names_out()))\n",
    "\n",
    "idf = np.log((1 + num_docs) / (1 + terms_in_docs)) + 1\n",
    "print(f'\\nidf')\n",
    "\n",
    "display(pd.DataFrame(idf,columns = vect.get_feature_names_out()).round(3))\n",
    "# Apply IDF to TF matrix\n",
    "tfidf_matrix = tf_matrix.multiply(idf)\n",
    "\n",
    "# Normalize the TF-IDF matrix\n",
    "tfidf_matrix = normalize(tfidf_matrix, norm='l2', axis=1)\n",
    "\n",
    "# Convert to array and print\n",
    "tfidf_matrix = tfidf_matrix.toarray()\n",
    "print('\\ntf-idf matrix:')\n",
    "display(pd.DataFrame(tfidf_matrix, columns = vect.get_feature_names_out()).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5o6NfhWUBQzp"
   },
   "source": [
    "## Expanding Bag-of-Words with n-Grams\n",
    "\n",
    "The Bag-of-Words (BoW) model is a simple yet powerful way to represent text data for machine learning. However, it has a significant limitation: it ignores word order. For example, \"it's bad, not good at all\" and \"it's good, not bad at all\" would have identical BoW representations despite their opposite meanings. This is where n-grams come into play.\n",
    "\n",
    "### Capturing Context with n-Grams\n",
    "\n",
    "To capture more context, we can extend the BoW model to consider sequences of words:\n",
    "- **Bigrams**: Pairs of consecutive words.\n",
    "- **Trigrams**: Triplets of consecutive words.\n",
    "- **n-Grams**: Sequences of 'n' consecutive words.\n",
    "\n",
    "\n",
    "### Implementing n-Grams with CountVectorizer\n",
    "\n",
    "The `CountVectorizer` and `TfidfVectorizer` classes in scikit-learn can be configured to use n-grams by setting the `ngram_range` parameter. Here's how you can do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8NRco_26BRDz"
   },
   "outputs": [],
   "source": [
    "docs = [\"Calgary is known for its annual Stampede.\",\n",
    "        \"The Calgary Tower offers stunning views of the city.\",\n",
    "        \"Calgary's weather can be unpredictable.\"\n",
    "        ]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1), stop_words = \"english\").fit(docs)\n",
    "# ngram_rangetuple (min_n, max_n), default=(1, 1)\n",
    "print(f\"Vocabulary size: {vectorizer.vocabulary_}\")\n",
    "print(f\"Vocabulary:\")\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "ngram_words = vectorizer.transform(docs)\n",
    "pd.DataFrame(ngram_words.toarray(), columns= vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UKMObZ8rBXLB"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2), stop_words = \"english\").fit(docs)\n",
    "print(f\"Vocabulary size: {vectorizer.vocabulary_}\")\n",
    "print(f\"Vocabulary:\")\n",
    "pprint(vectorizer.get_feature_names_out())\n",
    "\n",
    "ngram_words = vectorizer.transform(docs)\n",
    "pd.DataFrame(ngram_words.toarray(), columns= vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NoCR9I4XBeRx"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 2), stop_words = \"english\").fit(docs)\n",
    "print(f\"Vocabulary size: {vectorizer.vocabulary_}\")\n",
    "print(f\"Vocabulary:\")\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "ngram_words = vectorizer.transform(docs)\n",
    "pd.DataFrame(ngram_words.toarray(), columns= vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONqH_cC3CAIy"
   },
   "source": [
    "## Td-idf n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xKcPOIIPBr44"
   },
   "outputs": [],
   "source": [
    "docs = [\"Calgary is known for its annual Stampede.\",\n",
    "        \"The Calgary Tower offers stunning views of the city.\",\n",
    "        \"Calgary's weather can be unpredictable.\"\n",
    "        ]\n",
    "vect = TfidfVectorizer(ngram_range=(1,2), stop_words = \"english\")\n",
    "vect.fit(docs)\n",
    "tfidf_words = vect.transform(docs)\n",
    "df = pd.DataFrame(tfidf_words.toarray(), columns=vect.get_feature_names_out())\n",
    "display(df.round(3))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
