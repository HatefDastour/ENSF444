{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# The Best Sources for Public Datasets -- Part1\n",
        "\n",
        "-   **[Census Datasets - Statistics\n",
        "    Canada](https://www12.statcan.gc.ca/datasets/Index-eng.cfm):** A\n",
        "    portal that gives you access to selected datasets from the Canadian\n",
        "    census from 1981 to 2021.\n",
        "\n",
        "-   **[Data.gov](https://data.gov/):** The home of the U.S. government's\n",
        "    open data, where you can access over 300,000 datasets on topics such\n",
        "    as agriculture, education, health, public safety, and more.\n",
        "\n",
        "-   **[Data.world](https://data.world/datasets/canada):** A platform\n",
        "    that hosts thousands of datasets contributed by users and\n",
        "    organizations from around the world.\n",
        "\n",
        "-   **[Datahub.io](https://datahub.io/):** A free, open-source data\n",
        "    management platform that provides access to high-quality,\n",
        "    standardized, and linked datasets.\n",
        "\n",
        "-   **[Google Dataset\n",
        "    Search](https://datasetsearch.research.google.com/):** A search\n",
        "    engine that lets you find datasets from various domains and sources\n",
        "\n",
        "-   **[Kaggle](https://www.kaggle.com/):** A platform for data science\n",
        "    and machine learning competitions, where you can find and download\n",
        "    thousands of datasets uploaded by users and organizations.\n",
        "\n",
        "-   **[Open data, statistics and archives -\n",
        "    Canada.ca](https://www.canada.ca/en/services/science/open-data.html):**\n",
        "    A website that provides data, statistics, analyses and archival\n",
        "    information from the Canadian government.\n",
        "\n",
        "-   **[UCI Machine Learning Repository](https://archive.ics.uci.edu/):**\n",
        "    A collection of over 500 datasets that have been used for machine\n",
        "    learning research and education.\n",
        "\n",
        "-   **[Using open data \\| Open Government - Government of\n",
        "    Canada](https://open.canada.ca/en/using-open-data):** A website that\n",
        "    helps you learn how to use open data, find applications and APIs,\n",
        "    and share your own data projects.\n"
      ],
      "metadata": {
        "id": "bFzaDzkcAVri"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primary sources are original documents or records that provide first-hand evidence or direct information about a topic. Secondary sources are sources that analyze, interpret, or summarize primary sources."
      ],
      "metadata": {
        "id": "NsIlOFVsBjZ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Many Packages Provide Access to Various Datasets\n",
        "\n",
        "-   [Datasets](https://github.com/huggingface/datasets): A library that provides a collection of datasets and metrics for natural language processing, hosted by Hugging Face.\n",
        "\n",
        "-   [OpenML](https://www.openml.org/apis): A platform that hosts thousands of datasets contributed by users and organizations from around the world. It also provides a standardized web API for retrieving data.\n",
        "\n",
        "-   [scikit-learn](https://scikit-learn.org/stable/datasets.html):  A popular library for machine learning in Python. It provides many built-in datasets, as well as functions to fetch datasets from external sources, such as OpenML.\n",
        "\n",
        "-   [seaborn](https://github.com/mwaskom/seaborn-data): A library for statistical data visualization in Python. It also includes some sample datasets for demonstration purposes.\n",
        "\n",
        "-   [tensorflow_datasets](https://www.tensorflow.org/datasets): A library that provides a collection of datasets ready to use with TensorFlow or other Python ML frameworks, such as Jax.\n",
        "\n",
        "-   [yellowbrick](https://www.scikit-yb.org/en/latest/api/datasets/index.html): A library that extends scikit-learn with visual analysis and diagnostic tools. It also offers some datasets for testing and comparison."
      ],
      "metadata": {
        "id": "Yfrun3wwBsXX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='Blue'><b>Examples:</b></font>"
      ],
      "metadata": {
        "id": "13uoQaf9C3cm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AHoWAQLKFYe"
      },
      "outputs": [],
      "source": [
        "# Example: OpenML\n",
        "try:\n",
        "  import openml\n",
        "except ImportError:\n",
        "  !pip install openml\n",
        "  import openml\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore', category=FutureWarning, message=r\".*download_data.*\")\n",
        "\n",
        "\n",
        "# List all datasets and their properties\n",
        "openml.datasets.list_datasets(output_format=\"dataframe\")\n",
        "\n",
        "# Get dataset by ID\n",
        "dataset = openml.datasets.get_dataset(61)\n",
        "\n",
        "# Get dataset by name\n",
        "dataset = openml.datasets.get_dataset('Fashion-MNIST', download_data=True, download_qualities=False, download_features_meta_data=False)\n",
        "\n",
        "# Get the data itself as a dataframe (or otherwise)\n",
        "\n",
        "X, y, _, _ = dataset.get_data(dataset_format=\"dataframe\")\n",
        "\n",
        "display(X)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: scikit-learn\n",
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data # features\n",
        "y = iris.target # target\n",
        "print(X.shape, y.shape)\n",
        "print(iris.feature_names)\n",
        "print(iris.target_names)"
      ],
      "metadata": {
        "id": "ypVa6z57EGBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: yellowbrick\n",
        "try:\n",
        "  from yellowbrick.datasets import load_concrete\n",
        "except ImportError:\n",
        "  !pip install yellowbrick\n",
        "  from yellowbrick.datasets import load_concrete\n",
        "\n",
        "dataset = load_concrete(return_dataset=True)\n",
        "df = dataset.to_dataframe()\n",
        "display(df)"
      ],
      "metadata": {
        "id": "nnNR3RxEEH9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to Handle Missing Values\n",
        "- Missing values, or NaNs, are common in real-world data and can affect the performance and accuracy of machine learning models.\n",
        "- One way to deal with missing values is to drop them from the dataset, either by rows or by columns.\n",
        "- For a Series, dropping missing values is straightforward, as you can use the [dropna()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html) method to remove any element that is NaN.\n",
        "- For a DataFrame, dropping missing values is more complex, as you have to consider the impact on the whole dataset. You cannot have rows or columns with different lengths, so you have to be careful when dropping values.\n",
        "- You can use the [dropna()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html) method on a DataFrame as well, but you have to specify the axis (0 for rows, 1 for columns) and the criteria for dropping values. You can use the following parameters:\n",
        "  - how: 'any' or 'all'. If 'any', drop the row or column if any value is NaN. If 'all', drop the row or column if all values are NaN.\n",
        "  - thresh: an integer. Drop the row or column if the number of non-NaN values is less than thresh.\n",
        "- For example, ```df.dropna(axis=0, how='any')``` will drop any row that has at least one NaN value. ```df.dropna(axis=1, thresh=3)``` will drop any column that has less than three non-NaN values."
      ],
      "metadata": {
        "id": "QQkYkCxXD7MV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='Blue'><b>Examples:</b></font>"
      ],
      "metadata": {
        "id": "qBhP8rimE_y-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create a sample dataframe with some missing values\n",
        "df = pd.DataFrame({\"A\": [1, 2, np.nan, 4], \"B\": [5, np.nan, 7, 8], \"C\": [np.nan, np.nan, np.nan, 10]})\n",
        "print(\"Original DataFrame:\")\n",
        "display(df)\n",
        "\n",
        "# Drop any row that has at least one NaN value\n",
        "df1 = df.dropna(axis=0, how='any')\n",
        "print(\"\\nDataFrame after dropping rows with any NaN value:\")\n",
        "display(df1)\n",
        "\n",
        "# Drop any column that has less than three non-NaN values\n",
        "df2 = df.dropna(axis=1, thresh=3)\n",
        "print(\"\\nDataFrame after dropping columns with less than three non-NaN values:\")\n",
        "display(df2)"
      ],
      "metadata": {
        "id": "uZikWNhiGnrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# https://climate.weather.gc.ca/climate_data/daily_data_e.html?StationID=50430\n",
        "# Define the data retrieval link\n",
        "link = 'https://climate.weather.gc.ca/climate_data/bulk_data_e.html?format=csv&stationID=50430&Year=2024&Month=1&Day=1&time=&timeframe=2&submit=Download+Data'\n",
        "\n",
        "# Read the CSV file and select relevant columns\n",
        "df = pd.read_csv(link, usecols=['Date/Time', 'Mean Temp (°C)'])\n",
        "\n",
        "# Rename the 'Date/Time' column, drop missing values, select the last six rows, and set the 'Date' column as the index\n",
        "df = df.rename(columns={'Date/Time': ''}).dropna().tail(6).set_index('')\n",
        "\n",
        "# Making values at positions 2 and 4 missing\n",
        "df.iloc[1] = pd.NA  # Position 2\n",
        "df.iloc[3] = pd.NA  # Position 4\n",
        "\n",
        "# Display the original dataframe\n",
        "print(\"Original DataFrame:\")\n",
        "display(df)\n",
        "\n",
        "# Drop rows with missing values\n",
        "df_clean = df.dropna()\n",
        "print(\"\\nDataFrame after dropping rows with missing values:\")\n",
        "display(df_clean)"
      ],
      "metadata": {
        "id": "ZV7ZcMqbDOGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filling NaN Values\n",
        "\n",
        "- **Forward-fill**: This method uses the previous valid value to fill the missing value, which can be useful for time series data, where the values are ordered by time and the missing value can be assumed to be similar to the previous one. For example, if the temperature data for a day is missing, we can use the temperature data from the previous day to fill it.\n",
        "- **Back-fill**: This method uses the next valid value to fill the missing value, which can be useful for reverse time series data, where the values are ordered by time in reverse and the missing value can be assumed to be similar to the next one. For example, if the stock price data for a day is missing, we can use the stock price data from the next day to fill it.\n",
        "- **Custom code**: This method allows us to write our own logic to fill in the missing values, which can be useful for complex or specific cases, where the other methods are not suitable or accurate. For example, if the age data for a person is missing, we can use the mean or median age of the population to fill it or use some other criteria based on the data.\n",
        "\n",
        "The choice of method depends on the source and nature of the data and the desired outcome."
      ],
      "metadata": {
        "id": "aqOn0ixuE9W4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='Blue'><b>Examples:</b></font>"
      ],
      "metadata": {
        "id": "EBs4DZ6lFAi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Backward fill missing values\n",
        "df_bfill = df.bfill()\n",
        "print(\"\\nDataFrame after backward fill:\")\n",
        "display(df_bfill)\n",
        "\n",
        "# Forward fill missing values\n",
        "df_ffill = df.ffill()\n",
        "print(\"\\nDataFrame after forward fill:\")\n",
        "display(df_ffill)\n",
        "\n",
        "# Linear interpolation for missing values\n",
        "df_interpolated = df.interpolate(method='linear')\n",
        "print(\"\\nDataFrame after linear interpolation:\")\n",
        "display(df_interpolated)"
      ],
      "metadata": {
        "id": "BA_kkSfaISkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detecting Errors in Real Measurements\n",
        "\n",
        "Real-world data collection may introduce errors due to various sources such as equipment failure or environmental noise. You can use different methods to identify these errors depending on the data's characteristics and shape, such as visualization or statistical analysis. For example, you can measure their distance from the data's center. You can handle the errors in the same way as the NaN values. A simple coding solution is to replace all the erroneous values with np.nan and then use your chosen method to fill the missing values.\n"
      ],
      "metadata": {
        "id": "NTS8g97gG2j9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Use a custom style for the plot (adjust the path to your style file)\n",
        "plt.style.use('https://raw.githubusercontent.com/HatefDastour/ENSF444/main/Files/mystyle.mplstyle')\n",
        "\n",
        "# Load the diabetes dataset\n",
        "from sklearn.datasets import load_diabetes\n",
        "diabetes = load_diabetes()\n",
        "df = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
        "\n",
        "# Create a figure with two subplots, specifying the size and ratios\n",
        "fig, (ax_box, ax_hist) = plt.subplots(2, figsize=(8, 5), gridspec_kw={\"height_ratios\": (.15, .85)})\n",
        "\n",
        "# Implement a box plot for the 'bmi' column\n",
        "sns.boxplot(x=df['bmi'], ax=ax_box)\n",
        "\n",
        "# Integrate a histogram for the 'bmi' column with kernel density estimation\n",
        "sns.histplot(x=df['bmi'], ax=ax_hist, kde=True)\n",
        "\n",
        "# Add grid lines to the histogram on the x-axis\n",
        "ax_hist.grid(axis='x')\n",
        "\n",
        "# Omit the x-axis label for the box plot\n",
        "ax_box.set(xlabel='')\n",
        "\n",
        "# Add a title to the plot\n",
        "plt.suptitle('Distribution of Body Mass Index (BMI)')\n",
        "\n",
        "# Display the figure\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8ULBk3zEG7sU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optional Content\n",
        "\n",
        "There are several ways to find the values of outliers in your data using Python. One way is to use the interquartile range (IQR) method, which defines outliers as the values that are below Q1 - 1.5 * IQR or above Q3 + 1.5 * IQR, where Q1 and Q3 are the first and third quartiles of the data, respectively. You can use the pandas library to calculate the IQR and the outliers for each column of your dataframe. Here is an example of how to do it for the bmi column:"
      ],
      "metadata": {
        "id": "3dkesVJLIAuc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import pandas library\n",
        "import pandas as pd\n",
        "\n",
        "# Load the diabetes dataset\n",
        "from sklearn.datasets import load_diabetes\n",
        "diabetes = load_diabetes()\n",
        "df = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
        "\n",
        "# Calculate the IQR for the bmi column\n",
        "Q1 = df['bmi'].quantile(0.25)\n",
        "Q3 = df['bmi'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Find the outliers using the IQR rule\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "outliers = df[(df['bmi'] < lower_bound) | (df['bmi'] > upper_bound)]\n",
        "\n",
        "# Print the number and index of outliers\n",
        "print(\"Number of outliers:\", len(outliers))\n",
        "print(\"Outlier indices:\", outliers.index.values)\n",
        "\n",
        "# Print the values of outliers\n",
        "print(\"Outlier values:\", outliers['bmi'].values)"
      ],
      "metadata": {
        "id": "-rIqGYAsG9k2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}