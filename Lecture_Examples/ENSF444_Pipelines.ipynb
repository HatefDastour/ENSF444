{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WMfms9RqCv9"
   },
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZG-QPP1_qEHl"
   },
   "source": [
    "## Introduction\n",
    "In this lecture, we will learn about **pipelines** in sklearn, which are a powerful tool for building and evaluating machine learning models. We will cover the following topics:\n",
    "\n",
    "- What is a pipeline and what are its benefits?\n",
    "- How to create a pipeline using the `make_pipeline` function\n",
    "- How to access and modify the steps and parameters of a pipeline\n",
    "- How to use a pipeline for cross-validation and grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RfU_hbTqFsc"
   },
   "source": [
    "## What is a pipeline and what are its benefits?\n",
    "A typical machine learning workflow involves several steps, such as data cleaning, feature extraction, feature selection, scaling, dimensionality reduction, and model fitting. Applying these steps manually can be tedious and error-prone, especially when we need to ensure that the same steps are applied consistently to both the training and the test data.\n",
    "\n",
    "A **pipeline** is a way of automating this workflow by chaining together a sequence of data transformers with an optional final predictor. A pipeline allows us to:\n",
    "\n",
    "- **Simplify the code** by avoiding intermediate variables and fit/transform calls\n",
    "- **Ensure consistent preprocessing** by applying the same steps to both the training and the test data\n",
    "- **Enable parameter setting** by using the names of the steps and the parameter names separated by a '__'\n",
    "- **Allow for easy cross-validation and grid search** over the whole process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfFBZB4U_2VJ"
   },
   "source": [
    "## How to create a pipeline using the `make_pipeline` function\n",
    "\n",
    "One of the easiest ways to create a pipeline in sklearn is to use the [`make_pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html) function, which takes a list of estimator objects as arguments and returns a pipeline object. The `make_pipeline` function does not require and does not permit, naming the estimators. Instead, it automatically assigns names to the steps based on their types, in lowercase. For example, if we pass a `StandardScaler` and a `SVC` object, the names of the steps will be 'standardscaler' and 'svc', respectively.\n",
    "\n",
    "The syntax of the `make_pipeline` function is:\n",
    "\n",
    "```python\n",
    "make_pipeline(*steps, memory=None, verbose=False)\n",
    "```\n",
    "\n",
    "where:\n",
    "\n",
    "- `steps` is a list of estimator objects that are chained together\n",
    "- `memory` is an optional argument that can be used to cache the fitted transformers of the pipeline\n",
    "- `verbose` is an optional argument that can be used to print the time elapsed while fitting each step\n",
    "\n",
    "For example, to create a pipeline that scales the data and applies a support vector classifier, we can write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dyGTrsySquGx"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipe = make_pipeline(StandardScaler(), SVC())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5Qa_eIzqcLi"
   },
   "source": [
    "## How to access the steps and parameters of a pipeline\n",
    "- A pipeline object has several attributes and methods that can be used to access the steps and parameters of the pipeline\n",
    "- Some of the most common ones are:\n",
    "    * `named_steps`: a dictionary that maps the names of the steps to the estimators\n",
    "    * `steps`: a list of tuples that contains the names and the estimators of the steps\n",
    "    * `get_params()`: a method that returns a dictionary of all the parameters of the pipeline and the estimators\n",
    "    * `set_params()`: a method that sets the parameters of the pipeline and the estimators\n",
    "    * `fit()`: a method that fits the pipeline on the data\n",
    "    * `predict()`: a method that makes predictions using the pipeline\n",
    "    * `score()`: a method that returns the score of the pipeline on the data\n",
    "- For example, to access the scaler and the SVC objects in the previous pipeline, we can write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "irTjeDmFq1TF"
   },
   "outputs": [],
   "source": [
    "print(pipe.named_steps['standardscaler'])\n",
    "print(pipe.named_steps['svc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmDf5kYrq7jW"
   },
   "source": [
    "- To get the value of the C parameter of the SVC, we can write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hFoRhrgVq8-F"
   },
   "outputs": [],
   "source": [
    "print(pipe.get_params()['svc__C'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZOe7qqOrAaR"
   },
   "source": [
    "- To set the value of the C parameter of the SVC to 10, we can write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uvYlWh9JrCRN"
   },
   "outputs": [],
   "source": [
    "pipe.set_params(svc__C=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ujbfxp-1rKCA"
   },
   "source": [
    "- To fit the pipeline on the training data and score it on the test data, we can write:\n",
    "\n",
    "```python\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wztqMb-Hr1Nh"
   },
   "source": [
    "<font color='Blue'><b>Example:</b></font> This example is based on  [scikit-learn.org](https://scikit-learn.org/stable/auto_examples/ensemble/plot_stack_predictors.html#sphx-glr-auto-examples-ensemble-plot-stack-predictors-py).\n",
    "\n",
    "**Dateset:** The Ames Housing dataset is a realistic and complex dataset for house price prediction. It contains 2,919 observations of housing sales in Ames, Iowa between 2006 and 2010, with 80 features describing various aspects of the houses. The target variable is the sale price of the house in dollars.\n",
    "\n",
    "The dataset can be loaded from sklearn using the `fetch_openml` function, which returns a `Bunch` object containing the data, the target, the feature names, and some metadata. The dataset has some missing values, which are marked with the character \"?\" in the original CSV file. We can use the `na_values` argument of `fetch_openml` to specify this missing value marker, so that pandas can parse the data correctly. We can also drop the \"Id\" column, which is not relevant for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mFw9w9HssY8h"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_ames_housing(n, random_state = 0):\n",
    "    # Fetch the Ames Housing dataset from openml\n",
    "    df = fetch_openml(name=\"house_prices\", as_frame=True, parser='auto')\n",
    "\n",
    "    # Extract features (X) and sale price (y)\n",
    "    X = df.data\n",
    "    y = df.target\n",
    "\n",
    "    # Selected features for analysis\n",
    "    features = [\n",
    "        \"YrSold\",\n",
    "        \"HeatingQC\",\n",
    "        \"Street\",\n",
    "        \"YearRemodAdd\",\n",
    "        \"Heating\",\n",
    "        \"MasVnrType\",\n",
    "        \"BsmtUnfSF\",\n",
    "        \"Foundation\",\n",
    "        \"MasVnrArea\",\n",
    "        \"MSSubClass\",\n",
    "        \"ExterQual\",\n",
    "        \"Condition2\",\n",
    "        \"GarageCars\",\n",
    "        \"GarageType\",\n",
    "        \"OverallQual\",\n",
    "        \"TotalBsmtSF\",\n",
    "        \"BsmtFinSF1\",\n",
    "        \"HouseStyle\",\n",
    "        \"MiscFeature\",\n",
    "        \"MoSold\",\n",
    "    ]\n",
    "\n",
    "    # Select only the relevant features from the dataset\n",
    "    X = X.loc[:, features]\n",
    "\n",
    "    # Shuffle the dataset for randomness\n",
    "    X, y = shuffle(X, y, random_state = random_state)\n",
    "\n",
    "    # Limit the dataset to the first 600 instances for efficiency\n",
    "    X = X.iloc[:n]\n",
    "    y = y.iloc[:n]\n",
    "\n",
    "    # Return the preprocessed features (X) and target variable (y)\n",
    "    return X, np.log(y)\n",
    "\n",
    "# Load the Ames Housing dataset using the defined function\n",
    "X, y = load_ames_housing(n = 1000)\n",
    "print('\\nX')\n",
    "display(X)\n",
    "print('\\ny')\n",
    "display(y.to_frame('ln(Sale Price)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fwtYBGvRCuQ6"
   },
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,                              # Feature matrix\n",
    "    y,                              # Target variable\n",
    "    test_size=0.25,                 # Proportion of the dataset to include in the test split\n",
    "    random_state=0                  # Seed for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtxZE9kZu3Kj"
   },
   "source": [
    "**Making a pipeline to preprocess the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KPqzF7mYu72X"
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_selector\n",
    "\n",
    "# Define a column selector for categorical features using the make_column_selector function\n",
    "cat_selector = make_column_selector(dtype_include=object)\n",
    "\n",
    "# Define a column selector for numerical features using the make_column_selector function\n",
    "num_selector = make_column_selector(dtype_include=np.number)\n",
    "\n",
    "# Apply the categorical feature selector to the feature matrix X\n",
    "cat_features = cat_selector(X)\n",
    "print('Categorical features:', cat_features)\n",
    "\n",
    "# Apply the numerical feature selector to the feature matrix X\n",
    "num_features = num_selector(X)\n",
    "print('Numerical features:', num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6j-zdnqj91Cm"
   },
   "source": [
    "In this code, we are using the `make_column_selector` function from sklearn to create two column selectors: one for categorical features and one for numerical features. A column selector is a callable that can select columns from a pandas DataFrame based on their names or data types. We can use column selectors with the `ColumnTransformer` class to apply different transformations to different subsets of features.\n",
    "\n",
    "The `make_column_selector` function takes three optional arguments: `pattern`, `dtype_include`, and `dtype_exclude`. The `pattern` argument is a regular expression that matches the names of the columns to be selected. The `dtype_include` and `dtype_exclude` arguments are lists of data types that specify which columns to include or exclude based on their types. For more details, see the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_transformer.html).\n",
    "\n",
    "In this code, we are using the `dtype_include` argument to create two column selectors: one for categorical features and one for numerical features. We pass the `object` data type to the `dtype_include` argument of the `cat_selector` to select the columns that have string values. We pass the `np.number` data type to the `dtype_include` argument of the `num_selector` to select the columns that have numeric values. We use the `numpy` library as `np` to access the `number` data type.\n",
    "\n",
    "After creating the column selectors, we apply them to the feature matrix `X`, which is a pandas DataFrame. We assign the output of the `cat_selector` to the variable `cat_features`, which is a list of the names of the categorical features. We assign the output of the `num_selector` to the variable `num_features`, which is a list of the names of the numerical features. We print the values of these variables to see the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1G9Rpn21vs1a"
   },
   "source": [
    "**Defining the pipeline required for the tree-based models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YCILxo2Nvm0a"
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Define an OrdinalEncoder for categorical features in a decision tree context\n",
    "cat_tree_processor = OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                                    unknown_value=-1,\n",
    "                                    encoded_missing_value=-2)\n",
    "\n",
    "# Define a SimpleImputer for numerical features in a decision tree context\n",
    "num_tree_processor = SimpleImputer(strategy=\"mean\", add_indicator=True)\n",
    "\n",
    "# Create a column transformer for preprocessing both numerical and categorical features\n",
    "tree_preprocessor = make_column_transformer(\n",
    "    (num_tree_processor, num_selector),   # Apply num_tree_processor to numerical features\n",
    "    (cat_tree_processor, cat_selector)    # Apply cat_tree_processor to categorical features\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v7guBr3a-Tye"
   },
   "source": [
    "In this code, we are creating a column transformer for decision tree models, which can handle missing values and categorical features differently from other models. We define two transformers and two column selectors for this purpose:\n",
    "\n",
    "- `cat_tree_processor` is an `OrdinalEncoder` that encodes categorical features as ordinal integers. It can handle unknown values and missing values by assigning them special codes: -1 for unknown values and -2 for missing values. This way, the decision tree can split on these values and learn from them.\n",
    "- `num_tree_processor` is a `SimpleImputer` that imputes missing values in numerical features with the mean value. It also adds an indicator feature that marks the missing values with a 1 and the non-missing values with a 0. This allows the decision tree to split on the indicator feature and learn from the missingness pattern.\n",
    "- `num_selector` is a column selector that selects numerical features based on their data type. It returns a list of the names of the numerical features in the data.\n",
    "- `cat_selector` is a column selector that selects categorical features based on their data type. It returns a list of the names of the categorical features in the data.\n",
    "\n",
    "We pass these tuples to the `make_column_transformer` function, which creates a column transformer object called `tree_preprocessor`. We can use this object to fit and transform the data, and pass the output to a decision tree model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HS2C4uyCv-g8"
   },
   "source": [
    "**Defining the preprocessor used when the ending regressor is a linear model**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c4_QHh3Tv_xA"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Define a OneHotEncoder for categorical features in a linear regression context\n",
    "cat_linear_processor = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "# Create a pipeline for numerical features in a linear regression context\n",
    "num_linear_processor = make_pipeline(\n",
    "    StandardScaler(),                     # Standardize numerical features\n",
    "    SimpleImputer(strategy=\"mean\", add_indicator=True)  # Impute missing values with mean and add indicator\n",
    "    )\n",
    "\n",
    "# Create a column transformer for preprocessing both numerical and categorical features\n",
    "linear_preprocessor = make_column_transformer(\n",
    "    (num_linear_processor, num_selector),   # Apply num_linear_processor to numerical features\n",
    "    (cat_linear_processor, cat_selector)    # Apply cat_linear_processor to categorical features\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKIidQQnAFoU"
   },
   "source": [
    "In this code, we are creating a column transformer for linear regression models, which require different preprocessing for numerical and categorical features. We define two transformers and two column selectors for this purpose:\n",
    "\n",
    "- `cat_linear_processor` is a `OneHotEncoder` that encodes categorical features as one-hot vectors. It can handle unknown values by ignoring them and not generating any output for them.\n",
    "- `num_linear_processor` is a pipeline that applies two transformations to numerical features: a `StandardScaler` that standardizes the features by removing the mean and scaling to unit variance, and a `SimpleImputer` that imputes missing values with the mean value and adds an indicator feature that marks the missing values with a 1 and the non-missing values with a 0.\n",
    "- `num_selector` is a column selector that selects numerical features based on their data type. It returns a list of the names of the numerical features in the data.\n",
    "- `cat_selector` is a column selector that selects categorical features based on their data type. It returns a list of the names of the categorical features in the data.\n",
    "\n",
    "We pass these tuples to the `make_column_transformer` function, which creates a column transformer object called `linear_preprocessor`. We can use this object to fit and transform the data, and pass the output to a linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fr0NZI3LwF6H"
   },
   "source": [
    "**Stack of predictors on a single data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u22az6TswDhe"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# Create a pipeline for Lasso regression, including preprocessing with linear_preprocessor\n",
    "lasso_pipeline = make_pipeline(\n",
    "    linear_preprocessor,   # Apply linear_preprocessor for feature transformation\n",
    "    LassoCV(random_state = 0)   # Apply LassoCV for linear regression with L1 regularization\n",
    ")\n",
    "\n",
    "lasso_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iVXJ7FmZwL3y"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Create a pipeline for Random Forest regression, including preprocessing with tree_preprocessor\n",
    "rf_pipeline = make_pipeline(\n",
    "    tree_preprocessor,                  # Apply tree_preprocessor for feature transformation\n",
    "    RandomForestRegressor(random_state=0)  # Apply RandomForestRegressor for ensemble regression\n",
    ")\n",
    "\n",
    "rf_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ieMzvPUnxiua"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Create a pipeline for Gradient Boosting regression, including preprocessing with tree_preprocessor\n",
    "gbr_pipeline = make_pipeline(\n",
    "    tree_preprocessor,                     # Apply tree_preprocessor for feature transformation\n",
    "    GradientBoostingRegressor(random_state=0)  # Apply GradientBoostingRegressor for ensemble regression\n",
    ")\n",
    "\n",
    "gbr_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking is an ensemble learning technique that combines the predictions of multiple base models, such as decision trees, linear models, etc., and uses a meta-model to produce the final output. The meta-model can be either a regressor or a classifier, depending on the task.\n",
    "\n",
    "The main advantage of stacking is that it can leverage the strengths of each base model and learn how to best weigh their predictions with the meta-model. This can improve the performance and generalization ability of a single model, especially when the base models have different biases or assumptions.\n",
    "\n",
    "However, stacking also has some drawbacks, such as increased complexity and computational cost, and the need for careful tuning of the hyperparameters of the base and meta-models.\n",
    "\n",
    "In Python, stacking can be implemented using the following modules from the scikit-learn library:\n",
    "\n",
    "- [StackingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingRegressor.html) for regression tasks\n",
    "- [StackingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html) for classification tasks\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/HatefDastour/ENSF444/71daeddd7eda2b9c19d1157ee4ec0c27afbc5d30/Images/Stacking_Model.png\" alt=\"picture\" width=\"700\">\n",
    "<br>\n",
    "<b>Figure</b>: Stacking Regressor/Classifier.\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3dGBlcW_wN2u"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Define a list of base estimators for stacking\n",
    "estimators = [\n",
    "    (\"Random Forest\", rf_pipeline),\n",
    "    (\"Lasso\", lasso_pipeline),\n",
    "    (\"Gradient Boosting\", gbr_pipeline)\n",
    "]\n",
    "\n",
    "# Create a StackingRegressor with specified base estimators and final estimator as SVR\n",
    "stacking_regressor = StackingRegressor(\n",
    "    estimators=estimators,      # List of base estimators\n",
    "    final_estimator=SVR()       # Final estimator for meta-regression (SVR in this case)\n",
    ")\n",
    "\n",
    "stacking_regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iopVVNAZw0tW"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import PredictionErrorDisplay\n",
    "from sklearn.model_selection import cross_val_predict, cross_validate\n",
    "from scipy.stats import sem\n",
    "\n",
    "plt.style.use('https://raw.githubusercontent.com/HatefDastour/ENSF444/main/Files/mystyle.mplstyle')\n",
    "\n",
    "def visualize_prediction_errors(X, y,\n",
    "                                suptitle = \"Single predictors versus stacked predictors\",\n",
    "                                estimators = estimators, stacking_regressor = stacking_regressor):\n",
    "\n",
    "    # Create subplots for visualizing prediction errors\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(11, 10))\n",
    "    axs = axs.ravel()\n",
    "\n",
    "    # Iterate through base estimators and the stacking regressor\n",
    "    for ax, (name, est) in zip(axs, estimators + [(\"Stacking Regressor\", stacking_regressor)]):\n",
    "        scorers = {\"$R^2$\": \"r2\", \"MSE\": \"neg_mean_squared_error\"}\n",
    "\n",
    "        # Measure the time taken for cross-validation\n",
    "        start_time = time.time()\n",
    "        scores = cross_validate(est, X, y, scoring=list(scorers.values()), n_jobs=-1, verbose=0)\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        # Obtain predictions using cross-validation\n",
    "        y_pred = cross_val_predict(est, X, y, n_jobs=-1, verbose=0)\n",
    "\n",
    "        # Calculate and format evaluation scores\n",
    "        scores = {\n",
    "            key: (\n",
    "                f\"{np.abs(np.mean(scores[f'test_{value}'])):.3f} ± \"\n",
    "                f\"{(sem(scores[f'test_{value}'], ddof=0)):.3f}\"\n",
    "            )\n",
    "            for key, value in scorers.items()\n",
    "        }\n",
    "\n",
    "        # Display prediction errors using PredictionErrorDisplay\n",
    "        display = PredictionErrorDisplay.from_predictions(\n",
    "            y_true=y,\n",
    "            y_pred=y_pred,\n",
    "            kind=\"actual_vs_predicted\",\n",
    "            ax=ax,\n",
    "            scatter_kwargs={\"alpha\": 0.2, \"color\": \"tab:blue\"},\n",
    "            line_kwargs={\"color\": \"tab:red\", 'linewidth':2},\n",
    "        )\n",
    "        ax.set_title(f\"{name}\\nEvaluation in {elapsed_time:.2f} seconds\")\n",
    "\n",
    "        # Add legend for evaluation scores\n",
    "        for name, score in scores.items():\n",
    "            ax.plot([], [], \" \", label=f\"{name}: {score}\")\n",
    "        ax.legend(loc=\"upper left\")\n",
    "\n",
    "    # Set overall plot title and adjust layout\n",
    "    plt.suptitle(suptitle, weight = 'bold', fontsize = 14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cmQIAfhp25dy"
   },
   "outputs": [],
   "source": [
    "visualize_prediction_errors(X_train, y_train, suptitle = \"Single Predictors vs. Stacked Predictors (Train Set)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fBgUB9KU6-kw"
   },
   "outputs": [],
   "source": [
    "visualize_prediction_errors(X_test, y_test, suptitle = \"Single Predictors vs. Stacked Predictors (Test Set)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wEQkTIn-qgXl"
   },
   "source": [
    "## How to use a pipeline for cross-validation and grid search\n",
    "\n",
    "A pipeline is a useful tool that allows us to evaluate and optimize the whole process of data preprocessing and model fitting as a single unit. This means that we can use a pipeline as any other estimator for cross-validation and grid search, using the same functions and classes from sklearn.\n",
    "\n",
    "- To use a pipeline for **cross-validation**, we can use the `cross_val_score` function, which takes a pipeline object, the data, the target, and the number of folds as arguments, and returns an array of scores for each fold. We can then compute the mean score to get an estimate of the performance of the pipeline on the data. Cross-validation is a technique that splits the data into multiple folds, and uses one fold as the validation set and the rest as the training set. This is repeated for each fold, and the average performance is reported.\n",
    "\n",
    "- To use a pipeline for **grid search**, we can use the `GridSearchCV` class, which takes a pipeline object, a parameter grid, and the number of folds as arguments, and returns a grid search object that can be fitted on the data. The parameter grid is a dictionary that maps the names of the steps and the parameters to the values that we want to try. We need to use the '__' syntax to specify the step and the parameter name, for example 'svc__C' for the C parameter of the SVC step. The grid search object has attributes such as `best_params_` and `best_score_` that can be used to get the best combination of parameters and the corresponding score. Grid search is a tool that helps you find the best hyperparameters for your model by using cross-validation.\n",
    "\n",
    "For example, suppose we want to perform a 5-fold cross-validation and a grid search over the C and gamma parameters of the SVC in the previous pipeline. We can write:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "\n",
    "# 5-fold cross-validation\n",
    "scores = cross_val_score(pipe, X_train or X_test, y _train or y_test, cv=5)\n",
    "print(\"Mean cross-validation score:\", scores.mean())\n",
    "\n",
    "# grid search\n",
    "param_grid = {'svc__C': [0.1, 1, 10], 'svc__gamma': [0.01, 0.1, 1]}\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "print(\"Best score:\", grid.best_score_)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RP5CTxUJn6gI"
   },
   "source": [
    "# Summary\n",
    "In this lecture, we learned how to use **pipelines** in sklearn to automate and optimize our machine learning workflow. We covered the following key points:\n",
    "\n",
    "- A pipeline is a sequence of data transformers with an optional final predictor that can be applied to both the training and the test data\n",
    "- A pipeline simplifies the code by avoiding intermediate variables and fit/transform calls, and enables parameter setting by using the '__' syntax\n",
    "- A pipeline can be created using the `make_pipeline` function, which automatically names the steps based on their types, or using the `Pipeline` class, which allows us to name the steps explicitly\n",
    "- A pipeline can be accessed and modified using the `named_steps`, `steps`, `get_params()`, and `set_params()` attributes and methods of the pipeline object\n",
    "- A pipeline can be used as any other estimator for cross-validation and grid search, using the `cross_val_score` function and the `GridSearchCV` class from sklearn"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
