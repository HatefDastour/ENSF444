{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVg6G22aB1eK"
      },
      "source": [
        "# Hyperparameter Tuning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qw8vb4FaBxta"
      },
      "source": [
        "## Review: Cross-validation\n",
        "\n",
        "[Cross-validation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold) is a technique for evaluating the performance of a machine learning model on different subsets of the data. It helps to avoid overfitting and underfitting, and to estimate the generalization error of the model.\n",
        "\n",
        "One common method of cross-validation is **k-fold cross-validation**, where the data is split into k equal parts, or folds. The model is trained on k-1 folds and tested on the remaining fold. This process is repeated k times, each time using a different fold as the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2721bMHHn0i"
      },
      "source": [
        "<font color='Blue'><b>Example</b></font>. The Auto MPG dataset retrieved from the [UCI Machine Learning Repository](http://archive.ics.uci.edu/dataset/9/auto+mpg)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJUOwE6MJGkl"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from ucimlrepo import fetch_ucirepo\n",
        "except ImportError:\n",
        "    !pip3 install -U ucimlrepo\n",
        "    from ucimlrepo import fetch_ucirepo\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# fetch dataset\n",
        "auto_mpg = fetch_ucirepo(name = 'Auto MPG')\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X = auto_mpg.data.features\n",
        "y = auto_mpg.data.targets\n",
        "\n",
        "# drop rows with missing values from X\n",
        "X = X.dropna(axis=0, how='any')\n",
        "\n",
        "# align X and y by index\n",
        "X, y = X.align(y, join='inner', axis=0)\n",
        "# We could also use sklearn.preprocessing.OneHotEncoder\n",
        "# and fit it on the train set and then apply it on train and test set.\n",
        "# the outcome would be similar to what we are doing here.\n",
        "X.origin = X.origin.replace({1: 'USA', 2: 'Germany', 3: 'Japan'})\n",
        "X = pd.get_dummies(X, dtype= 'int16')\n",
        "\n",
        "# ln(mpg)\n",
        "y = np.log(y['mpg'])\n",
        "y.name = 'ln(mpg)'\n",
        "print('X:')\n",
        "display(X)\n",
        "print('\\ny:')\n",
        "print(y)\n",
        "print('\\nInfo:')\n",
        "X.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyVhvny1B3i4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from scipy.stats import sem\n",
        "\n",
        "# Initialize KFold cross-validator\n",
        "random_state = 0\n",
        "n_splits = 5\n",
        "kf = KFold(n_splits=n_splits, shuffle=True, random_state = random_state)\n",
        "\n",
        "# Lists to store train and test scores for each fold\n",
        "train_r2_scores, test_r2_scores, train_MSE_scores, test_MSE_scores = [], [], [], []\n",
        "\n",
        "rfr = RandomForestRegressor(n_estimators = 100, n_jobs = -1, random_state = random_state)\n",
        "\n",
        "def _Line(n = 80):\n",
        "    print(n * '_')\n",
        "\n",
        "def print_bold(txt):\n",
        "    _left = \"\\033[1;34m\"\n",
        "    _right = \"\\033[0m\"\n",
        "    print(_left + txt + _right)\n",
        "\n",
        "_Line()\n",
        "\n",
        "# Perform Cross-Validation\n",
        "for fold, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
        "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]  # Extract training and testing subsets by index\n",
        "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "    rfr.fit(X_train, y_train)  # Train the SVR model\n",
        "\n",
        "    # train\n",
        "    y_train_pred = rfr.predict(X_train)\n",
        "    train_r2_scores.append(metrics.r2_score(y_train, y_train_pred))\n",
        "    train_MSE_scores.append(metrics.mean_squared_error(y_train, y_train_pred))\n",
        "\n",
        "    # test\n",
        "    y_test_pred = rfr.predict(X_test)\n",
        "    test_r2_scores.append(metrics.r2_score(y_test, y_test_pred))\n",
        "    test_MSE_scores.append(metrics.mean_squared_error(y_test, y_test_pred))\n",
        "\n",
        "    #  Print the Train and Test Scores for each fold\n",
        "    print_bold(f'Fold {fold + 1}:')\n",
        "    train_proportion = 100*len(train_idx)/len(X)\n",
        "    test_proportion = 100*len(test_idx)/len(X)\n",
        "    print(f\"\\tTrain Set is {train_proportion:.2f}% of the dataset, and Test Set is {test_proportion:.2f}% of the dataset.\")\n",
        "    print(f\"\\tTrain R-Squared Score = {train_r2_scores[fold]:.5f}, Test R-Squared Score = {test_r2_scores[fold]:.5f}\")\n",
        "    print(f\"\\tTrain MSE Score = {train_MSE_scores[fold]:.5f}, Test MSE Score= {test_MSE_scores[fold]:.5f}\")\n",
        "\n",
        "\n",
        "df_scores = pd.DataFrame({'Fold': np.arange(1, len(train_r2_scores)+1),\n",
        "                          'Train R-Squared Score':train_r2_scores,\n",
        "                          'Test R-Squared Score':test_r2_scores,\n",
        "                          'Train MSE Score':train_MSE_scores,\n",
        "                          'Test MSE Score':test_MSE_scores,\n",
        "                          }).set_index('Fold')\n",
        "\n",
        "display(df_scores)\n",
        "\n",
        "_Line()\n",
        "print_bold('R-Squared Score:')\n",
        "print(f\"\\tMean Train R-Squared Score: {np.mean(train_r2_scores):.5f} ± {sem(train_r2_scores, ddof = 0):.5f}\")\n",
        "print(f\"\\tMean Test R-Squared Score: {np.mean(test_r2_scores):.5f} ± {sem(test_r2_scores, ddof = 0):.5f}\")\n",
        "print_bold('MSE Score:')\n",
        "print(f\"\\tMean Train MSE Accuracy Score: {np.mean(train_MSE_scores):.5f} ± {sem(train_MSE_scores, ddof = 0):.5f}\")\n",
        "print(f\"\\tMean Test MSE Accuracy Score: {np.mean(test_MSE_scores):.5f} ± {sem(test_MSE_scores, ddof = 0):.5f}\")\n",
        "_Line()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMOfyKGCPfjh"
      },
      "source": [
        "### Standard Error\n",
        "\n",
        "The **standard error (SE)** is a way to measure how accurate an estimated value is, like the coefficients in a linear regression. It shows the average amount by which estimates can vary when we take multiple samples from the same group.\n",
        "\n",
        "The standard error (SE) is typically calculated using this formula:\n",
        "\n",
        "\\begin{equation} \\text{SE} = \\frac{s}{\\sqrt{n}} \\end{equation}\n",
        "\n",
        "Where:\n",
        "- $ s $ represents the sample standard deviation, which measures the variability of data points within the sample.\n",
        "- $ n $ is the number of data points (sample size)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YF8xMyANQd3"
      },
      "source": [
        "For example, the standard error in Mean Train R-Squared Score: 0.98481 ± 0.00055 can be calculated as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYQBfcMGNFkg"
      },
      "outputs": [],
      "source": [
        "print(f'Standard Error = {(np.std(train_r2_scores, ddof = 0)/np.sqrt(len(train_r2_scores))):.5f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ikz1FmKhS_dM"
      },
      "source": [
        "### Advantages and Disadvantages of Cross Validation\n",
        "\n",
        "- **Advantages:**\n",
        "    * It helps to prevent overfitting, which occurs when a model is trained too well on the training data and performs poorly on new, unseen data.\n",
        "    * It provides a more realistic estimate of the model’s generalization performance, i.e., its ability to perform well on new, unseen data.\n",
        "    * It can be used to compare different models and select the one that performs the best on average.\n",
        "- **Disadvantages:**\n",
        "    * It is computationally expensive, as it requires training and testing the model multiple times.\n",
        "    * It can introduce variability in the results, depending on how the data is partitioned and shuffled.\n",
        "    * It may not be suitable for some types of data, such as time series or spatial data, where the order or location of the data points matters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_m-KqGEVuRN"
      },
      "source": [
        "## Stratified Cross-Validation\n",
        "\n",
        "[Stratified cross-validation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html) is a method of evaluating the performance of a machine learning model on unseen data. It is similar to regular cross-validation, but it ensures that each fold or subset of the data has the same proportion of classes as the original data. This helps to avoid bias and improve the accuracy of the model.\n",
        "\n",
        "Stratified cross-validation is especially useful for classification problems, where the outcome variable has two or more categories. For example, if we want to predict whether a person has a disease or not, we need to make sure that the training and testing sets have the same ratio of positive and negative cases. Otherwise, the model might overfit or underfit the data.\n",
        "\n",
        "There are different types of stratified cross-validation, such as stratified k-fold cross-validation, stratified repeated k-fold cross-validation, and stratified shuffle-split cross-validation. They differ in how they split the data into folds and how many times they repeat the process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TFdsrwkOJvA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_blobs\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "colors = [\"#f5645a\", \"#0096ff\", '#B2FF66']\n",
        "edge_colors = ['#8A0002', '#2e658c', '#6A993D']\n",
        "markers = ['o', '*', 's']\n",
        "# Generate synthetic data\n",
        "X, y = make_blobs(n_samples = [500, 300, 200],\n",
        "                  centers=[[0, 0], [2, 2], [4, 4]],\n",
        "                  n_features=2,\n",
        "                  random_state=0, cluster_std=[1.0, 1, .6])\n",
        "\n",
        "# Create a scatter plot using Seaborn\n",
        "fig, ax = plt.subplots(1, 2, figsize=(10, 7), gridspec_kw={'width_ratios': [8, 2]})\n",
        "\n",
        "for num in np.unique(y):\n",
        "    ax[0].scatter(X[:, 0][y == num], X[:, 1][y == num], c=colors[num],\n",
        "               s=40, ec=edge_colors[num], marker=markers[num], label=str(num))\n",
        "\n",
        "ax[0].grid(True)\n",
        "ax[0].legend(title='Class', fontsize=14)\n",
        "ax[0].set(xlim = [-4, 6], ylim = [-4, 6], xlabel = 'Feature 1', ylabel = 'Feature 2')\n",
        "ax[0].set_title('Synthetic Dataset', weight = 'bold', fontsize = 16, y = 1.02)\n",
        "\n",
        "bar_heights, bar_labels = np.unique(y, return_counts=True)\n",
        "bars = ax[1].bar(bar_heights, bar_labels, color=colors, edgecolor='k')\n",
        "\n",
        "# Add xticks with labels\n",
        "ax[1].set_xticks(bar_heights)\n",
        "ax[1].set_xticklabels(['0', '1', '2'])\n",
        "\n",
        "ax[1].grid(which='major', axis='y')\n",
        "ax[1].set_title('Class Distribution', weight='bold', fontsize=16, y = 1.02)\n",
        "\n",
        "# Add labels for bar heights inside each bar\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    Percentage = 100*height/X.shape[0]\n",
        "    ax[1].annotate(f'{Percentage:.1f}%', xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                   xytext=(0, 3), textcoords='offset points', ha='center', fontsize=12)\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SK1Cw98KQs-X"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn import metrics\n",
        "from scipy.stats import sem\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "# Function to print a line of underscores for separation\n",
        "def _Line(n = 60, c = '_'):\n",
        "    print(n * c)\n",
        "\n",
        "def print_bold(txt, c = 31):\n",
        "    print(f\"\\033[1;{c}m\" + txt + \"\\033[0m\")\n",
        "\n",
        "# Create classifier instances\n",
        "models = [RandomForestClassifier(),\n",
        "          GradientBoostingClassifier()]\n",
        "model_names = ['Random Forest Classifier', 'Gradient Boosting Classifier']\n",
        "model_alphs = 'ab'\n",
        "\n",
        "# Loop through each model\n",
        "for model, name, alph in zip(models, model_names, model_alphs):\n",
        "    _Line(n = 80, c = '=')\n",
        "    print_bold(f'({alph}) {name}', c = 34)\n",
        "    _Line(n = 80, c = '=')\n",
        "\n",
        "    # Initialize KFold cross-validator\n",
        "    n_splits = 5\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    # The splitt would be 80-20!\n",
        "\n",
        "    # Lists to store train and test scores for each fold\n",
        "    train_acc_scores, test_acc_scores, train_f1_scores, test_f1_scores = [], [], [], []\n",
        "    train_class_proportions, test_class_proportions = [], []\n",
        "\n",
        "    # Perform Cross-Validation\n",
        "    for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), 1):\n",
        "        X_train, X_test = X[train_idx], X[test_idx]\n",
        "        y_train, y_test = y[train_idx], y[test_idx]\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Calculate class proportions for train and test sets\n",
        "        train_class_proportions.append([np.mean(y_train == model) for model in np.unique(y)])\n",
        "        test_class_proportions.append([np.mean(y_test == model) for model in np.unique(y)])\n",
        "\n",
        "        # train\n",
        "        y_train_pred = model.predict(X_train)\n",
        "        train_acc_scores.append(metrics.accuracy_score(y_train, y_train_pred))\n",
        "        train_f1_scores.append(metrics.f1_score(y_train, y_train_pred, average = 'weighted'))\n",
        "\n",
        "        # test\n",
        "        y_test_pred = model.predict(X_test)\n",
        "        test_acc_scores.append(metrics.accuracy_score(y_test, y_test_pred))\n",
        "        test_f1_scores.append(metrics.f1_score(y_test, y_test_pred, average = 'weighted'))\n",
        "\n",
        "    _Line()\n",
        "    #  Print the Train and Test Scores for each fold\n",
        "    for fold in range(n_splits):\n",
        "        print_bold(f'Fold {fold + 1}:')\n",
        "        print(f\"\\tTrain Class Proportions: {train_class_proportions[fold]}*{len(y_train)}\")\n",
        "        print(f\"\\tTest Class Proportions: {test_class_proportions[fold]}*{len(y_test)}\")\n",
        "        print(f\"\\tTrain Accuracy Score = {train_acc_scores[fold]:.5f}, Test Accuracy Score = {test_acc_scores[fold]:.5f}\")\n",
        "        print(f\"\\tTrain F1 Score (weighted) = {train_f1_scores[fold]:.5f}, Test F1 Score (weighted)= {test_f1_scores[fold]:.5f}\")\n",
        "\n",
        "    df_scores = pd.DataFrame({'Fold': np.arange(1, len(train_acc_scores)+1),\n",
        "                          'Train Accuracy Score':train_acc_scores,\n",
        "                          'Test Accuracy Score':test_r2_scores,\n",
        "                          'Train F1 Score (weighted)':train_f1_scores,\n",
        "                          'Test F1 Score (weighted)':test_f1_scores,\n",
        "                          }).set_index('Fold')\n",
        "    display(df_scores)\n",
        "\n",
        "    _Line()\n",
        "    print_bold('Accuracy Score:')\n",
        "    print(f\"\\tMean Train Accuracy Score: {np.mean(train_acc_scores):.5f} ± {sem(train_acc_scores, ddof = 0):.5f}\")\n",
        "    print(f\"\\tMean Test Accuracy Score: {np.mean(test_acc_scores):.5f} ± {sem(test_acc_scores, ddof = 0):.5f}\")\n",
        "    print_bold('F1 Score:')\n",
        "    print(f\"\\tMean Train F1 Accuracy Score (weighted): {np.mean(train_f1_scores):.5f} ± {sem(train_f1_scores, ddof = 0):.5f}\")\n",
        "    print(f\"\\tMean Test F1 Accuracy Score (weighted): {np.mean(test_f1_scores):.5f} ± {sem(test_f1_scores, ddof = 0):.5f}\")\n",
        "    _Line()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HueJqyK8YsXF"
      },
      "source": [
        "It does look like overfitting. Overfitting is when a model performs well on the training data, but poorly on the test data. This means that the model has learned the specific patterns and noise in the training data, but fails to generalize to new and unseen data. Overfitting can lead to poor performance and unreliable predictions.\n",
        "\n",
        "There are several ways to prevent or reduce overfitting in gradient-boosting classifiers, such as:\n",
        "\n",
        "- Limiting the number of trees in the model. More trees can increase the complexity and variance of the model, leading to overfitting. We can use the `n_estimators` parameter to control the number of trees.\n",
        "\n",
        "- Applying learning rates. Learning rates can shrink the contribution of each tree, making the model more robust to noise and outliers. We can use the `learning_rate` parameter to adjust the learning rate.\n",
        "\n",
        "- Tuning other hyperparameters, such as `max_depth`, `min_samples_split`, `min_samples_leaf`, `max_features`, and `gamma`. These parameters can control the size and complexity of the trees, and the minimum loss reduction required to make a further split.\n",
        "\n",
        "- We can use the cross-validation grid search to find the optimal values for these parameters."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}