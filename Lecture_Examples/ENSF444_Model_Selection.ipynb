{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Example: Iris Flower Dataset\n",
        "\n",
        "-   The [Iris Flower\n",
        "    dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set),\n",
        "    introduced by the British biologist and statistician [Ronald A.\n",
        "    Fisher](https://en.wikipedia.org/wiki/Ronald_Fisher) in 1936, serves\n",
        "    as a benchmark dataset in the field of pattern recognition and\n",
        "    machine learning.\n",
        "\n",
        "-   This dataset comprises 150 instances of iris flowers, each belonging\n",
        "    to one of three species: **setosa, versicolor, or virginica**.\n",
        "\n",
        "-   **Four measured features**: sepal length, sepal width, petal length,\n",
        "    and petal width, all in centimeters \\[scikit-learn Developers,\n",
        "    2023\\].\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/max/3500/1*f6KbPXwksAliMIsibFyGJw.png\" alt=\"picture\" width=\"700\">\n",
        "<br>\n",
        "<b>Figure</b>: Image sourced\n",
        "from Kaggle.com\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "EeCUHy6fJ110"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "print(iris['DESCR'])\n",
        "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "iris_df['species'] = iris.target_names[iris.target]\n",
        "\n",
        "# Display the Iris dataset\n",
        "display(iris_df)"
      ],
      "metadata": {
        "id": "512jzsuzBynZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Use a custom style for the plot (adjust the path to your style file)\n",
        "plt.style.use('https://raw.githubusercontent.com/HatefDastour/ENSF444/main/Files/mystyle.mplstyle')\n",
        "\n",
        "# Perform the train-test split\n",
        "X = iris_df.drop('species', axis=1)  # Features\n",
        "y = iris_df['species']  # Labels\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create dataframes for train and test sets\n",
        "train_data = pd.DataFrame({'Set': 'Train', 'Class': y_train})\n",
        "test_data = pd.DataFrame({'Set': 'Test', 'Class': y_test})\n",
        "\n",
        "# Combine the training and testing sets for visualization using pd.concat\n",
        "combined_data = pd.concat([train_data, test_data], ignore_index=True)\n",
        "\n",
        "# Create a bar plot\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "sns.countplot(x='Class', hue='Set', ec='k', lw=1.25, data=combined_data, palette=['#46aa46', '#2986cc'], ax=ax)\n",
        "\n",
        "# Set plot properties\n",
        "ax.set_title('Distribution of Classes in Train and Test Sets (Iris Dataset)', weight='bold')\n",
        "ax.set(xlabel='Class', ylabel='Count', ylim=[0, 45])\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a7A8PdhFKCf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='Blue'><b>Example</b></font>: Returning to the Iris dataset, we employ a cross-validation strategy with a 5-fold partitioning scheme. In this approach, the dataset is divided into five subsets of roughly equal size, and the model is trained and evaluated five times. Each time, a different subset is held out as the test set, while the remaining four subsets are combined to form the training set. This process ensures that every data point is used for both training and testing, leading to a more robust evaluation of the model's performance."
      ],
      "metadata": {
        "id": "6MXQjybqUlLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use groupby to count observations by species\n",
        "species_counts = iris_df.groupby('species').size().to_frame('Count')\n",
        "\n",
        "print(\"Number of Observations by Species:\")\n",
        "display(species_counts.T)"
      ],
      "metadata": {
        "id": "08wLwjKLUCpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def print_bold(txt):\n",
        "    print(\"\\033[1;34m\" + txt + \"\\033[0m\")\n",
        "\n",
        "def print_bold(txt):\n",
        "    print(\"\\033[1;34m\" + txt + \"\\033[0m\")\n",
        "\n",
        "n_splits = 5\n",
        "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform Cross-Validation\n",
        "for C, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
        "    print_bold(f'Fold {C + 1}:')\n",
        "\n",
        "    # Extract target names for train and test sets\n",
        "    train_targets = iris.target_names[iris.target][train_idx]\n",
        "    test_targets = iris.target_names[iris.target][test_idx]\n",
        "\n",
        "    # Create DataFrames for train and test sets\n",
        "    df_train = pd.Series(train_targets).value_counts().to_frame('').T\n",
        "    df_train.insert(0, \"Set\", ['Train'])\n",
        "\n",
        "    df_test = pd.Series(test_targets).value_counts().to_frame('').T\n",
        "    df_test.insert(0, \"Set\", ['Test'])\n",
        "\n",
        "    # Concatenate DataFrames\n",
        "    df_concat = pd.concat([df_train, df_test])\n",
        "\n",
        "    # Define styles for the data rows\n",
        "    styles = [{'selector': 'tbody tr:nth-child(1)', 'props': [('background-color', '#f0fff0')]},\n",
        "              {'selector': 'tbody tr:nth-child(2)', 'props': [('background-color', '#cfe2f3')]}]\n",
        "\n",
        "    # Apply styles to the DataFrame and display it\n",
        "    display(df_concat.style.set_table_styles(styles))"
      ],
      "metadata": {
        "id": "pCcOprKKUodu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When working with a small dataset like the Iris dataset, which has only 150 samples, the choice of cross-validation strategy becomes even more crucial. Using StratifiedKFold for splitting a small dataset into train and test sets offers several benefits:\n",
        "\n",
        "1. **Preserving Data Distribution**: The Iris dataset contains three classes (species) with 50 samples each. If you use simple random splitting without stratification, there's a chance that one or more classes might be underrepresented or absent in either the train or test set. StratifiedKFold ensures that each fold maintains the original class distribution, which is particularly important when working with a limited number of samples.\n",
        "\n",
        "2. **More Reliable Performance Estimates**: In small datasets, individual data points can have a larger impact on model training and evaluation. By using stratification, you're making sure that each fold accurately represents the underlying data distribution. This leads to more reliable performance estimates and reduces the risk of overfitting or underestimation.\n",
        "\n",
        "3. **Preventing Overfitting**: Small datasets are prone to overfitting, especially if you're using a complex model. Using stratified cross-validation helps in mitigating this risk by providing consistent evaluation across folds and ensuring that each fold has a representative distribution of classes.\n",
        "\n",
        "4. **Robustness to Variability**: Small datasets often have more variability in terms of data distribution and noise. StratifiedKFold provides a way to handle this variability by maintaining class balance, leading to a more stable evaluation process.\n",
        "\n",
        "5. **Comparable Results**: StratifiedKFold ensures that performance metrics are calculated over similar data distributions for each fold. This makes your results more comparable and interpretable."
      ],
      "metadata": {
        "id": "bc3ZbcRRHBtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "n_splits = 5\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "def print_bold(txt):\n",
        "    print(\"\\033[1;31m\" + txt + \"\\033[0m\")\n",
        "\n",
        "# Perform Cross-Validation\n",
        "for C, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n",
        "    print_bold(f'Fold {C + 1}:')\n",
        "\n",
        "    # Extract target names for train and test sets\n",
        "    train_targets = iris.target_names[iris.target][train_idx]\n",
        "    test_targets = iris.target_names[iris.target][test_idx]\n",
        "\n",
        "    # Create DataFrames for train and test sets\n",
        "    df_train = pd.Series(train_targets).value_counts().to_frame('').T\n",
        "    df_train.insert(0, \"Set\", ['Train'])\n",
        "\n",
        "    df_test = pd.Series(test_targets).value_counts().to_frame('').T\n",
        "    df_test.insert(0, \"Set\", ['Test'])\n",
        "\n",
        "    # Concatenate DataFrames\n",
        "    df_concat = pd.concat([df_train, df_test])\n",
        "\n",
        "    # Define styles for the data rows\n",
        "    styles = [{'selector': 'tbody tr:nth-child(1)', 'props': [('background-color', '#f0fff0')]},\n",
        "              {'selector': 'tbody tr:nth-child(2)', 'props': [('background-color', '#cfe2f3')]}]\n",
        "\n",
        "    # Apply styles to the DataFrame and display it\n",
        "    display(df_concat.style.set_table_styles(styles))"
      ],
      "metadata": {
        "id": "rsRSh9OyUo4u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}