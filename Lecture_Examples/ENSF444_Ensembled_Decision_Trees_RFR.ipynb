{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forests"
      ],
      "metadata": {
        "id": "c4go456S7pLq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Random Forest Regressor\n",
        "\n",
        "<font color='Blue'><b>Example</b></font>. The Auto MPG dataset retrieved from the [UCI Machine Learning Repository](http://archive.ics.uci.edu/dataset/9/auto+mpg)."
      ],
      "metadata": {
        "id": "CHMzPK9JGDeS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from ucimlrepo import fetch_ucirepo\n",
        "except ImportError:\n",
        "    !pip3 install -U ucimlrepo\n",
        "    from ucimlrepo import fetch_ucirepo\n",
        "import numpy as np\n",
        "\n",
        "# fetch dataset\n",
        "auto_mpg = fetch_ucirepo(name = 'Auto MPG')\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X = auto_mpg.data.features\n",
        "y = auto_mpg.data.targets\n",
        "\n",
        "# drop rows with missing values from X\n",
        "X = X.dropna(axis=0, how='any')\n",
        "\n",
        "# align X and y by index\n",
        "X, y = X.align(y, join='inner', axis=0)\n",
        "\n",
        "# ln(mpg)\n",
        "y = np.log(y['mpg'])\n",
        "y.name = 'ln(mpg)'\n",
        "print('X:')\n",
        "display(X)\n",
        "print('\\ny:')\n",
        "print(y)\n",
        "print('\\nInfo:')\n",
        "X.info()"
      ],
      "metadata": {
        "id": "70VcVB_FOSpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
        "set_size_df = pd.DataFrame({'Size': [len(X_train), len(X_test)]}, index = ['Train', 'Test'])\n",
        "display(set_size_df.T)"
      ],
      "metadata": {
        "id": "9YG9_8OtOV_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our goal is to create a Random Forest Regression model with certain specifications. In this setup, the model consists of four decision trees, each constrained to a maximum of three leaf nodes. The intention behind these parameter choices is to build an ensemble of decision trees that work together to make accurate regression predictions. The restriction on the number of nodes in each tree serves to manage the overall complexity of the model. The next step involves training the model using the provided training data, where `X_train` represents the features, and `y_train` represents the corresponding target values. Throughout this training process, the model evaluates the importance of each feature, contributing to a comprehensive understanding of its predictive capabilities."
      ],
      "metadata": {
        "id": "b8qQKAqUGB0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn import metrics\n",
        "from sklearn import tree\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "plt.style.use('https://raw.githubusercontent.com/HatefDastour/ENSF444/main/Files/mystyle.mplstyle')\n",
        "\n",
        "# RandomForestRegressor with specified parameters\n",
        "rfr = RandomForestRegressor(n_estimators=4, random_state=0, max_leaf_nodes=3)\n",
        "rfr.fit(X_train, y_train)\n",
        "\n",
        "# Create subplots for each estimator\n",
        "fig, ax = plt.subplots(2, 2, figsize=(11, 8))\n",
        "ax = ax.ravel()\n",
        "\n",
        "# Initialize DataFrame for feature importance\n",
        "feat_importance_df = pd.DataFrame()\n",
        "\n",
        "# Iterate over estimators to plot trees and calculate MSE\n",
        "for i, (estimator, ax) in enumerate(zip(rfr.estimators_, ax), start=1):\n",
        "    tree.plot_tree(estimator, ax=ax, feature_names=X.columns.tolist(), filled=True,\n",
        "                   fontsize=11, rounded=True)\n",
        "\n",
        "    ax.set_title(f'Estimator {i}', fontsize=14, weight='bold')\n",
        "\n",
        "    # Calculate MSE for both training and test sets\n",
        "    mse_train = metrics.mean_squared_error(y_train, estimator.predict(X_train.values))\n",
        "    mse_test = metrics.mean_squared_error(y_test, estimator.predict(X_test.values))\n",
        "    txt = f'MSE (Train) = {mse_train:.5f}\\nMSE (Test) = {mse_test:.5f}'\n",
        "    print(f'\\nEstimator {i}:\\n'+ txt)\n",
        "\n",
        "    # Display MSE values on each subplot\n",
        "    text = ax.text(0.4, -0.05, txt,\n",
        "                  transform=ax.transAxes, fontsize=11, weight='bold',\n",
        "                  bbox=dict(facecolor='#dfc8f0', alpha=0.7))\n",
        "\n",
        "    # Create DataFrame with feature importances for each estimator\n",
        "    df_temp = pd.DataFrame({f'Estimator {i}': 100*estimator.feature_importances_}, index=X.columns)\n",
        "    feat_importance_df = pd.concat([feat_importance_df, df_temp], axis=1)\n",
        "\n",
        "# Ensure tight layout for subplots\n",
        "plt.tight_layout()\n",
        "\n",
        "# Apply background gradient to the DataFrame and round importance values to 2 decimal places\n",
        "styled_importance = feat_importance_df.style.\\\n",
        "                    background_gradient(cmap='Reds', axis=1, vmin=0, vmax=100).format(precision=2)\n",
        "\n",
        "# Display the styled DataFrame\n",
        "print('\\nFeature Importance:')\n",
        "display(styled_importance)"
      ],
      "metadata": {
        "id": "l5CCzl1T74aC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxWeo9V27dWk"
      },
      "outputs": [],
      "source": [
        "mse_train = metrics.mean_squared_error(y_train, rfr.predict(X_train))\n",
        "mse_test = metrics.mean_squared_error(y_test, rfr.predict(X_test))\n",
        "txt = f'MSE (Train) = {mse_train:.5f}\\nMSE (Test) = {mse_test:.5f}'\n",
        "print(txt)\n",
        "\n",
        "# Create a DataFrame to store feature importances\n",
        "Importance = pd.DataFrame({'Importance': 100*rfr.feature_importances_}, index = X.columns)\n",
        "\n",
        "# Apply a background gradient to the DataFrame and round importance values to 2 decimal places\n",
        "styled_importance = Importance.style.background_gradient(cmap='Oranges',\n",
        "                                                         subset=['Importance']).format({'Importance': '{:.2f}'})\n",
        "\n",
        "# Display the styled DataFrame\n",
        "display(styled_importance)\n",
        "\n",
        "# Create a bar plot to visualize feature importances\n",
        "fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
        "bars = ax.bar(Importance.index, Importance.Importance,\n",
        "              color='#f9cb9c', edgecolor='#cc0000', hatch=\"\\\\\\\\\", lw=2, zorder = 2)\n",
        "\n",
        "# Set plot labels and title\n",
        "ax.set_xlabel('Features', fontsize=12, weight='bold', color='#191970')\n",
        "ax.set_ylabel('Importance', fontsize=12, weight='bold', color='#191970')\n",
        "ax.set_title('Feature Importance', y = 1.05,\n",
        "             fontsize=16, weight='bold', color='#2F4F4F')\n",
        "\n",
        "# Set y-axis limits and adjust tick parameters\n",
        "ax.set_ylim([0, 80])\n",
        "ax.tick_params(axis='x', rotation=45, labelsize=12, color='#696969')\n",
        "ax.tick_params(axis='y', labelsize=12, color='#696969')\n",
        "\n",
        "# Customize plot aesthetics\n",
        "ax.spines[['top', 'right']].set_visible(False)\n",
        "ax.spines[['bottom', 'left']].set_color('#696969')\n",
        "ax.grid(axis='x')\n",
        "\n",
        "# Ensure a tight layout for better visualization\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predictions"
      ],
      "metadata": {
        "id": "8m_0ivfUit1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_x_test = X_test.iloc[-2:-1]\n",
        "display(sample_x_test)\n",
        "pred_list = []\n",
        "for i, estimator in enumerate(rfr.estimators_, start=1):\n",
        "  pred_ = estimator.predict(sample_x_test.values)[0]\n",
        "  pred_list.append(pred_)\n",
        "  print(f'Prediction from Estimator {i} = {pred_:.6f}')\n",
        "\n",
        "print(f'\\nMean Prediction from All Estimators = {np.mean(pred_list):.6f}')\n",
        "print(f'\\nPrediction from RFR = {rfr.predict(sample_x_test)[0]:.6f}')"
      ],
      "metadata": {
        "id": "19FzoEVdi3qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Number of Estimators"
      ],
      "metadata": {
        "id": "fe0UXNPmZp2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import OrderedDict\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Define the ensemble regressors with specific parameters\n",
        "ensemble_regrs = [(\"RandomForestRegressor, max_leaf_nodes=5\",\n",
        "                    RandomForestRegressor(random_state=0, max_leaf_nodes=5),\n",
        "                   ),\n",
        "                  (\"RandomForestRegressor, max_leaf_nodes=7\",\n",
        "                    RandomForestRegressor(random_state=0, max_leaf_nodes= 7),\n",
        "                   ),\n",
        "                  (\"RandomForestRegressor, max_leaf_nodes=11\",\n",
        "                    RandomForestRegressor(random_state=0, max_leaf_nodes= 11),\n",
        "                   )\n",
        "]\n",
        "\n",
        "# Initialize dictionaries to store error rates and test mean squared errors\n",
        "error_rate = OrderedDict((label, []) for label, _ in ensemble_regrs)\n",
        "test_error = OrderedDict((label, []) for label, _ in ensemble_regrs)\n",
        "\n",
        "# Define the range of `n_estimators` values to explore\n",
        "min_estimators = 15\n",
        "max_estimators = 120\n",
        "\n",
        "# Iterate over ensemble regressors and `n_estimators` values\n",
        "for label, regr in ensemble_regrs:\n",
        "    for i in range(min_estimators, max_estimators + 1, 5):\n",
        "        # Set the number of estimators\n",
        "        regr.set_params(n_estimators=i)\n",
        "\n",
        "        # Fit the model on the training data\n",
        "        regr.fit(X_train, y_train)\n",
        "\n",
        "        # Record the train mean squared error for each `n_estimators=i` setting\n",
        "        oob_pred = regr.predict(X_train)\n",
        "        oob_mse = mean_squared_error(y_train, oob_pred)\n",
        "        error_rate[label].append((i, oob_mse))\n",
        "\n",
        "        # Record the test mean squared error for each `n_estimators=i` setting\n",
        "        test_pred = regr.predict(X_test)\n",
        "        test_mse = mean_squared_error(y_test, test_pred)\n",
        "        test_error[label].append((i, test_mse))"
      ],
      "metadata": {
        "id": "2j-IgfAwZrrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create subplots using fig and ax, sharing x-axis\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(11, 6), sharex=True)\n",
        "\n",
        "for label, regr_err in error_rate.items():\n",
        "    xs, ys = zip(*regr_err)\n",
        "    ax1.plot(xs, ys, lw=2, label=label)\n",
        "\n",
        "for label, regr_err in test_error.items():\n",
        "    xs, ys = zip(*regr_err)\n",
        "    ax2.plot(xs, ys, lw=2, label=label)\n",
        "\n",
        "# Set y-axis to logarithmic scale\n",
        "ax1.set_yscale('log')\n",
        "ax2.set_yscale('log')\n",
        "\n",
        "# Set plot parameters and display the plots\n",
        "ax1.set_xlim(min_estimators, max_estimators)\n",
        "ax1.set_ylabel(\"Mean Squared Error\\n(Train Set)\")\n",
        "\n",
        "ax2.set_xlim(min_estimators, max_estimators)\n",
        "ax2.set_xlabel(\"n_estimators\")\n",
        "ax2.set_ylabel(\"Mean Squared Error\\n(Test Set)\")\n",
        "\n",
        "# Combine legends for both plots\n",
        "ax1.legend(loc=\"upper left\", bbox_to_anchor=(0.56, 1.35))\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "aPuoDD7gZ16e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest Classifier"
      ],
      "metadata": {
        "id": "bS0qexHSjgTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
        "\n",
        "# Use a custom style for the plot (adjust the path to your style file)\n",
        "plt.style.use('https://raw.githubusercontent.com/HatefDastour/ENSF444/main/Files/mystyle.mplstyle')\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Create a DataFrame for visualization\n",
        "df = pd.DataFrame(data=X, columns=feature_names)\n",
        "df['class'] = y\n",
        "display(df)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\n",
        "\n",
        "# Initialize RandomForestClassifier with specified parameters\n",
        "rfc = RandomForestClassifier(n_estimators=4, random_state=0, max_leaf_nodes=3)\n",
        "rfc.fit(X_train, y_train)\n",
        "\n",
        "# Display accuracy score for each estimator\n",
        "for i, estimator in enumerate(rfc.estimators_, start=1):\n",
        "    accuracy_train = metrics.accuracy_score(estimator.predict(X_train), y_train)\n",
        "    accuracy_test = metrics.accuracy_score(estimator.predict(X_test), y_test)\n",
        "    txt = f'Estimator {i}: Accuracy (Train) = {accuracy_train:.4f}, Accuracy (Test) = {accuracy_test:.4f}'\n",
        "    print(txt)"
      ],
      "metadata": {
        "id": "ui8X5V4dvZsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.tree import plot_tree\n",
        "\n",
        "# Plot decision trees in a 2 by 2 layout\n",
        "fig, axes = plt.subplots(2, 2, figsize=(13, 9))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i, (estimator, ax) in enumerate(zip(rfc.estimators_, axes), start=1):\n",
        "    # Plot decision tree\n",
        "    plot_tree(estimator, filled=False, feature_names=feature_names, class_names= iris.target_names,\n",
        "              ax=ax, fontsize=11, impurity=True, rounded=True, proportion= True)\n",
        "    ax.set_title(f'Estimator {i}', fontsize=14, weight='bold')\n",
        "\n",
        "# Add a super title for the entire figure\n",
        "fig.suptitle('Decision Trees', fontsize=16, weight='bold')\n",
        "\n",
        "# Adjust layout and display the plots\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "P6B2gRopn87t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_train = metrics.accuracy_score(y_train, rfc.predict(X_train))\n",
        "accuracy_test = metrics.accuracy_score(y_test, rfc.predict(X_test))\n",
        "txt = f'Accuracy Score(Train) = {accuracy_train:.5f}\\nAccuracy Score(Test) = {accuracy_test:.5f}'\n",
        "print(txt)\n",
        "\n",
        "# Create a DataFrame to store feature importances\n",
        "Importance = pd.DataFrame({'Importance': 100*rfc.feature_importances_}, index = feature_names)\n",
        "\n",
        "# Apply a background gradient to the DataFrame and round importance values to 2 decimal places\n",
        "styled_importance = Importance.style.background_gradient(cmap='Oranges',\n",
        "                                                         subset=['Importance']).format({'Importance': '{:.2f}'})\n",
        "\n",
        "# Display the styled DataFrame\n",
        "display(styled_importance)\n",
        "\n",
        "# Create a bar plot to visualize feature importances\n",
        "fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
        "bars = ax.bar(Importance.index, Importance.Importance,\n",
        "              color='#f9cb9c', edgecolor='#cc0000', hatch=\"\\\\\\\\\", lw=2, zorder = 2)\n",
        "\n",
        "# Set plot labels and title\n",
        "ax.set_xlabel('Features', fontsize=12, weight='bold', color='#191970')\n",
        "ax.set_ylabel('Importance', fontsize=12, weight='bold', color='#191970')\n",
        "ax.set_title('Feature Importance', y = 1.05,\n",
        "             fontsize=16, weight='bold', color='#2F4F4F')\n",
        "\n",
        "# Set y-axis limits and adjust tick parameters\n",
        "ax.set_ylim([0, 80])\n",
        "ax.tick_params(axis='x', rotation=45, labelsize=12, color='#696969')\n",
        "ax.tick_params(axis='y', labelsize=12, color='#696969')\n",
        "\n",
        "# Customize plot aesthetics\n",
        "ax.spines[['top', 'right']].set_visible(False)\n",
        "ax.spines[['bottom', 'left']].set_color('#696969')\n",
        "ax.grid(axis='x')\n",
        "\n",
        "# Ensure a tight layout for better visualization\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "kjJeXylzunAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
        "\n",
        "# Use a custom style for the plot (adjust the path to your style file)\n",
        "plt.style.use('https://raw.githubusercontent.com/HatefDastour/ENSF444/main/Files/mystyle.mplstyle')\n",
        "\n",
        "def plot_cm(model, X_train, X_test, y_train, y_test, class_names, figsize=(12, 8), title='Confusion Matrices'):\n",
        "    # Create a figure and axes for displaying confusion matrices side by side\n",
        "    fig, ax = plt.subplots(1, 2, figsize=figsize)\n",
        "\n",
        "    datasets = [(X_train, y_train, 'Train'), (X_test, y_test, 'Test')]\n",
        "\n",
        "    for i in range(2):\n",
        "        X, y, dataset_name = datasets[i]\n",
        "\n",
        "        # Compute confusion matrix for the dataset predictions\n",
        "        cm = confusion_matrix(y, model.predict(X))\n",
        "\n",
        "        # Create a ConfusionMatrixDisplay and plot it on the respective axis\n",
        "        cm_display = ConfusionMatrixDisplay(cm, display_labels=class_names) \\\n",
        "            .plot(ax=ax[i],\n",
        "                  im_kw=dict(cmap='Greens' if dataset_name == 'Train' else 'Blues'),\n",
        "                  text_kw={\"size\": 16}, colorbar=False)\n",
        "        ax[i].set_title(f'{dataset_name} Data')\n",
        "        ax[i].grid(False)\n",
        "\n",
        "    # Rotate x-axis labels\n",
        "    for ax_ in ax:\n",
        "        ax_.set_xticklabels(ax_.get_xticklabels(), rotation=45, ha='right')\n",
        "\n",
        "    # Add a super title for the entire figure\n",
        "    fig.suptitle(title, fontsize=16, weight='bold')\n",
        "\n",
        "    # Adjust the layout for better spacing\n",
        "    plt.tight_layout()\n",
        "\n",
        "# Assuming rfc and axes are defined before this point\n",
        "for i, (estimator, ax) in enumerate(zip(rfc.estimators_, axes), start=1):\n",
        "    plot_cm(estimator, X_train, X_test, y_train, y_test, class_names=iris.target_names,\n",
        "            title=f'Confusion Matrices for Estimator {i}', figsize=(6, 3))\n"
      ],
      "metadata": {
        "id": "-GGmApQRpkjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_cm(rfc, X_train, X_test, y_train, y_test, class_names = iris.target_names, figsize=(10, 5), title='Confusion Matrices for RFC')"
      ],
      "metadata": {
        "id": "SRy5i6PC3zBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predictions"
      ],
      "metadata": {
        "id": "fiSi-a_kszXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_x_test = X_test[-2:-1,:]\n",
        "display(pd.DataFrame( sample_x_test, columns = feature_names))\n",
        "\n",
        "pred_list = []\n",
        "\n",
        "def most_frequent_item(lst):\n",
        "    unique_elements, counts = np.unique(lst, return_counts=True)\n",
        "    index_of_max_frequency = np.argmax(counts)\n",
        "    most_frequent_string = unique_elements[index_of_max_frequency]\n",
        "    return most_frequent_string\n",
        "\n",
        "for i, estimator in enumerate(rfc.estimators_, start=1):\n",
        "  pred_ = estimator.predict(sample_x_test)[0]\n",
        "  pred_ = iris.target_names[int(pred_)]\n",
        "  pred_list.append(pred_)\n",
        "  print(f'Prediction from Estimator {i} = {pred_}')\n",
        "\n",
        "print(f'\\nMode Prediction from All Estimators = {most_frequent_item(pred_list)}')\n",
        "pred_ = rfc.predict(sample_x_test)[0]\n",
        "pred_ = iris.target_names[int(pred_)]\n",
        "print(f'\\nPrediction from RFR = {pred_}')"
      ],
      "metadata": {
        "id": "wwW2YAXPp56w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}