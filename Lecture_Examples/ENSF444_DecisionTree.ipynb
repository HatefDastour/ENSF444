{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUP6zOXlWb9t"
   },
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDIEPlCeV5yQ"
   },
   "source": [
    "Gini is used in decision tree classification as a measure of how well a node or a split can separate the data into different classes. The lower the Gini value, the better the separation. The Gini value is calculated as:\n",
    "\n",
    "$$\n",
    "Gini = 1 - \\sum_{i=1}^k p_i^2\n",
    "$$\n",
    "\n",
    "where $k$ is the number of classes and $p_i$ is the probability of choosing an element of class $i$. The Gini value ranges from 0 to 0.5, where 0 means the node is pure (all elements belong to the same class) and 0.5 means the node is impure (equal probability of choosing any class).\n",
    "\n",
    "To use Gini in decision tree classification, the algorithm compares the Gini values of different possible splits and chooses the one that minimizes the Gini value. This means that the algorithm tries to find the best feature and the best threshold to divide the data into two subsets, such that the subsets are more pure than the original node. The algorithm repeats this process recursively until all the nodes are pure or some stopping criteria are met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7qPH_ZOWh-p"
   },
   "source": [
    "## Decision Tree Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 893
    },
    "id": "7XBamZ1qbofL",
    "outputId": "08d4e27b-e902-4f95-ee9d-9389b88cf132",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "# Use a custom style for the plot (adjust the path to your style file)\n",
    "plt.style.use('https://raw.githubusercontent.com/HatefDastour/ENSF444/main/Files/mystyle.mplstyle')\n",
    "\n",
    "def print_bold(txt, c=31):\n",
    "    \"\"\"\n",
    "    Function to print text in bold with specified color.\n",
    "\n",
    "    Parameters:\n",
    "    - txt (str): Text to be printed.\n",
    "    - c (int): Color code for the printed text.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    print(f\"\\033[1;{c}m\" + txt + \"\\033[0m\")\n",
    "\n",
    "def format_confusion_matrix(cm, title):\n",
    "    true_neg, false_pos, false_neg, true_pos = cm.ravel()\n",
    "    result = f\"\\033[1m{title} Set Confusion Matrix\\033[0m:\\n\"\n",
    "    result += f\"- {true_neg} instances were correctly predicted as class 0.\\n\"\n",
    "    result += f\"- {true_pos} instances were correctly predicted as class 1.\\n\"\n",
    "    result += f\"- {false_pos} instance was incorrectly predicted as class 1 when it was actually class 0.\\n\"\n",
    "    result += f\"- {false_neg} instances were incorrectly predicted as class 0 when they were actually class 1.\\n\"\n",
    "\n",
    "    return result\n",
    "\n",
    "def plot_cm(model, X_train, X_test, y_train, y_test, class_names, figsize=(7, 4)):\n",
    "    # Create a figure and axes for displaying confusion matrices side by side\n",
    "    fig, ax = plt.subplots(1, 2, figsize=figsize)\n",
    "\n",
    "    datasets = [(X_train, y_train, 'Train'), (X_test, y_test, 'Test')]\n",
    "\n",
    "    for i in range(2):\n",
    "        X, y, dataset_name = datasets[i]\n",
    "\n",
    "        # Compute confusion matrix for the dataset predictions\n",
    "        cm = confusion_matrix(y, model.predict(X))\n",
    "\n",
    "        result = format_confusion_matrix(cm, dataset_name)\n",
    "        print(result)\n",
    "\n",
    "        # Create a ConfusionMatrixDisplay and plot it on the respective axis\n",
    "        cm_display = ConfusionMatrixDisplay(cm, display_labels=class_names)\\\n",
    "                        .plot(ax=ax[i],\n",
    "                              im_kw=dict(cmap='Greens' if dataset_name == 'Train' else 'Blues'),\n",
    "                              text_kw={\"size\": 16}, colorbar=False)\n",
    "        ax[i].set_title(f'{dataset_name} Data')\n",
    "        ax[i].grid(False)\n",
    "\n",
    "    # Add a super title for the entire figure\n",
    "    fig.suptitle('Confusion Matrices', fontsize=16, weight = 'bold')\n",
    "\n",
    "    # Adjust the layout for better spacing\n",
    "    plt.tight_layout()\n",
    "    \n",
    "feature_names = [f'Feature_{i + 1}' for i in range(2)]\n",
    "\n",
    "# Generate synthetic data using make_blobs\n",
    "n_samples = 200\n",
    "n_features = 2\n",
    "centers = 2\n",
    "cluster_std = 1.0\n",
    "X, y = make_blobs(n_samples=n_samples, n_features=n_features,\n",
    "                  centers=centers, random_state=0, cluster_std=cluster_std)\n",
    "df = pd.DataFrame(data=X, columns=feature_names)\n",
    "df['y'] = y\n",
    "display(df)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                    random_state=0, stratify=y)\n",
    "\n",
    "#  Plot decision boundaries\n",
    "fig, axes = plt.subplots(1, 2, figsize=(9.5, 4.5))\n",
    "dtc = DecisionTreeClassifier(max_depth=1, max_leaf_nodes=2, random_state=0)\n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "# Define colors and colormap for the plot\n",
    "colors = [\"#f44336\", \"#2986cc\"]\n",
    "\n",
    "# Define a list of color names for the colormap\n",
    "_cmap = ListedColormap(colors)\n",
    "for ax, X_set, y_set in zip(axes, [X_train, X_test], [y_train, y_test]):\n",
    "    DecisionBoundaryDisplay.from_estimator(dtc, X_set, cmap=_cmap,\n",
    "                                           ax=ax, response_method=\"predict\",\n",
    "                                           plot_method=\"pcolormesh\",\n",
    "                                           xlabel= f'${feature_names[0]}$',\n",
    "                                           ylabel= f'${feature_names[1]}$',\n",
    "                                           shading=\"auto\",\n",
    "                                           grid_resolution=300,\n",
    "                                           alpha=0.3)\n",
    "    # Scatter plot for data points\n",
    "    for num in np.unique(y_set):\n",
    "        ax.scatter(X_set[:, 0][y_set == num], X_set[:, 1][y_set == num], c= colors[num],\n",
    "                   s=40, edgecolors='k', marker='o', label=str(num), zorder=2)\n",
    "    ax.legend(fontsize=12)\n",
    "    ax.grid(False)\n",
    "\n",
    "axes[0].set_title('Train Set', fontsize=14, weight='bold')\n",
    "axes[1].set_title('Test Set', fontsize=14, weight='bold')\n",
    "\n",
    "# Display Accuracy Score on the plot\n",
    "accuracy_train = metrics.accuracy_score(dtc.predict(X_train), y_train)\n",
    "accuracy_test = metrics.accuracy_score(dtc.predict(X_test), y_test)\n",
    "# Print F1 values\n",
    "txt = f'Accuracy Score (Train) = {accuracy_train:.3f}, Accuracy Score (Test) = {accuracy_test:.3f}'\n",
    "print_bold('Accuracy')\n",
    "print(f'\\t{txt}')\n",
    "\n",
    "# Adjust layout and display the plots\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subplot for the tree plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(9.5, 3))\n",
    "\n",
    "# Plot the decision tree\n",
    "_ = tree.plot_tree(dtc, ax=ax,\n",
    "                   filled=True,\n",
    "                   node_ids=True,\n",
    "                   feature_names= feature_names,\n",
    "                   proportion=True,\n",
    "                   fontsize=12)\n",
    "\n",
    "# Ensure tight layout\n",
    "plt.tight_layout()\n",
    "\n",
    "print_bold('Tree Plot')\n",
    "print(tree.export_text(dtc, feature_names = feature_names, decimals = 3))\n",
    "\n",
    "# Assuming you have your dataset X and labels y\n",
    "# X is your feature matrix, and y is your target variable\n",
    "\n",
    "# Access the feature importances, which correspond to Gini impurity\n",
    "gini_importances = dtc.feature_importances_\n",
    "\n",
    "# You can also compute the overall Gini impurity of a node or a tree\n",
    "# For example, to calculate the Gini impurity of a specific node:\n",
    "for node in range(len(dtc.tree_.value)):\n",
    "    # print(dtc.tree_.value[node])\n",
    "    node_gini = 1 - np.sum(np.square(dtc.tree_.value[node] / np.sum(dtc.tree_.value[node])))\n",
    "    print(f'Gini for Node #{node} = {node_gini:.3f}')\n",
    "\n",
    "print('\\n')\n",
    "plot_cm(dtc, X_train, X_test, y_train, y_test, class_names = ['0', '1'])\n",
    "# del dtc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hck7cWjPSgRu"
   },
   "source": [
    "To simplify calculate these numbers using math, you need to understand how the Gini impurity is computed for each node in the decision tree. The Gini impurity is a measure of how likely a randomly chosen element from a set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the set. It is calculated as:\n",
    "\n",
    "$$\n",
    "Gini = 1 - \\sum_{i=1}^k p_i^2\n",
    "$$\n",
    "\n",
    "where $k$ is the number of classes and $p_i$ is the probability of choosing an element of class $i$. The Gini impurity ranges from 0 to 0.5, where 0 means the set is perfectly pure (all elements belong to the same class) and 0.5 means the set is completely impure (equal probability of choosing any class).\n",
    "\n",
    "The numbers in the brackets represent the number of samples of each class at each node. For example, [[80, 80]] means that there are 80 samples of class 0 and 80 samples of class 1 at node #0. To calculate the Gini impurity for node #0, you need to find the probabilities of choosing each class and plug them into the formula. The probabilities are:\n",
    "\n",
    "$$\n",
    "p_0 = \\frac{80}{80 + 80} = 0.5\n",
    "$$\n",
    "\n",
    "$$\n",
    "p_1 = \\frac{80}{80 + 80} = 0.5\n",
    "$$\n",
    "\n",
    "Then, the Gini impurity is:\n",
    "\n",
    "$$\n",
    "Gini_0 = 1 - (0.5^2 + 0.5^2) = 0.5\n",
    "$$\n",
    "\n",
    "Similarly, for node #1, the probabilities are:\n",
    "\n",
    "$$\n",
    "p_0 = \\frac{2}{2 + 78} = 0.025\n",
    "$$\n",
    "\n",
    "$$\n",
    "p_1 = \\frac{78}{2 + 78} = 0.975\n",
    "$$\n",
    "\n",
    "And the Gini impurity is:\n",
    "\n",
    "$$\n",
    "Gini_1 = 1 - (0.025^2 + 0.975^2) = 0.04875\n",
    "$$\n",
    "\n",
    "For node #2, the probabilities are:\n",
    "\n",
    "$$\n",
    "p_0 = \\frac{78}{78 + 2} = 0.975\n",
    "$$\n",
    "\n",
    "$$\n",
    "p_1 = \\frac{2}{78 + 2} = 0.025\n",
    "$$\n",
    "\n",
    "And the Gini impurity is:\n",
    "\n",
    "$$\n",
    "Gini_2 = 1 - (0.975^2 + 0.025^2) = 0.04875\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would be the output of the following (using the tree only)?\n",
    "\n",
    "```dtc.predict(np.array([[0, 1]]))```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc.predict(np.array([[0, 1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Example 2\n",
    "\n",
    "\n",
    "- **DecisionTreeClassifier** is a machine learning model that can be used for classification tasks, such as predicting the class label of a data point based on its features.\n",
    "\n",
    "- **max_depth** is a parameter that controls the maximum depth of the tree, which is the longest path from the root node to a leaf node. A deeper tree can capture more complex patterns in the data, but it may also overfit and perform poorly on new data. Setting max_depth to None means that the tree will grow until all the nodes are pure or some other stopping criteria are met.\n",
    "\n",
    "- **max_leaf_nodes** is a parameter that controls the maximum number of leaf nodes in the tree, which are the nodes that do not have any children. A smaller number of leaf nodes can reduce the complexity of the model and prevent overfitting, but it may also underfit and miss some important patterns in the data. Setting max_leaf_nodes to 4 means that the tree will have at most 4 leaf nodes, regardless of the depth.\n",
    "\n",
    "- **random_state** is a parameter that controls the randomness of the model, such as the choice of the feature and the threshold to split each node. Setting random_state to 0 means that the model will use a fixed seed for the random number generator, which ensures that the results are reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decision boundaries\n",
    "fig, axes = plt.subplots(1, 2, figsize=(9.5, 4.5))\n",
    "dtc = DecisionTreeClassifier(max_depth=None, max_leaf_nodes= 4, random_state=0)\n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "# Define colors and colormap for the plot\n",
    "colors = [\"#f44336\", \"#2986cc\"]\n",
    "\n",
    "# Define a list of color names for the colormap\n",
    "_cmap = ListedColormap(colors)\n",
    "for ax, X_set, y_set in zip(axes, [X_train, X_test], [y_train, y_test]):\n",
    "    DecisionBoundaryDisplay.from_estimator(dtc, X_set, cmap=_cmap,\n",
    "                                           ax=ax, response_method=\"predict\",\n",
    "                                           plot_method=\"pcolormesh\",\n",
    "                                           xlabel= f'${feature_names[0]}$',\n",
    "                                           ylabel= f'${feature_names[1]}$',\n",
    "                                           shading=\"auto\",\n",
    "                                           grid_resolution=300,\n",
    "                                           alpha=0.3)\n",
    "    # Scatter plot for data points\n",
    "    for num in np.unique(y_set):\n",
    "        ax.scatter(X_set[:, 0][y_set == num], X_set[:, 1][y_set == num], c= colors[num],\n",
    "                   s=40, edgecolors='k', marker='o', label=str(num), zorder=2)\n",
    "    ax.legend(fontsize=12)\n",
    "    ax.grid(False)\n",
    "\n",
    "axes[0].set_title('Train Set', fontsize=14, weight='bold')\n",
    "axes[1].set_title('Test Set', fontsize=14, weight='bold')\n",
    "\n",
    "# Display Accuracy Score on the plot\n",
    "accuracy_train = metrics.accuracy_score(dtc.predict(X_train), y_train)\n",
    "accuracy_test = metrics.accuracy_score(dtc.predict(X_test), y_test)\n",
    "# Print F1 values\n",
    "txt = f'Accuracy Score (Train) = {accuracy_train:.3f}, Accuracy Score (Test) = {accuracy_test:.3f}'\n",
    "print_bold('Accuracy')\n",
    "print(f'\\t{txt}')\n",
    "\n",
    "# Adjust layout and display the plots\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "0l3hdbwxSfPE",
    "outputId": "f0288b3e-3b78-488b-aa14-0260becb8f40"
   },
   "outputs": [],
   "source": [
    "# Create a subplot for the tree plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(9.5, 5))\n",
    "\n",
    "# Plot the decision tree\n",
    "_ = tree.plot_tree(dtc, ax=ax,\n",
    "                   filled=True,\n",
    "                   node_ids=True,\n",
    "                   feature_names= feature_names,\n",
    "                   proportion=True,\n",
    "                   fontsize=12)\n",
    "\n",
    "# Ensure tight layout\n",
    "plt.tight_layout()\n",
    "\n",
    "print_bold('Tree Plot')\n",
    "print(tree.export_text(dtc, feature_names = feature_names, decimals = 3))\n",
    "\n",
    "# Assuming you have your dataset X and labels y\n",
    "# X is your feature matrix, and y is your target variable\n",
    "\n",
    "# Access the feature importances, which correspond to Gini impurity\n",
    "gini_importances = dtc.feature_importances_\n",
    "\n",
    "# You can also compute the overall Gini impurity of a node or a tree\n",
    "# For example, to calculate the Gini impurity of a specific node:\n",
    "for node in range(len(dtc.tree_.value)):\n",
    "    # print(dtc.tree_.value[node])\n",
    "    node_gini = 1 - np.sum(np.square(dtc.tree_.value[node] / np.sum(dtc.tree_.value[node])))\n",
    "    print(f'Gini for Node #{node} = {node_gini:.3f}')\n",
    "\n",
    "print('\\n')\n",
    "plot_cm(dtc, X_train, X_test, y_train, y_test, class_names = ['0', '1'])\n",
    "del dtc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "def print_bold(txt, c=31):\n",
    "    \"\"\"\n",
    "    Function to print text in bold with specified color.\n",
    "\n",
    "    Parameters:\n",
    "    - txt (str): Text to be printed.\n",
    "    - c (int): Color code for the printed text.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    print(f\"\\033[1;{c}m\" + txt + \"\\033[0m\")\n",
    "\n",
    "def format_confusion_matrix(cm, title):\n",
    "    true_neg, false_pos, false_neg, true_pos = cm.ravel()\n",
    "    result = f\"\\033[1m{title} Set Confusion Matrix\\033[0m:\\n\"\n",
    "    result += f\"- {true_neg} instances were correctly predicted as class 0.\\n\"\n",
    "    result += f\"- {true_pos} instances were correctly predicted as class 1.\\n\"\n",
    "    result += f\"- {false_pos} instance was incorrectly predicted as class 1 when it was actually class 0.\\n\"\n",
    "    result += f\"- {false_neg} instances were incorrectly predicted as class 0 when they were actually class 1.\\n\"\n",
    "\n",
    "    return result\n",
    "\n",
    "def plot_cm(model, X_train, X_test, y_train, y_test, class_names, figsize=(7, 4)):\n",
    "    # Create a figure and axes for displaying confusion matrices side by side\n",
    "    fig, ax = plt.subplots(1, 2, figsize=figsize)\n",
    "\n",
    "    datasets = [(X_train, y_train, 'Train'), (X_test, y_test, 'Test')]\n",
    "\n",
    "    for i in range(2):\n",
    "        X, y, dataset_name = datasets[i]\n",
    "\n",
    "        # Compute confusion matrix for the dataset predictions\n",
    "        cm = confusion_matrix(y, model.predict(X))\n",
    "\n",
    "        result = format_confusion_matrix(cm, dataset_name)\n",
    "        print(result)\n",
    "\n",
    "        # Create a ConfusionMatrixDisplay and plot it on the respective axis\n",
    "        cm_display = ConfusionMatrixDisplay(cm, display_labels=class_names)\\\n",
    "                        .plot(ax=ax[i],\n",
    "                              im_kw=dict(cmap='Greens' if dataset_name == 'Train' else 'Blues'),\n",
    "                              text_kw={\"size\": 16}, colorbar=False)\n",
    "        ax[i].set_title(f'{dataset_name} Data')\n",
    "        ax[i].grid(False)\n",
    "\n",
    "    # Add a super title for the entire figure\n",
    "    fig.suptitle('Confusion Matrices', fontsize=16, weight = 'bold')\n",
    "\n",
    "    # Adjust the layout for better spacing\n",
    "    plt.tight_layout()\n",
    "    \n",
    "feature_names = [f'Feature_{i + 1}' for i in range(2)]\n",
    "\n",
    "# Generate synthetic data using make_blobs\n",
    "n_samples = 500\n",
    "n_features = 2\n",
    "centers = 2\n",
    "cluster_std = 3.0\n",
    "X, y = make_blobs(n_samples=n_samples, n_features=n_features,\n",
    "                  centers=centers, random_state=0, cluster_std=cluster_std)\n",
    "df = pd.DataFrame(data=X, columns=feature_names)\n",
    "df['y'] = y\n",
    "display(df)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                    random_state=0, stratify=y)\n",
    "\n",
    "#  Plot decision boundaries\n",
    "fig, axes = plt.subplots(1, 2, figsize=(9.5, 4.5))\n",
    "dtc = DecisionTreeClassifier(max_depth= None, max_leaf_nodes= None, random_state=0)\n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "# Define colors and colormap for the plot\n",
    "colors = [\"#f44336\", \"#2986cc\"]\n",
    "\n",
    "# Define a list of color names for the colormap\n",
    "_cmap = ListedColormap(colors)\n",
    "for ax, X_set, y_set in zip(axes, [X_train, X_test], [y_train, y_test]):\n",
    "    DecisionBoundaryDisplay.from_estimator(dtc, X_set, cmap=_cmap,\n",
    "                                           ax=ax, response_method=\"predict\",\n",
    "                                           plot_method=\"pcolormesh\",\n",
    "                                           xlabel= f'${feature_names[0]}$',\n",
    "                                           ylabel= f'${feature_names[1]}$',\n",
    "                                           shading=\"auto\",\n",
    "                                           grid_resolution=300,\n",
    "                                           alpha=0.3)\n",
    "    # Scatter plot for data points\n",
    "    for num in np.unique(y_set):\n",
    "        ax.scatter(X_set[:, 0][y_set == num], X_set[:, 1][y_set == num], c= colors[num],\n",
    "                   s=40, edgecolors='k', marker='o', label=str(num), zorder=2)\n",
    "    ax.legend(fontsize=12)\n",
    "    ax.grid(False)\n",
    "\n",
    "axes[0].set_title('Train Set', fontsize=14, weight='bold')\n",
    "axes[1].set_title('Test Set', fontsize=14, weight='bold')\n",
    "\n",
    "# Display Accuracy Score on the plot\n",
    "accuracy_train = metrics.accuracy_score(dtc.predict(X_train), y_train)\n",
    "accuracy_test = metrics.accuracy_score(dtc.predict(X_test), y_test)\n",
    "# Print F1 values\n",
    "txt = f'Accuracy Score (Train) = {accuracy_train:.3f}, Accuracy Score (Test) = {accuracy_test:.3f}'\n",
    "print_bold('Accuracy')\n",
    "print(f'\\t{txt}')\n",
    "\n",
    "# Adjust layout and display the plots\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Tree Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generating synthetic data\n",
    "np.random.seed(42)\n",
    "n = 200\n",
    "X = 2 * np.random.rand(n, 2)  # 2D input features\n",
    "y = 3 * X[:, 0] + 5 * X[:, 1] + np.random.randn(n)  # Linear combination with noise\n",
    "\n",
    "feature_names = [f'Feature_{i + 1}' for i in range(2)]\n",
    "df = pd.DataFrame(data=X, columns=feature_names)\n",
    "df['y'] = y\n",
    "display(df)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Decision Tree Regressor with specific settings\n",
    "dtr = DecisionTreeRegressor(criterion='squared_error',\n",
    "                                 splitter='best', max_leaf_nodes= 2)\n",
    "\n",
    "# Fit the Decision Tree Regressor to the data\n",
    "_ = dtr.fit(X, y)\n",
    "\n",
    "# Create a figure and axis for plotting the decision tree\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 2.75))\n",
    "\n",
    "# Visualize the decision tree\n",
    "_ = tree.plot_tree(dtr, ax=ax,\n",
    "                   impurity=True,         # Show impurity in nodes\n",
    "                   node_ids=True,         # Show node IDs\n",
    "                   filled=True,           # Fill nodes with colors\n",
    "                   feature_names=feature_names,  # Names of the features\n",
    "                   proportion=True)       # Display class proportion\n",
    "\n",
    "# Adjust the layout for better visualization\n",
    "plt.tight_layout()\n",
    "\n",
    "print(tree.export_text(dtr, feature_names = feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tree structure\n",
    "tree_structure = dtr.tree_\n",
    "\n",
    "# Extract feature indices and thresholds for each node\n",
    "feature_indices = tree_structure.feature\n",
    "thresholds = tree_structure.threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get squared_error and value for node#1, we have,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Node 1\n",
    "# =============================================================================\n",
    "\n",
    "# Define the thresholds\n",
    "Feat2_threshold = thresholds[0]\n",
    "\n",
    "# Node #01: Predict and Evaluate Subset\n",
    "print('Node #1: Predict and Evaluate Subset')\n",
    "# Import metrics from scikit-learn\n",
    "from sklearn import metrics\n",
    "\n",
    "# Predict the target variable for data points where 'Cylinders' is less than 5.5\n",
    "y_hat = dtr.predict(X[X[:, 1] <= Feat2_threshold])\n",
    "\n",
    "# Calculate the mean squared error (MSE) for the predicted values\n",
    "mse = metrics.mean_squared_error(y_hat, y[X[:, 1] <= Feat2_threshold])\n",
    "\n",
    "# Print the squared error (MSE) with three decimal places\n",
    "print(f'squared_error = {mse:.3f}')\n",
    "\n",
    "# Calculate the percentage of samples in this node compared to the entire dataset\n",
    "sample_percentage = (len(y_hat) / len(y) * 100)\n",
    "\n",
    "# Print the percentage of samples in this node\n",
    "print(f'samples = {sample_percentage:.1f}%')\n",
    "\n",
    "# Calculate and print the mean value of the predicted target variable in this node\n",
    "mean_value_node1 = y_hat.mean()\n",
    "print(f'value = {mean_value_node1:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Node 2\n",
    "# =============================================================================\n",
    "\n",
    "# Define the thresholds\n",
    "Feat2_threshold = thresholds[0]\n",
    "\n",
    "# Node #01: Predict and Evaluate Subset\n",
    "print('Node #1: Predict and Evaluate Subset')\n",
    "# Import metrics from scikit-learn\n",
    "from sklearn import metrics\n",
    "\n",
    "# Predict the target variable for data points where 'Cylinders' is less than 5.5\n",
    "y_hat = dtr.predict(X[X[:, 1] > Feat2_threshold])\n",
    "\n",
    "# Calculate the mean squared error (MSE) for the predicted values\n",
    "mse = metrics.mean_squared_error(y_hat, y[X[:, 1] > Feat2_threshold])\n",
    "\n",
    "# Print the squared error (MSE) with three decimal places\n",
    "print(f'squared_error = {mse:.3f}')\n",
    "\n",
    "# Calculate the percentage of samples in this node compared to the entire dataset\n",
    "sample_percentage = (len(y_hat) / len(y) * 100)\n",
    "\n",
    "# Print the percentage of samples in this node\n",
    "print(f'samples = {sample_percentage:.1f}%')\n",
    "\n",
    "# Calculate and print the mean value of the predicted target variable in this node\n",
    "mean_value_node2 = y_hat.mean()\n",
    "print(f'value = {mean_value_node1:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and axis for the plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "\n",
    "# Define tick positions and limits for the plot\n",
    "xlim = [-1, 3]\n",
    "ylim = [0, 20]\n",
    "\n",
    "# Create a scatter plot of data points\n",
    "_ = ax.scatter(X[:, 1], y, marker='o',\n",
    "               facecolor='DodgerBlue', edgecolor='Navy', alpha=0.3)\n",
    "\n",
    "# Set labels, ticks, and limits for the axes\n",
    "_ = ax.set(xlabel='Feature 2', ylabel='y', xlim=xlim, ylim=ylim)\n",
    "\n",
    "# Add a vertical dashed line at x = 4.5\n",
    "_ = ax.vlines(Feat2_threshold, ylim[0], ylim[1],\n",
    "              linestyles='dashed', linewidth=2, colors='Black')\n",
    "\n",
    "# Add horizontal dashed lines for mean values\n",
    "_ = ax.hlines(mean_value_node1, xlim[0], Feat2_threshold,\n",
    "              linestyles='dashed', linewidth=4, colors='Red',\n",
    "              label = f'y = {mean_value_node1:.3f}')\n",
    "_ = ax.hlines(mean_value_node2, Feat2_threshold, xlim[-1],\n",
    "              linestyles='dashed', linewidth=4, colors='Green',\n",
    "              label = f'y = {mean_value_node2:.3f}')\n",
    "\n",
    "# Annotations for the regions\n",
    "_ = ax.annotate(r'$\\mathbf{R_1}$', xy=(0, 8), fontsize=30)\n",
    "_ = ax.annotate(r'$\\mathbf{R_2}$', xy=(2.2, 12), fontsize=30)\n",
    "\n",
    "# Fill regions with different colors\n",
    "_ = ax.fill_between([xlim[0], Feat2_threshold], ylim[0], ylim[1],\n",
    "                    color='LimeGreen', alpha=0.1)\n",
    "_ = ax.fill_between([Feat2_threshold, xlim[-1]], ylim[0], ylim[1],\n",
    "                    color='purple', alpha=0.1)\n",
    "\n",
    "# Add a grid to the plot\n",
    "_ = ax.grid(True)\n",
    "_ = ax.legend(fontsize = 12)\n",
    "# Ensure a tight layout for the plot\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Tree Example 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results show the performance of a Decision Tree Regressor (DTR) model with different values of the max_depth parameter on a training and a test dataset. The performance metric used is the Mean Squared Error (MSE), which is the average of the squared differences between the predicted and the actual values. A lower MSE indicates a better fit of the model to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This is an modified version of an example by Sklearn:\n",
    "# https://scikit-learn.org/stable/auto_examples/tree/plot_tree_dtrression.html\n",
    "\n",
    "# Import the necessary modules and libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Create a random dataset\n",
    "rng = np.random.RandomState(1)\n",
    "X = np.sort(5 * rng.rand(160, 1), axis=0)\n",
    "y = np.cos(X).ravel()\n",
    "y[::5] += 3 * (0.5 - rng.rand(32))\n",
    "\n",
    "df = pd.DataFrame(data=X, columns=['x'])\n",
    "df['y'] = y\n",
    "display(df)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# Predict\n",
    "X_gen = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n",
    "\n",
    "# Create decision tree dtrressors with different max depths\n",
    "max_depths = [1, 2, 3, 4]\n",
    "colors = [\"DodgerBlue\", \"Violet\", \"OrangeRed\", \"Green\"]\n",
    "\n",
    "# Plot the results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(9.5, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, (depth, color) in enumerate(zip(max_depths, colors)):\n",
    "    ax = axes[i]\n",
    "    ax.scatter(X_train, y_train, s= 30, edgecolor=\"black\", c=\"darkorange\", label=\"data\")\n",
    "    \n",
    "    dtrr = DecisionTreeRegressor(max_depth=depth).fit(X_train, y_train)\n",
    "    y_pred = dtrr.predict(X_gen)\n",
    "    ax.plot(X_gen, y_pred, color=color, linewidth=2)\n",
    "    \n",
    "    _ = ax.set_title(f\"max_depth = {depth}\", weight = 'bold')\n",
    "    _ = ax.set(xlabel = \"x\", ylabel = \"y\")\n",
    "    # Display Accuracy Score on the plot\n",
    "    mse_train = metrics.mean_squared_error(dtrr.predict(X_train), y_train)\n",
    "    mse_test = metrics.mean_squared_error(dtrr.predict(X_test), y_test)\n",
    "    # Print F1 values\n",
    "    txt = f'MSE (Training) = {mse_train:.3f}, MSE (Test) = {mse_test:.3f}'\n",
    "    print_bold(f'DTR (max_depth = {depth})')\n",
    "    print(f'\\t{txt}')\n",
    "fig.suptitle('Training Data', weight = 'bold')\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
