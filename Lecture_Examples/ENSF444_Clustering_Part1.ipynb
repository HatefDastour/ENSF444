{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering"
      ],
      "metadata": {
        "id": "5a2KYZqOsihn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Clustering?\n",
        "\n",
        "**Clustering** is a technique of **unsupervised learning** that **groups data points into clusters** based on their similarities. Unlike supervised learning, which uses labeled data, clustering works with **unlabeled data**.\n",
        "\n",
        "Some key points about clustering are:\n",
        "\n",
        "1. **Definition**: Clustering finds patterns in data and assigns them to groups based on their features. For example, we can cluster music by genre or decade, or flowers by petal size and shape.\n",
        "1. **Purpose**: Clustering reveals the hidden structure of data and helps us understand and analyze it better. We can also use clustering to make predictions about new data points based on their cluster membership.\n",
        "1. **Example**: One common example of clustering is the analysis of the Iris dataset, which contains measurements of four attributes for 150 flowers from three species. Clustering can help us classify the flowers into three groups based on their sepal length and width.\n",
        "1. **Methods**: There are many clustering algorithms, each with a different way of finding and forming clusters. Some of the popular ones are:\n",
        "    - **K-means**: This algorithm partitions data into K clusters, where each cluster is represented by its center or centroid. The algorithm iteratively assigns each data point to the closest cluster and updates the centroids until convergence.\n",
        "    - **Hierarchical clustering**: This algorithm creates a hierarchy of clusters, where each cluster is either a single data point or a combination of smaller clusters. The algorithm can be either agglomerative, which starts with individual data points and merges them into larger clusters, or divisive, which starts with the whole data and splits it into smaller clusters.\n",
        "    - **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**: This algorithm identifies clusters as dense regions of data points that are separated by low-density regions. The algorithm also distinguishes noise or outliers from the clusters.\n",
        "4.  **Applications**:\n",
        "\n",
        "    -   **Market segmentation**: Companies use clustering to group\n",
        "        customers with similar behaviors or preferences.\n",
        "\n",
        "        -   Example: <https://doi.org/10.1016/j.eswa.2005.11.028>\n",
        "\n",
        "    -   **Social network analysis**: Clustering helps identify\n",
        "        communities within networks.\n",
        "\n",
        "        -   Example: <https://doi.org/10.1007/978-3-030-34770-3_2>\n",
        "\n",
        "    -   **Search result grouping**: Search engines organize results into\n",
        "        relevant clusters.\n",
        "\n",
        "        -   Example: <https://doi.org/10.1007/978-81-322-2752-6_34>\n",
        "\n",
        "    -   **Medical imaging**: Clustering aids in segmenting medical\n",
        "        images.\n",
        "\n",
        "        -   Example: <https://doi.org/10.1016/j.chb.2016.03.056>\n",
        "\n",
        "    -   **Anomaly detection**: Unusual patterns can be detected by\n",
        "        identifying outliers.\n",
        "\n",
        "        -   Example: <https://doi.org/10.1007/978-981-13-1056-0_48>\n"
      ],
      "metadata": {
        "id": "iHWELDlB1Xrr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='Blue'><b>Example:</b></font> The main objective of this example is to demonstrate how to use KMeans clustering to group Iris flowers into three clusters based on their sepal length and width, and how to visualize the original and clustered data using scatter plots."
      ],
      "metadata": {
        "id": "kMxXueTzcd0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Iris Data Example\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "data = iris.data\n",
        "\n",
        "# Use a custom style from a specified URL\n",
        "plt.style.use('https://raw.githubusercontent.com/HatefDastour/ENSF444/main/Files/mystyle.mplstyle')\n",
        "\n",
        "# Define a custom color map\n",
        "_cmap = ListedColormap([\"#f44336\", \"#295F27\", \"#2986cc\"])\n",
        "\n",
        "# Apply KMeans clustering\n",
        "kmeans = KMeans(n_clusters=3, n_init=10, random_state=0)\n",
        "kmeans.fit(data)\n",
        "\n",
        "# Create subplots\n",
        "fig, axs = plt.subplots(1, 2, figsize=(11, 5))\n",
        "\n",
        "# Scatter plot for the original data\n",
        "_ =  axs[0].scatter(data[:, 0], data[:, 1], facecolor='gray',\n",
        "                    edgecolor='k', linewidth=1, alpha=0.7, s=40)\n",
        "\n",
        "# Scatter plot for clustered data using KMeans\n",
        "_ = axs[1].scatter(data[:, 0], data[:, 1], c=kmeans.labels_, cmap=_cmap,\n",
        "                   edgecolor='k', linewidth=0.5, alpha=0.7, s=40)\n",
        "\n",
        "# Plot cluster centers as stars\n",
        "_ = axs[1].scatter(kmeans.cluster_centers_[:, 0],\n",
        "                   kmeans.cluster_centers_[:, 1],\n",
        "                   c=np.arange(3),\n",
        "                   marker='*', cmap=_cmap, edgecolor='k',\n",
        "                   linewidth=1, alpha=1, s=200)\n",
        "\n",
        "# Set titles for subplots\n",
        "axs[0].set_title('Original', fontsize=14)\n",
        "axs[1].set_title('Clusters using KMeans', fontsize=14)\n",
        "\n",
        "# Set labels for axes\n",
        "for ax in axs:\n",
        "    _ = ax.set(xlabel='Sepal Length (cm)', ylabel='Sepal Width (cm)')\n",
        "\n",
        "# Adjust layout for better presentation\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "hsRDudl4SreV",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering Methods\n",
        "\n",
        "- **K-means:** A partitioning method that divides data into non-overlapping subsets (clusters) where each data point belongs to only one group.\n",
        "\n",
        "- **Hierarchical Clustering:** Builds a hierarchy of clusters either through a bottom-up approach (agglomerative) or top-down approach (divisive).\n",
        "\n",
        "- **DBSCAN:** A density-based clustering method that groups points that are closely packed together, marking as outliers points that lie alone in low-density regions.\n",
        "\n",
        "- **Gaussian Mixture Models (GMM):** A probabilistic model that assumes all the data points are generated from a mixture of several Gaussian distributions with unknown parameters.\n",
        "\n",
        "- **Affinity Propagation:** Creates clusters by sending messages between pairs of samples until convergence.\n",
        "\n",
        "- **Mean Shift:** A sliding-window-based algorithm that assigns the data points to the clusters iteratively by shifting points towards the mode (the highest density of data points).\n",
        "\n",
        "- **Spectral Clustering:** Uses the eigenvalues of a similarity matrix to reduce dimensionality before clustering in fewer dimensions.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/clustering.html#clustering"
      ],
      "metadata": {
        "id": "fgwWwjrYsub3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering Terminology\n",
        "\n",
        "- **Cluster Label:** The identifier for the cluster to which a data point is assigned.\n",
        "\n",
        "- **Cluster Prediction:** Applying the `predict()` method to assign cluster labels to new data samples.\n",
        "\n",
        "- **Label Retrieval:** Accessing the `.labels_` attribute to obtain the cluster labels of the trained model."
      ],
      "metadata": {
        "id": "EAjySoU3tFSi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Distance Metrics in Clustering\n",
        "\n",
        "### Core Concept\n",
        "Distance metrics are fundamental in clustering algorithms to measure the proximity or similarity between data points.\n",
        "\n",
        "### Function\n",
        "These metrics assess the \"closeness\" of data points, which is pivotal for the formation of clusters.\n",
        "\n",
        "### Usage\n",
        "The selection of a distance metric significantly affects the resulting cluster shapes, sizes, and the overall effectiveness of the clustering algorithm.\n",
        "\n",
        "### Variety\n",
        "A variety of distance metrics are employed, each suitable for different types of data and clustering requirements:\n",
        "\n",
        "- **Euclidean Distance (L2):** Calculates the straight-line distance between two points in Euclidean space.\n",
        "- **Manhattan Distance (L1):** Measures the sum of the absolute differences of their coordinates.\n",
        "- **Minkowski Distance:** Generalizes Euclidean and Manhattan distances by introducing a parameter $ p $ that allows different metrics to be derived.\n",
        "- **Hamming Distance:** Counts the number of positions at which the corresponding symbols are different, typically used for categorical data.\n",
        "- **Cosine Similarity:** Evaluates the cosine of the angle between two vectors, effectively measuring their orientation, not magnitude.\n",
        "\n",
        "Each metric is chosen based on the data characteristics and the clustering algorithm's specific needs.\n",
        "\n",
        "### Distance Functions\n",
        "Here's a table summarizing the distance functions for each metric:\n",
        "\n",
        "<center>\n",
        "\n",
        "| Distance metric | Arguments |                      Distance Function                     |\n",
        "|:---------------:|:---------:|:----------------------------------------------------------:|\n",
        "|  Euclidean (L2) |    N/A    |              $\\sqrt{\\sum_{}^{}{(x - y)}^{2}}$              |\n",
        "|  Manhattan (L1) |    N/A    |                    $\\sum_{}^{}\\|x - y\\|$                   |\n",
        "|    Chebyshev    |    N/A    |                      $\\max_{i}(|x_{i} - y_{i}|)$                     |\n",
        "|    Minkowski    |    p, w   | $\\left( \\sum_{}^{}{w*\\|x - y\\|^{p}} \\right)^{\\frac{1}{p}}$ |\n",
        "|      Cosine     |    N/A    |                $\\frac{x.y}{\\sqrt{\\sum x^{2}}*\\sqrt{\\sum y^{2}}}$               |\n",
        "\n",
        "</center>\n",
        "\n",
        "For more detailed information and examples, you can refer to the [scikit-learn documentation on distance metrics](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.DistanceMetric.html)."
      ],
      "metadata": {
        "id": "Zv2Rnjw1uIsS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-means Clustering\n",
        "\n",
        "**What is K-means?**\n",
        "K-means is a method to divide a set of data points into **'k'** distinct clusters, where 'k' is a predefined number. The goal is to partition the data such that points in the same cluster are similar to each other, while points in different clusters are not.\n",
        "\n",
        "**How does it work?**\n",
        "1. **Initialization**: Randomly select **'k'** points from the data as the initial cluster centers (centroids).\n",
        "2. **Assignment**: Assign each data point to the nearest centroid, forming **'k'** clusters.\n",
        "3. **Update**: Calculate the new centroid of each cluster by taking the mean of all points assigned to that cluster.\n",
        "4. **Repeat**: Repeat the assignment and update steps until the centroids no longer change significantly.\n",
        "\n",
        "**The Math Behind K-means**\n",
        "\n",
        "The algorithm minimizes the within-cluster variance, which is the sum of squared distances between each point and its corresponding centroid. This is known as the **inertia** or **within-cluster sum of squares (WCSS)** and is given by the formula:\n",
        "\n",
        "$$ \\text{Inertia  or WCSS} = \\sum_{i=1}^{k} \\sum_{x \\in C_i} \\|x - \\mu_i\\|^2 $$\n",
        "\n",
        "where:\n",
        "- $k$ is the number of clusters,\n",
        "- $C_i$ is the set of points in cluster $i$,\n",
        "- $x$ is a data point in cluster $C_i$,\n",
        "- $\\mu_i$ is the centroid of cluster $C_i$,\n",
        "- $\\|x - \\mu_i\\|$ is the Euclidean distance between point $x$ and centroid $\\mu_i$.\n",
        "\n",
        "The algorithm iterates through the assignment and update steps to minimize the WCSS, which effectively makes the clusters as compact and separated as possible."
      ],
      "metadata": {
        "id": "3yCubCsTfJGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Brief explanation of each hyperparameter for KMeans in scikit-learn:\n",
        "\n",
        "- **n_clusters**: The number of clusters and centroids to form. It's the 'k' in k-means.\n",
        "- **init**: Method for initialization of centroids. Options include 'k-means++' for faster convergence, 'random' for random initialization, or a custom method.\n",
        "- **n_init**: Number of times the algorithm will run with different centroid seeds. The best output in terms of inertia is chosen.\n",
        "- **max_iter**: Maximum number of iterations for a single run of the algorithm.\n",
        "- **tol**: Tolerance for declaring convergence based on the Frobenius norm of the difference in cluster centers.\n",
        "- **verbose**: Verbosity mode for logging the process of the algorithm.\n",
        "- **random_state**: Seed for random number generation, ensuring reproducibility.\n",
        "- **copy_x**: Whether to copy the input data; affects numerical stability.\n",
        "- **algorithm**: Choice of algorithm to use; 'lloyd' for the standard approach, 'elkan' for a more efficient but memory-intensive method.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
        "\n",
        "<font color='Blue'><b>Example:</b></font>"
      ],
      "metadata": {
        "id": "zpomk_5kgxDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import make_blobs\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate synthetic two-dimensional data\n",
        "X, y = make_blobs(random_state = 2)\n",
        "\n",
        "df = pd.DataFrame(X, columns=['Feature 1', 'Feature 2'])\n",
        "display(df)\n",
        "\n",
        "# Create subplots for original data and clustered data\n",
        "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
        "\n",
        "ax.scatter(df['Feature 1'], df['Feature 2'], c='gray', edgecolor='k',\n",
        "               linewidth=2, alpha=0.7, s=40)\n",
        "ax.set(xlabel='Feature 1', ylabel='Feature 2')\n",
        "\n",
        "# Adjust layout for better presentation\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "cS_fxobOoDAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "k = 3\n",
        "# Build the KMeans clustering model\n",
        "kmeans = KMeans(n_clusters = k, random_state=0, n_init='auto')\n",
        "kmeans.fit(X)\n",
        "\n",
        "# Output coordinates of cluster centers\n",
        "print(f\"Cluster Centers:\\n{kmeans.cluster_centers_.round(3)}\")\n",
        "\n",
        "# Output cluster memberships\n",
        "print(f\"\\nCluster Memberships:\\n{kmeans.labels_}\")\n",
        "\n",
        "# Output sum of squared distances of samples to their closest cluster center\n",
        "print(f\"\\nInertia (or WCSS):\\n{kmeans.inertia_:.4f}\")\n",
        "\n",
        "# Output number of iterations run\n",
        "print(f\"\\nNumber of Iterations Run:\\n{kmeans.n_iter_}\")\n",
        "\n",
        "# Define a custom color map for the clusters\n",
        "colors = [\"#f44336\", \"#4e9130\", \"#2986cc\"]\n",
        "cmap = ListedColormap(colors)\n",
        "\n",
        "# Create subplots for original data and clustered data\n",
        "fig, axs = plt.subplots(1, 2, figsize=(11, 5))\n",
        "\n",
        "# Scatter plot for the original data\n",
        "axs[0].scatter(df['Feature 1'], df['Feature 2'], c='gray', edgecolor='k',\n",
        "               linewidth=1, alpha=0.7, s=40)\n",
        "axs[0].set_title('Original Data', fontsize=14)\n",
        "\n",
        "# Scatter plot for the clustered data\n",
        "scatter = axs[1].scatter(df['Feature 1'], df['Feature 2'], c=kmeans.labels_, cmap=cmap,\n",
        "               edgecolor='k', linewidth=0.5, alpha=0.7, s=40)\n",
        "axs[1].set_title(f'Clusters using KMeans (k = {k})', fontsize=14)\n",
        "\n",
        "# Plot the cluster centers\n",
        "centers = axs[1].scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
        "               c=range(k), marker='*', cmap=cmap, edgecolor='k',\n",
        "               linewidth=1, alpha=1, s=200)\n",
        "\n",
        "_ = axs[1].text(0.02, 0.02, f'Inertia = {kmeans.inertia_:.4f}',\n",
        "               transform=axs[1].transAxes, fontsize=12, weight='bold',\n",
        "               bbox=dict(facecolor='Whitesmoke', alpha=0.7))\n",
        "\n",
        "# Decision boundary display\n",
        "DecisionBoundaryDisplay.from_estimator(kmeans, X, cmap=cmap,\n",
        "                                        ax=axs[1], response_method=\"predict\",\n",
        "                                        plot_method=\"pcolormesh\",\n",
        "                                        xlabel='Feature 1',\n",
        "                                        ylabel='Feature 2',\n",
        "                                        shading=\"auto\",\n",
        "                                        grid_resolution=300,\n",
        "                                        alpha=0.2)\n",
        "\n",
        "# Set labels for axes\n",
        "for ax in axs:\n",
        "    ax.set(xlabel='Feature 1', ylabel='Feature 2')\n",
        "\n",
        "# Adjust layout for better presentation\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "BmWfgmq_1ydO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Calculate cluster counts using Counter\n",
        "counts = Counter(kmeans.labels_)\n",
        "\n",
        "# Plotting an aesthetically pleasing pie chart for cluster size distribution\n",
        "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
        "\n",
        "# Creating a pie chart with specified colors, shadows, and text properties\n",
        "ax.pie(counts.values(), labels= [f'Cluster {i}' for i in counts.keys()],\n",
        "      #  autopct='%1.1f%%',\n",
        "       colors=colors, shadow=True,\n",
        "       wedgeprops= dict(edgecolor = 'k', linewidth = 1),\n",
        "       textprops= dict(fontsize = 12, color = 'k', weight = 'bold'),\n",
        "       autopct=lambda p: f'{p:.1f}%\\n({p*sum(counts.values())/100:.0f} Instances)',\n",
        "       )\n",
        "\n",
        "# Adding a title to the plot with bold font weight\n",
        "ax.set_title('Distribution of Cluster Sizes', weight='bold')\n",
        "\n",
        "# Ensuring a tight layout for better visualization\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "IKfoZrAb3hRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Copying the dataframe and adding a new column for cluster labels\n",
        "df_labels = df.assign(Clusters=kmeans.labels_.astype(str))\n",
        "\n",
        "# Creating subplots for boxplots of Feature 1 and Feature 2 with cluster labels\n",
        "fig, ax = plt.subplots(2, 1, figsize=(10, 5))\n",
        "\n",
        "# Plotting boxplots for Feature 1 and Feature 2\n",
        "for i, feature in enumerate([\"Feature 1\", \"Feature 2\"]):\n",
        "    sns.boxplot(ax=ax[i], data=df_labels, x=feature, y=\"Clusters\",\n",
        "                medianprops={\"color\": \"k\", \"linewidth\": 2},\n",
        "                color=\"white\", linecolor=\"#5b5b5b\", linewidth=1.5)\n",
        "\n",
        "    # Adding vertical dashed lines at cluster centers for both features\n",
        "    for j in range(3):\n",
        "        ax[i].axvline(x=kmeans.cluster_centers_[j, i], color=colors[j], lw=3, ls='--', label=f'Cluster {j}')\n",
        "\n",
        "    # Removing the legends from the subplots\n",
        "    ax[i].legend().remove()\n",
        "\n",
        "# Creating a single legend for both subplots and placing it on the right side of the figure\n",
        "# Using the handles and labels from the first subplot\n",
        "handles, labels = ax[0].get_legend_handles_labels()\n",
        "fig.legend(handles, labels, loc='center right', title='Clusters Centers', bbox_to_anchor=(1.13, 0.55))\n",
        "\n",
        "# Ensuring a tight layout for better visualization\n",
        "plt.tight_layout()\n"
      ],
      "metadata": {
        "id": "-Lwdn51s50Rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from collections import Counter\n",
        "# import matplotlib.pyplot as plt\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "\n",
        "# # Creating subplots for cluster centers per feature\n",
        "# fig, axes = plt.subplots(3, 1, figsize=(6, 4), sharex=True)\n",
        "# centers = pd.DataFrame(kmeans.cluster_centers_, columns=['Feature 1', 'Feature 2'])\n",
        "\n",
        "# for i, ax in enumerate(axes):\n",
        "#     center = centers.loc[i, :]\n",
        "\n",
        "#     # Adding a vertical line at x=0 for reference\n",
        "#     ax.axvline(color='k', lw=3)\n",
        "\n",
        "#     # Plotting horizontal bar chart with specified colors, hatch, and edge color\n",
        "#     center.plot.barh(ax=ax, color=['#38761d' if l > 0 else '#cc0000' for l in center], hatch='//', ec='white')\n",
        "\n",
        "#     # Setting ylabel with cluster number, color, and bold font weight\n",
        "#     ax.set_ylabel(f'Cluster {i}', color=colors[i], weight='bold')\n",
        "\n",
        "#     # Limiting x-axis for better visualization\n",
        "#     ax.set_xlim(-10, 2)\n",
        "\n",
        "#     # Adding grid lines along the y-axis\n",
        "#     ax.grid(axis='y')\n",
        "\n",
        "#     # Setting title for the first subplot\n",
        "#     if i == 0:\n",
        "#         ax.set_title('Cluster Centers per Feature on Original Scale')\n",
        "\n",
        "# # Ensuring a tight layout for better visualization\n",
        "# plt.tight_layout()"
      ],
      "metadata": {
        "id": "6odgBd-_08OR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation Metrics for Clustering\n",
        "\n",
        "- **Silhouette Coefficient:** Measures how similar an object is to its own cluster compared to other clusters. A higher silhouette value indicates better-defined clusters.\n",
        "\n",
        "- **Calinski-Harabasz Index:** Also known as the Variance Ratio Criterion, this metric evaluates the cluster validity based on the ratio between the within-cluster dispersion and the between-cluster dispersion.\n",
        "\n",
        "- **Davies-Bouldin Index:** This index signifies the average 'similarity' between clusters, where lower values indicate better clustering.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation\n"
      ],
      "metadata": {
        "id": "L3mKlMZruvkl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Silhouette Coefficient\n",
        "\n",
        "- **a:** Mean distance between a sample and all other points in the same cluster.\n",
        "- **b:** Mean distance between a sample and all other points in the next nearest cluster.\n",
        "- **Formula:** The coefficient for a sample is computed as $$\\frac{b - a}{\\max(a, b)}$$.\n",
        "- **Score:** The silhouette score of a dataset is the mean of the silhouette coefficient for each sample, calculated using `sklearn.metrics.silhouette_score(X, labels)`.\n",
        "\n",
        "This metric is useful for assessing the appropriateness of the clustering by measuring how well each object lies within its cluster.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score"
      ],
      "metadata": {
        "id": "qiQL5-1CuvnF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='Blue'><b>Example:</b></font> Suppose we have the following dataset with two clusters\n",
        "\n",
        "-   $X  = \\lbrack(1,\\ 2),\\ (2,\\ 3),\\ (3,\\ 4),\\ (7,\\ 8),\\ (8,\\ 9),\\ (9,\\ 10)\\rbrack$\n",
        "\n",
        "-   $labels\\  = \\ \\lbrack 0,\\ 0,\\ 0,\\ 1,\\ 1,\\ 1\\rbrack$\n"
      ],
      "metadata": {
        "id": "0r9-ah6zuV3n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- For the first point (1, 2):\n",
        "\n",
        "  - The distance between (1, 2) and (2, 3) is $$\\sqrt{(1 - 2)^2 + (2 - 3)^2} = \\sqrt{(-1)^2 + (-1)^2} = \\sqrt{2} = 1.41.$$\n",
        "\n",
        "    The distance between (1, 2) and (3, 4) is $$\\sqrt{(1 - 3)^2 + (2 - 4)^2} = \\sqrt{(-2)^2 + (-2)^2} = \\sqrt{8} = 2.83.$$\n",
        "\n",
        "    The mean distance to the points in the same cluster is **a = (1.41 + 2.83) / 2 = 2.12**.\n",
        "  \n",
        "  - The distance between (1, 2) and (7, 8) is $$\\sqrt{(1 - 7)^2 + (2 - 8)^2} = \\sqrt{(-6)^2 + (-6)^2} = \\sqrt{72} = 8.49.$$\n",
        "\n",
        "    The distance between (1, 2) and (8, 9) is $$\\sqrt{(1 - 8)^2 + (2 - 9)^2} = \\sqrt{(-7)^2 + (-7)^2} = \\sqrt{98} = 9.90.$$\n",
        "\n",
        "    The distance between (1, 2) and (9, 10) is $$\\sqrt{(1 - 9)^2 + (2 - 10)^2} = \\sqrt{(-8)^2 + (-8)^2} = \\sqrt{128} = 11.31.$$\n",
        "\n",
        "    The mean distance to the points in the next cluster is **b = (8.49 + 9.90 + 11.31) / 3 = 9.90**.\n",
        "\n",
        "  - The silhouette coefficient is\n",
        "    $$\\frac{b - a}{\\max(a, b)} = (9.90 - 2.12) / 9.90 = 0.79.$$\n",
        "\n",
        "- For the second point (2, 3):\n",
        "\n",
        "  - The distance between (2, 3) and (1, 2) is 1.41 as before.\n",
        "\n",
        "    The distance between (2, 3) and (3, 4) is also 1.41, since they are one unit apart.\n",
        "\n",
        "    The mean distance to the points in the same cluster is **a = (1.41 + 1.41) / 2 = 1.41**.\n",
        "\n",
        "  - The distance between (2, 3) and (7, 8) is $$\\sqrt{(2 - 7)^2 + (3 - 8)^2} = \\sqrt{(-5)^2 + (-5)^2} = \\sqrt{50} = 7.07.$$\n",
        "\n",
        "    The distance between (2, 3) and (8, 9) is $$\\sqrt{(2 - 8)^2 + (3 - 9)^2} = \\sqrt{(-6)^2 + (-6)^2} = \\sqrt{72} = 8.49.$$\n",
        "\n",
        "    The distance between (2, 3) and (9, 10) is $$\\sqrt{(2 - 9)^2 + (3 - 10)^2} = \\sqrt{(-7)^2 + (-7)^2} = \\sqrt{98} = 9.90.$$\n",
        "\n",
        "    The mean distance to the points in the next cluster is **b = (7.07 + 8.49 + 9.90) / 3 = 8.49**.\n",
        "\n",
        "  - The silhouette coefficient is\n",
        "    $$\\frac{b - a}{\\max(a, b)} = (8.49 - 1.41) / 8.49 = 0.83.$$\n",
        "\n",
        "- For the third point (3, 4):\n",
        "\n",
        "  - The distance between (3, 4) and (1, 2) is $$\\sqrt{(3 - 1)^2 + (4 - 2)^2} = \\sqrt{(2)^2 + (2)^2} = \\sqrt{8} = 2.83.$$\n",
        "\n",
        "    The distance between (3, 4) and (2, 3) is 1.41 as before.\n",
        "\n",
        "    The mean distance to the points in the same cluster is **a = (2.83 + 1.41) / 2 = 2.12**.\n",
        "\n",
        "  - The distance between (3, 4) and (7, 8) is $$\\sqrt{(3 - 7)^2 + (4 - 8)^2} = \\sqrt{(-4)^2 + (-4)^2} = \\sqrt{32} = 5.66.$$\n",
        "\n",
        "    The distance between (3, 4) and (8, 9) is $$\\sqrt{(3 - 8)^2 + (4 - 9)^2} = \\sqrt{(-5)^2 + (-5)^2} = \\sqrt{50} = 7.07.$$\n",
        "\n",
        "    The distance between (3, 4) and (9, 10) is $$\\sqrt{(3 - 9)^2 + (4 - 10)^2} = \\sqrt{(-6)^2 + (-6)^2} = \\sqrt{72} = 8.49.$$\n",
        "\n",
        "    The mean distance to the points in the next cluster is **b = (5.66 + 7.07 + 8.49) / 3 = 7.07**.\n",
        "\n",
        "  - The silhouette coefficient is\n",
        "    $$\\frac{b - a}{\\max(a, b)} = (7.07 - 2.12) / 7.07 = 0.70.$$\n",
        "\n",
        "- For the fourth point (7, 8):\n",
        "\n",
        "  - The distance between (7, 8) and (8, 9) is $$\\sqrt{(7 - 8)^2 + (8 - 9)^2} = \\sqrt{(-1)^2 + (-1)^2} = \\sqrt{2} = 1.41.$$\n",
        "\n",
        "    The distance between (7, 8) and (9, 10) is $$\\sqrt{(7 - 9)^2 + (8 - 10)^2} = \\sqrt{(-2)^2 + (-2)^2} = \\sqrt{8} = 2.83.$$\n",
        "\n",
        "    The mean distance to the points in the same cluster is **a = (1.41 + 2.83) / 2 = 2.12**.\n",
        "  \n",
        "  - The distance between (7, 8) and (1, 2) is $$\\sqrt{(7 - 1)^2 + (8 - 2)^2} = \\sqrt{(6)^2 + (6)^2} = \\sqrt{72} = 8.49.$$\n",
        "\n",
        "    The distance between (7, 8) and (2, 3) is $$\\sqrt{(7 - 2)^2 + (8 - 3)^2} = \\sqrt{(5)^2 + (5)^2} = \\sqrt{50} = 7.07.$$\n",
        "\n",
        "    The distance between (7, 8) and (3, 4) is $$\\sqrt{(7 - 3)^2 + (8 - 4)^2} = \\sqrt{(4)^2 + (4)^2} = \\sqrt{32} = 5.66.$$\n",
        "\n",
        "    The mean distance to the points in the next cluster is **b = (8.49 + 7.07 + 5.66) / 3 = 7.07**.\n",
        "\n",
        "  - The silhouette coefficient is\n",
        "    $$\\frac{b - a}{\\max(a, b)} = (7.07 - 2.12) / 7.07 = 0.70.$$\n",
        "\n",
        "- For the fifth point (8, 9):\n",
        "\n",
        "  - The distance between (8, 9) and (7, 8) is 1.41 as before.\n",
        "\n",
        "    The distance between (8, 9) and (9, 10) is also 1.41, since they are one unit apart.\n",
        "\n",
        "    The mean distance to the points in the same cluster is **a = (1.41 + 1.41) / 2 = 1.41**.\n",
        "\n",
        "  - The distance between (8, 9) and (1, 2) is $$\\sqrt{(8 - 1)^2 + (9 - 2)^2} = \\sqrt{(7)^2 + (7)^2} = \\sqrt{98} = 9.90.$$\n",
        "\n",
        "    The distance between (8, 9) and (2, 3) is $$\\sqrt{(8 - 2)^2 + (9 - 3)^2} = \\sqrt{(6)^2 + (6)^2} = \\sqrt{72} = 8.49.$$\n",
        "\n",
        "    The distance between (8, 9) and (3, 4) is $$\\sqrt{(8 - 3)^2 + (9 - 4)^2} = \\sqrt{(5)^2 + (5)^2} = \\sqrt{50} = 7.07.$$\n",
        "\n",
        "    The mean distance to the points in the next cluster is **b = (9.90 + 8.49 + 7.07) / 3 = 8.49**.\n",
        "\n",
        "  - The silhouette coefficient is\n",
        "    $$\\frac{b - a}{\\max(a, b)} = (8.49 - 1.41) / 8.49 = 0.83.$$\n",
        "\n",
        "- For the sixth point (9, 10):\n",
        "\n",
        "  - The distance between (9, 10) and (7, 8) is 2.83 as before.\n",
        "\n",
        "    The distance between (9, 10) and (8, 9) is 1.41 as before.\n",
        "\n",
        "    The mean distance to the points in the same cluster is **a = (2.83 + 1.41) / 2 = 2.12**.\n",
        "\n",
        "  - The distance between (9, 10) and (1, 2) is $$\\sqrt{(9 - 1)^2 + (10 - 2)^2} = \\sqrt{(8)^2 + (8)^2} = \\sqrt{128} = 11.31.$$\n",
        "\n",
        "    The distance between (9, 10) and (2, 3) is $$\\sqrt{(9 - 2)^2 + (10 - 3)^2} = \\sqrt{(7)^2 + (7)^2} = \\sqrt{98} = 9.90.$$\n",
        "\n",
        "    The distance between (9, 10) and (3, 4) is $$\\sqrt{(9 - 3)^2 + (10 - 4)^2} = \\sqrt{(6)^2 + (6)^2} = \\sqrt{72} = 8.49.$$\n",
        "\n",
        "    The mean distance to the points in the next cluster is **b = (11.31 + 9.90 + 8.49) / 3 = 9.90**.\n",
        "\n",
        "  - The silhouette coefficient is\n",
        "    $$\\frac{b - a}{\\max(a, b)} = (9.90 - 2.12) / 9.90 = 0.79.$$\n",
        "\n",
        "The coefficients for each point are (0.79, 0.83, 0.79, 0.70, 0.83, 0.79). The silhouette score as the mean of the coefficients is 0.79. Note that there are some rounding errors here and the final number is different from the calculations through Python code."
      ],
      "metadata": {
        "id": "TbWSuPh0uvj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppose we have the following dataset with two clusters\n",
        "X_exp = [[1, 2], [2, 3], [3, 4], [7, 8], [8, 9], [9, 10]]\n",
        "labels = [0, 0, 0, 1, 1, 1]\n",
        "\n",
        "# Define a function to calculate the Euclidean distance between two points\n",
        "def distance(p1, p2):\n",
        "    return ((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2) ** 0.5\n",
        "\n",
        "# Define a function to calculate the mean distance between a point and a list of points\n",
        "def mean_distance(p, points):\n",
        "    total = 0\n",
        "    for q in points:\n",
        "        total += distance(p, q)\n",
        "    return total / len(points)\n",
        "\n",
        "# Define a function to calculate the silhouette coefficient for a point\n",
        "def silhouette_coefficient(p, label, X, labels):\n",
        "    # Find the points in the same cluster as p\n",
        "    same_cluster = [q for q, l in zip(X, labels) if l == label and q != p]\n",
        "    # Find the points in the next nearest cluster as p\n",
        "    next_cluster = [q for q, l in zip(X, labels) if l != label]\n",
        "    # Calculate the mean distance to the same cluster (a)\n",
        "    a = mean_distance(p, same_cluster)\n",
        "    # Calculate the mean distance to the next cluster (b)\n",
        "    b = mean_distance(p, next_cluster)\n",
        "    # Return the silhouette coefficient\n",
        "    return (b - a) / max(a, b)\n",
        "\n",
        "# Calculate the silhouette coefficient for each sample\n",
        "coefficients = [silhouette_coefficient(p, l, X_exp, labels) for p, l in zip(X_exp, labels)]\n",
        "\n",
        "# Print the coefficients\n",
        "print(coefficients)\n",
        "\n",
        "# Calculate the silhouette score as the mean of the coefficients\n",
        "score = sum(coefficients) / len(coefficients)\n",
        "\n",
        "# Print the score\n",
        "print(f'silhouette score: {score:.4f}')"
      ],
      "metadata": {
        "id": "qDMgPe48uta6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We could also get similar results using `silhouette_score`."
      ],
      "metadata": {
        "id": "endAP82ZEE8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Calculate the Silhouette Coefficient for the clustering result\n",
        "silhouette_coefficient = silhouette_score(X_exp, labels)\n",
        "\n",
        "# Output the Silhouette Coefficient\n",
        "print(f\"Silhouette Coefficient for the clustering result: {silhouette_coefficient:.4f}\")"
      ],
      "metadata": {
        "id": "ag34R58Cw9I3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "daO5gpjjEJnW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Advantages of the Silhouette Coefficient\n",
        "\n",
        "- **Defined Range**: The Silhouette Coefficient scores range from -1 to +1. A score of -1 denotes poor clustering, whereas +1 indicates highly compact clusters.\n",
        "- **Indication of Overlap**: Scores near 0 indicate overlapping clusters, which means that data points might not be clearly categorized.\n",
        "- **Measure of Separation and Density**: A higher score reflects that clusters are well-separated and densely packed, which is consistent with the standard definition of a cluster.\n",
        "\n",
        "The Silhouette Coefficient quantitatively gauges the quality of clustering outcomes, capturing the essence of what constitutes effective clustering.\n",
        "\n",
        "### Drawbacks of the Silhouette Coefficient\n",
        "\n",
        "- **Preference for Convexity**: The Silhouette Coefficient inherently assumes that clusters are spherical or ellipsoidal in shape, which aligns with the concept of convex clusters. This assumption can lead to a bias in the scoring system, where it may favor clusters that fit this geometric shape over others. For instance, in scenarios where the natural grouping of data is non-convex, such as crescent or irregularly shaped clusters, the Silhouette Coefficient might not accurately reflect the true clustering structure. This is especially evident in comparison to density-based clustering algorithms like DBSCAN, which are designed to detect and handle arbitrarily shaped clusters. DBSCAN, for example, defines clusters based on high-density areas separated by low-density areas, without any assumption of their shape.\n",
        "\n",
        "As a result, when using the Silhouette Coefficient as a measure of clustering quality, one must be cautious if the data is expected to contain non-convex clusters. In such cases, the metric might misleadingly suggest suboptimal clustering due to its geometric bias, despite the clusters being meaningful according to the data's distribution and the clustering algorithm's criteria.\n",
        "\n",
        "<!-- ### Drawbacks of the Silhouette Coefficient\n",
        "\n",
        "- **Preference for Convexity**: The metric tends to assign higher scores to convex clusters compared to other types, such as those identified by density-based clustering algorithms like DBSCAN.\n",
        "\n",
        "While the Silhouette Coefficient is a valuable tool, it may not fully recognize the complexity of clusters that deviate from convex shapes. -->"
      ],
      "metadata": {
        "id": "5QM3Cdatuvp1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A Brief explanation of each hyperparameter for `silhouette_score`:\n",
        "\n",
        "- **X**: This is either the array of pairwise distances between samples or a feature array from which distances can be computed.\n",
        "\n",
        "- **Labels**: These are the predicted labels for each sample, indicating which cluster each sample belongs to.\n",
        "\n",
        "- **Metric**: Specifies the metric used to calculate distance between instances in a feature array. If `X` is a distance array, use `metric=\"precomputed\"`.\n",
        "- **Sample_size**: Determines the size of the sample used when computing the Silhouette Coefficient. If `None`, no sampling is used.\n",
        "\n",
        "- **Random_state**: Sets the seed for random number generation when selecting a subset of samples, ensuring reproducibility across multiple function calls.\n",
        "\n",
        "- ****kwargs**: Any additional parameters are passed directly to the distance function, which can vary depending on the metric used.\n",
        "\n",
        "The function computes the mean Silhouette Coefficient, which ranges from -1 (incorrect clustering) to +1 (highly dense clustering), with values near 0 indicating overlapping clusters.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html"
      ],
      "metadata": {
        "id": "5PD5TJaUiWtN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='Blue'><b>Example:</b></font>"
      ],
      "metadata": {
        "id": "_wbumxYZENxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Calculate the Silhouette Coefficient for the clustering result\n",
        "silhouette_coefficient = silhouette_score(X, kmeans.labels_)\n",
        "\n",
        "# Output the Silhouette Coefficient\n",
        "print(f\"Silhouette Coefficient for the clustering result: {silhouette_coefficient:.4f}\")"
      ],
      "metadata": {
        "id": "3vFte3C3i0wM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Silhouette Coefficient is a measure of how well-separated the clusters are in a clustering result. It takes values between -1 and 1, where:\n",
        "\n",
        "- A high positive value close to 1 indicates that the clusters are well-separated and the assignment of data points to clusters is appropriate.\n",
        "- A value around 0 suggests overlapping clusters or clusters that are too close to each other.\n",
        "- A negative value indicates that data points might have been assigned to the wrong cluster.\n",
        "\n",
        "In our specific example, a Silhouette Coefficient of 0.5577 suggests a reasonably good separation between clusters. It indicates that the clustering result is meaningful and the data points within each cluster are relatively close to each other compared to points in other clusters. However, the interpretation of the coefficient may depend on the specific characteristics of our data and the problem we are addressing. Generally, a higher Silhouette Coefficient is desired for better cluster quality."
      ],
      "metadata": {
        "id": "E0QJ8SoijJoU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calinski-Harabasz Index\n",
        "\n",
        "The **Calinski-Harabasz Index**, also known as the **Variance Ratio Criterion**, is a valuable metric for evaluating the quality of clusters in a dataset. It is an **internal evaluation method**, meaning that it does not use any external labels or information, but only the data itself.\n",
        "\n",
        "### Mathematical Explanation\n",
        "\n",
        "The index measures the ratio of the **between-cluster variance** to the **within-cluster variance**, normalized by their respective degrees of freedom. The formula consists of the following terms:\n",
        "\n",
        "1. **Calinski-Harabasz Index (CH)**:\n",
        "   - The CH value indicates how well the clusters are separated and compact.\n",
        "   - Higher CH values imply better clustering quality.\n",
        "\n",
        "2. **Between-Cluster Sum of Squares (BCSS)**:\n",
        "   - BCSS quantifies the separation between clusters.\n",
        "   - It is computed as the weighted sum of squared distances between each cluster centroid and the overall data centroid.\n",
        "   - The weight of each cluster is the number of points in that cluster.\n",
        "\n",
        "Given a data set of n points: $\\{x_1, ..., x_n\\}$, and the assignment of these points to k clusters: $\\{C_1, ..., C_k\\}$, the cluster centroid (mean) of $C_i$ is $c_i$, and the overall data centroid (mean) is $c$. The BCSS is defined as:\n",
        "$$\n",
        "BCSS=\\sum _{i=1}^{k}n_{i}\\|\\mathbf {c} _{i}-\\mathbf {c} \\|^{2}\n",
        "$$\n",
        "\n",
        "\n",
        "3. **Within-Cluster Sum of Squares (WCSS)**:\n",
        "   - WCSS measures the compactness of the clusters.\n",
        "   - It is the sum of squared distances between data points and their corresponding cluster centroids.\n",
        "\n",
        "Given a data set of n points: $\\{x_1, ..., x_n\\}$, and the assignment of these points to k clusters: $\\{C_1, ..., C_k\\}$, the cluster centroid (mean) of $C_i$ is $c_i$. The WCSS is defined as:\n",
        "$$\n",
        "WCSS=\\sum _{i=1}^{k}\\sum _{\\mathbf {x} \\in C_{i}}\\|\\mathbf {x} -\\mathbf {c} _{i}\\|^{2}\n",
        "$$\n",
        "\n",
        "\n",
        "4. **Degrees of Freedom**:\n",
        "   - For BCSS: $k - 1$ degrees of freedom (where $k$ is the number of clusters).\n",
        "     - This is because fixing the centroids of $k - 1$ clusters also determines the position of the $k^{th}$ centroid.\n",
        "   - For WCSS: $n - k$ degrees of freedom (where $n$ is the total number of data points).\n",
        "     - This is because fixing the centroid of each cluster reduces the degrees of freedom by one.\n",
        "\n",
        "### Formula:\n",
        "The Calinski-Harabasz Index ($CH$) is calculated as:\n",
        "\n",
        "$$\n",
        "CH = \\frac{BCSS / (k - 1)}{WCSS / (n - k)}\n",
        "$$\n",
        "\n",
        "### Practical Implications\n",
        "\n",
        "The Calinski-Harabasz Index can help us make informed decisions about clustering algorithms and their parameters. By maximizing the CH value, we aim for clusters that are well-separated and cohesive. However, this index is not the only criterion to consider, and we should also use other evaluation metrics and domain-specific knowledge when choosing the best clustering solution.\n",
        "\n",
        "* [Calinski, T., & Harabasz, J. (1974). A dendrite method for cluster analysis. Communications in Statistics-theory and Methods, 3(1), 1-27.](https://www.tandfonline.com/doi/abs/10.1080/03610927408827101)\n",
        "* [Liu, Y., Li, Z., Xiong, H., Gao, X., & Wu, J. (2010, December). Understanding of internal clustering validation measures. In 2010 IEEE international conference on data mining (pp. 911-916). IEEE.](https://ieeexplore.ieee.org/document/5694060)"
      ],
      "metadata": {
        "id": "qMHHDew4lxc_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- ### Calinski-Harabasz Index\n",
        "\n",
        "- **Definition**: The index measures clustering quality as the ratio of the sum of between-cluster dispersion to within-cluster dispersion, with dispersion being the sum of squared distances.\n",
        "- **Implication**: A higher index value indicates more distinct clustering.\n",
        "\n",
        "To calculate the score in Python, use: `sklearn.metrics.calinski_harabasz_score(X, labels)`. -->\n",
        "\n",
        "### Advantages of Calinski-Harabasz Index\n",
        "\n",
        "- **Efficiency**: The computation of the score is quick.\n",
        "- **Cluster Clarity**: The index is higher for clusters that are dense and well-separated, aligning with the classic definition of a cluster.\n",
        "\n",
        "### Drawbacks of Calinski-Harabasz Index\n",
        "\n",
        "- **Shape Bias**: The index typically scores convex clusters higher than non-convex clusters, such as those identified by density-based methods like DBSCAN."
      ],
      "metadata": {
        "id": "j7f7vRV8yaIj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='Blue'><b>Example:</b></font>"
      ],
      "metadata": {
        "id": "DJ2512g8SAh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import calinski_harabasz_score\n",
        "\n",
        "# Calculate the Calinski-Harabasz Index for the clustering result\n",
        "calinski_harabasz_index = calinski_harabasz_score(X, kmeans.labels_)\n",
        "\n",
        "# Output the Calinski-Harabasz Index\n",
        "print(f\"Calinski-Harabasz Index for the clustering result: {calinski_harabasz_index:.4f}\")\n"
      ],
      "metadata": {
        "id": "7DXX_MulmoMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Calinski-Harabasz Index is a metric used to evaluate the quality of a clustering result based on the ratio of between-cluster variance to within-cluster variance. The index is higher when clusters are well-separated and compact, indicating a better clustering solution.\n",
        "\n",
        "In our specific example, a Calinski-Harabasz Index of 332.5601 suggests a relatively good clustering result. A higher index indicates better-defined and more compact clusters. It implies that the variance between clusters is higher compared to the variance within clusters, reinforcing the quality of our clustering solution. However, the interpretation of the index may vary depending on the specific characteristics of our data and the problem at hand. Generally, a higher Calinski-Harabasz Index is indicative of a better clustering outcome."
      ],
      "metadata": {
        "id": "QvFYwQubybkk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Davies-Bouldin Index\n",
        "\n",
        "The **Davies-Bouldin Index (DBI)** is a way to measure how good a clustering is in machine learning. It uses the data itself to evaluate the clustering, without needing any external reference.\n",
        "\n",
        "Here is how it works mathematically:\n",
        "\n",
        "- **Intra-cluster Distance $S_i$**: The average distance from each point in cluster $i$ to the center of that cluster.\n",
        "- **Inter-cluster Distance $M_{ij}$**: The distance between the centers of two clusters $i$ and $j$.\n",
        "\n",
        "To calculate the DBI for a set of clusters, we do this:\n",
        "\n",
        "1. For each cluster $i$, find the cluster $ j$ that has the highest ratio $R_{ij} = \\dfrac{S_i + S_j}{M_{ij}}$, where $i \\neq j$.\n",
        "2. The DBI is the average of these highest ratios for all clusters:\n",
        "$$DB = \\frac{1}{k} \\sum_{i=1}^k \\max_{i \\neq j} R_{ij}$$\n",
        "\n",
        "A **lower DBI** means the clusters are well-separated and compact, which is what we want. The DBI is useful because it works for any number and shape of clusters.\n",
        "\n",
        "**Davies-Bouldin Index - Pros**\n",
        "\n",
        "- It is easy and fast to compute, unlike the Silhouette score which takes more time and resources.\n",
        "- It only depends on the data and the distances, without needing any extra information or assumptions.\n",
        "\n",
        "**Davies-Bouldin Index - Cons**\n",
        "\n",
        "- It prefers round clusters over other shapes, which may not match the real data, especially when using methods like DBSCAN that can find different shapes of clusters.\n",
        "- It only uses the Euclidean distance, which may not reflect how similar or different the data points are."
      ],
      "metadata": {
        "id": "Ww_490hiqFzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import davies_bouldin_score\n",
        "\n",
        "# Calculate the Davies-Bouldin Score for the clustering result\n",
        "davies_bouldin_result = davies_bouldin_score(X, kmeans.labels_)\n",
        "\n",
        "# Output the Davies-Bouldin Score\n",
        "print(f\"Davies-Bouldin Score for the clustering result: {davies_bouldin_result:.4f}\")"
      ],
      "metadata": {
        "id": "e0WuSxEFdu9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The clustering result has a Davies-Bouldin Score of 0.6643. This metric shows how good the clustering is, with lower scores meaning better and more distinct clusters. The score of 0.6643 indicates a fair level of cluster density and separation, which quantifies how well the clustering algorithm works on the data."
      ],
      "metadata": {
        "id": "TIzm8UZGgMI4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to Estimate the Number of Clusters?\n",
        "\n",
        "- **Elbow Method**: This method plots the inertia (or within-cluster sum-of-squares) against the number of clusters and looks for the point where the curve \"bends\" or changes direction sharply. This point indicates the optimal number of clusters, as adding more clusters will not significantly improve the clustering quality.\n",
        "\n",
        "- **Validation Metrics**: These metrics, such as the Silhouette Coefficient, the Calinski-Harabasz Index, or the Davies-Bouldin Index, measure how well the data points are grouped into clusters. We can plot these metrics against the number of clusters and choose the value that maximizes or minimizes the metric, depending on its definition. Ideally, the optimal value should match the elbow point."
      ],
      "metadata": {
        "id": "k6-W9NpmzkrK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The elbow method works as follows:\n",
        "\n",
        "1. Run the k-means algorithm for different values of k from a minimum to a maximum value.\n",
        "2. Calculate the WCSS for each k value.\n",
        "3. Plot the WCSS values against the number of clusters.\n",
        "4. Look for the point where the curve bends sharply, forming an \"elbow\". This is the optimal k value.\n"
      ],
      "metadata": {
        "id": "saI3tiN0Eh2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize an empty list to store the WCSS values\n",
        "wcss = []\n",
        "k_list = np.arange(2, 15)\n",
        "\n",
        "for k in k_list:\n",
        "    # Create a k-means model with k clusters\n",
        "    kmeans = KMeans(n_clusters=k, random_state=0, n_init=10).fit(X)\n",
        "    # Fit the model to the data\n",
        "    kmeans.fit(X)\n",
        "    # Append the WCSS value to the list\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize Figure and Axes\n",
        "fig, ax = plt.subplots(figsize = (8, 5))\n",
        "\n",
        "# Plotting WCSS values against the number of clusters\n",
        "my_plt_setting = dict(color='#c90076', linewidth=2, markersize=8,\n",
        "                      label='Data Points', markerfacecolor='#e8eafc',\n",
        "                      markeredgecolor = '#2236e1', markeredgewidth=3)\n",
        "ax.plot(k_list, wcss, 'o-', **my_plt_setting)\n",
        "\n",
        "# Set Labels and Title\n",
        "ax.set(xlabel = 'Number of clusters', ylabel = 'WCSS', title = r'WCSS for various values of $k$')\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "wcss_df = pd.DataFrame({'k':k_list, 'WCSS': wcss})\n",
        "display(wcss_df.style.hide(axis='index'))"
      ],
      "metadata": {
        "id": "gamylji0-aDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* k = 4 seems to be a good choice, as the WCSS decreases slowly after that point. However, the elbow method is not always clear and may depend on the shape and distribution of the data.\n",
        "\n",
        "\n",
        "* Another method that can be used to evaluate the optimal k is the **silhouette method**. This method measures how well each point fits into its assigned cluster, based on the average distance to other points in the same cluster (cohesion) and the average distance to points in the nearest cluster (separation). The silhouette score ranges from -1 to 1, where a higher value indicates a better fit."
      ],
      "metadata": {
        "id": "KbNI5LCME-Kf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import numpy as np\n",
        "\n",
        "sil_scores = []\n",
        "\n",
        "# Loop over different values of k\n",
        "for k in k_list:\n",
        "    # Create a k-means model and fit it to the data\n",
        "    kmeans = KMeans(n_clusters=k, random_state=0, n_init=10)\n",
        "    kmeans.fit(X)\n",
        "\n",
        "    # Calculate the silhouette score and append it to the list\n",
        "    sil_score = silhouette_score(X, kmeans.labels_)\n",
        "    sil_scores.append(sil_score)\n",
        "\n",
        "# Initialize Figure and Axes\n",
        "fig, ax = plt.subplots(figsize = (8, 5))\n",
        "\n",
        "# Plotting WCSS values against the number of clusters\n",
        "ax.plot(k_list, sil_scores, 'o-', **my_plt_setting)\n",
        "\n",
        "# Set Labels and Title\n",
        "ax.set(xlabel = 'Number of clusters', ylabel = 'Silhouette Score', title = r'Silhouette scores for various values of $k$')\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "sil_df = pd.DataFrame({'k':k_list, 'Silhouette Score': sil_scores})\n",
        "display(sil_df.style.hide(axis='index'))"
      ],
      "metadata": {
        "id": "uwJVkB1CSNMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we pick $k$ based on the highest Silhouette Score, then it would be 2."
      ],
      "metadata": {
        "id": "EacWZ6HDYcBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import calinski_harabasz_score\n",
        "import numpy as np\n",
        "\n",
        "ch_scores = []\n",
        "\n",
        "# Loop over different values of k\n",
        "for k in k_list:\n",
        "    # Create a k-means model and fit it to the data\n",
        "    kmeans = KMeans(n_clusters=k, random_state=0, n_init=10)\n",
        "    kmeans.fit(X)\n",
        "\n",
        "    # Calculate the silhouette score and append it to the list\n",
        "    ch_score = calinski_harabasz_score(X, kmeans.labels_)\n",
        "    ch_scores.append(ch_score)\n",
        "\n",
        "# Initialize Figure and Axes\n",
        "fig, ax = plt.subplots(figsize = (8, 5))\n",
        "\n",
        "# Plotting WCSS values against the number of clusters\n",
        "ax.plot(k_list, ch_scores, 'o-', **my_plt_setting)\n",
        "\n",
        "# Set Labels and Title\n",
        "ax.set(xlabel = 'Number of clusters', ylabel = 'Calinski Harabasz Score',\n",
        "       title = r'Calinski Harabasz scores for various values of $k$')\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "ch_df = pd.DataFrame({'k':k_list, 'Calinski Harabasz Score': ch_scores})\n",
        "display(ch_df.style.hide(axis='index'))"
      ],
      "metadata": {
        "id": "kC0KSLa4TrSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we pick $k$ based on the highest Calinski Harabasz Score, then it would be 3."
      ],
      "metadata": {
        "id": "e8TGiua2ZEtZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The kelbow_visualizer is a function from the yellowbrick library that helps you find the optimal number of clusters for K-means clustering. It implements the elbow method, which plots the score of the clustering model (such as distortion, silhouette, or calinski_harabasz) against different values of k, the number of clusters. The optimal k is usually the one where the score curve bends or forms an \"elbow\". You can read more about the kelbow_visualizer [here](https://www.scikit-yb.org/en/latest/api/cluster/elbow.html)."
      ],
      "metadata": {
        "id": "Qf5PRn2daUz_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also could this using [kelbow_visualizer](https://www.scikit-yb.org/en/latest/api/cluster/elbow.html) from `Yellowbrick` liberary."
      ],
      "metadata": {
        "id": "HY_F7AB9M5bL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import yellowbrick\n",
        "# yellowbrick.__version__\n",
        "\n",
        "plt.rcParams.update({'axes.grid.axis': 'y', \"grid.color\": \"0.7\"})\n",
        "from yellowbrick.cluster.elbow import kelbow_visualizer\n",
        "\n",
        "# distortion\n",
        "ke=kelbow_visualizer(KMeans(random_state= 0, n_init=10), X, k = k_list,\n",
        "                     metric='distortion', timings=False)\n",
        "\n",
        "# silhouette\n",
        "ke=kelbow_visualizer(KMeans(random_state= 0, n_init=10), X, k = k_list,\n",
        "                     metric='silhouette', timings=False)\n",
        "# calinski_harabasz\n",
        "ke=kelbow_visualizer(KMeans(random_state= 0, n_init=10), X, k = k_list,\n",
        "                     metric='calinski_harabasz', timings=False)"
      ],
      "metadata": {
        "id": "SmJ-abnHMqr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use the above suggested values for k."
      ],
      "metadata": {
        "id": "q1uZWXI17GqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "from matplotlib.colors import ListedColormap\n",
        "import pandas as pd\n",
        "\n",
        "def plot_kmeans_clusters(colors, k, X, ax):\n",
        "    \"\"\"\n",
        "    Generate a KMeans clustering plot with specified colors and number of clusters.\n",
        "\n",
        "    Parameters:\n",
        "    - colors (list): List of hex color codes for each cluster.\n",
        "    - k (int): Number of clusters.\n",
        "    - X (array-like): Input data for KMeans clustering.\n",
        "    - ax (matplotlib axes): Axes to plot on.\n",
        "\n",
        "    Returns:\n",
        "    - None: Displays the clustering plot on the provided axes.\n",
        "    \"\"\"\n",
        "\n",
        "    # Build the KMeans clustering model\n",
        "    kmeans = KMeans(n_clusters=k, random_state=0, n_init='auto')\n",
        "    kmeans.fit(X)\n",
        "\n",
        "    # Define a custom color map for the clusters\n",
        "    cmap = ListedColormap(colors)\n",
        "\n",
        "    # Scatter plot for the clustered data\n",
        "    scatter = ax.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap=cmap,\n",
        "                         edgecolor='k', linewidth=0.5, alpha=0.7, s=40)\n",
        "    ax.set_title(f'Clusters using KMeans (k = {k})', fontsize=14)\n",
        "\n",
        "    # Plot the cluster centers\n",
        "    centers = ax.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
        "                         c=range(k), marker='*', cmap=cmap, edgecolor='k',\n",
        "                         linewidth=1, alpha=1, s=200)\n",
        "\n",
        "    # Display cluster inertia\n",
        "    ax.text(0.02, 0.02, f'Inertia = {kmeans.inertia_:.4f}',\n",
        "            transform=ax.transAxes, fontsize=12, weight='bold',\n",
        "            bbox=dict(facecolor='Whitesmoke', alpha=0.7))\n",
        "\n",
        "    # Decision boundary display\n",
        "    DecisionBoundaryDisplay.from_estimator(kmeans, X, cmap=cmap,\n",
        "                                           ax=ax, response_method=\"predict\",\n",
        "                                           plot_method=\"pcolormesh\",\n",
        "                                           xlabel='Feature 1',\n",
        "                                           ylabel='Feature 2',\n",
        "                                           shading=\"auto\",\n",
        "                                           grid_resolution=300,\n",
        "                                           alpha=0.2)\n",
        "\n",
        "    # Set labels for axes\n",
        "    ax.set(xlabel='Feature 1', ylabel='Feature 2')\n",
        "\n",
        "# Define colors for clusters\n",
        "colors = [\"#f44336\", \"#4e9130\", \"#2986cc\", \"#FFF68F\", \"#c90076\"]\n",
        "\n",
        "# Create a 2x2 subplot grid\n",
        "fig, axs = plt.subplots(2, 2, figsize=(11, 10))\n",
        "axs = axs.ravel()\n",
        "# Plot original data in the top left subplot\n",
        "axs[0].scatter(df['Feature 1'], df['Feature 2'], c='gray', edgecolor='k',\n",
        "                   linewidth=1, alpha=0.7, s=40)\n",
        "axs[0].set_title('Original Data', fontsize=14)\n",
        "axs[0].set(xlabel='Feature 1', ylabel='Feature 2')\n",
        "\n",
        "# Plot clusters for k = 2 in the top right subplot\n",
        "plot_kmeans_clusters(colors[:2], 2, X, axs[1])\n",
        "\n",
        "# Plot clusters for k = 3 in the bottom left subplot\n",
        "plot_kmeans_clusters(colors[:3], 3, X, axs[2])\n",
        "\n",
        "# Plot clusters for k = 4 in the bottom right subplot\n",
        "plot_kmeans_clusters(colors[:4], 4, X, axs[3])\n",
        "\n",
        "for ax in axs:\n",
        "  ax.grid(False)\n",
        "# Adjust layout for better presentation\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "raqtmTb_Bv2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Drawbacks of inertia\n",
        "\n",
        "Inertia is a measure of how well a dataset is clustered by k-means. It is calculated by summing the squared distances of each point to its assigned centroid within each cluster. However, inertia has some limitations as a clustering evaluation metric:\n",
        "\n",
        "-   Inertia assumes that the clusters are convex and isotropic, which may not hold true for some datasets. For example, if the data has elongated or irregularly shaped clusters, inertia will tend to produce suboptimal results.\n",
        "\n",
        "-   Inertia is not normalized, meaning that it depends on the scale of the data and the number of clusters. Lower values of inertia are preferable, but they do not indicate how good the clustering is in an absolute sense. A zero inertia value means that each cluster only contains one point, which is usually not desirable.\n",
        "\n",
        "-   In very high-dimensional spaces, the Euclidean distance used by inertia becomes less meaningful, as all points tend to be far away from each other. This is known as the \"curse of dimensionality\". One way to mitigate this issue is to apply a dimensionality reduction technique, such as principal component analysis (PCA), before performing k-means clustering. This can also speed up the computation time and reduce the memory usage."
      ],
      "metadata": {
        "id": "8OsBjhVHxhAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "\n",
        "# Create a 2x2 subplot grid\n",
        "fig, axs = plt.subplots(1, 3, figsize=(13, 5))\n",
        "axs = axs.ravel()\n",
        "\n",
        "# Generate and plot varied circular data\n",
        "X_varied, _ = datasets.make_circles(random_state=0, n_samples=300, factor=0.5, noise=0.05)\n",
        "axs[0].scatter(X_varied[:, 0], X_varied[:, 1], c='#9fc5e8', edgecolor='#073763',\n",
        "               linewidth=1, alpha=0.7, s=40)\n",
        "\n",
        "# Generate and plot varied moon-shaped data\n",
        "X_varied, _ = datasets.make_moons(random_state=0, n_samples=300, noise=0.05)\n",
        "axs[1].scatter(X_varied[:, 0], X_varied[:, 1], c='#e06666', edgecolor='#990000',\n",
        "               linewidth=1, alpha=0.7, s=40)\n",
        "\n",
        "# Generate and plot varied blob-shaped data\n",
        "X_varied, _ = datasets.make_blobs(n_samples=500, random_state=0)\n",
        "X_varied = np.dot(X_varied, [[0.6, -0.6], [-0.4, 0.8]])\n",
        "axs[2].scatter(X_varied[:, 0], X_varied[:, 1], c='#c27ba0', edgecolor='#971f5e',\n",
        "               linewidth=1, alpha=0.7, s=40)\n",
        "\n",
        "# Set axis labels, aspect ratio, and grid for all subplots\n",
        "for ax in axs:\n",
        "    ax.set(xlabel='Feature 1', ylabel='Feature 2', aspect='equal')\n",
        "    ax.grid('both')\n",
        "\n",
        "# Set title for the entire figure\n",
        "fig.suptitle('Examples of Elongated and Irregularly Shaped Data', fontsize=14, weight='bold', x=0.53, y=0.8)\n",
        "\n",
        "# Ensure tight layout for better visual presentation\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "7Sht1fk3FL5m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}