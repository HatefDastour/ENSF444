{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# t-Distributed Stochastic Neighbor Embedding (t-SNE)"
      ],
      "metadata": {
        "id": "iHWELDlB1Xrr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding Manifolds in Machine Learning\n",
        "\n",
        "A manifold is essentially an underlying structure that is believed to be present within such data. To be more precise, a manifold is a space that is **locally Euclidean**. This means that when examining sufficiently small neighborhoods within the data, their properties mimic those of a flat, Euclidean space.\n",
        "\n",
        "Consider the analogy of the Earth's surface. On a local scale, it seems flat enough that two-dimensional maps can effectively represent different regions. Yet, in reality, the Earth is a three-dimensional sphere. In a similar vein, high-dimensional data can be visualized as resting on a curved surface inside a space with even more dimensions. The goal of **manifold learning** is to discover a lower-dimensional space that still faithfully conveys the critical structures and relationships inherent in the data. This process facilitates a more manageable and insightful analysis of complex datasets.\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/HatefDastour/ENSF444/602e4fc305507b4c1aa4e75e82894a99525be35d/Images/Earth3Dto2D.png\" alt=\"picture\" width=\"700\">\n",
        "<br>\n",
        "<b>Figure</b>: An example of reducing dimmension from 3D to 2D.\n",
        "</center>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xRVzHLNqIuXG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manifold Learning\n",
        "\n",
        "Manifold learning stands as a powerful unsupervised learning approach within machine learning, primarily utilized for dimensionality reduction. It operates on the premise that high-dimensional datasets often reside on a lower-dimensional manifold that is embedded within the higher-dimensional space. The objective of manifold learning is to reveal this lower-dimensional structure without relying on pre-established classifications.\n",
        "\n",
        "Some examples of Manifold Learning algorithms:\n",
        "\n",
        "- **Isomap**: Implemented as `sklearn.manifold.Isomap`, it is used for embedding high-dimensional data into a lower-dimensional space while preserving geodesic distances between points.\n",
        "\n",
        "- **Locally Linear Embedding (LLE)**: Available as `sklearn.manifold.LocallyLinearEmbedding`, this algorithm looks for a lower-dimensional projection of the data that preserves distances within local neighborhoods.\n",
        "\n",
        "- **t-Distributed Stochastic Neighbor Embedding (t-SNE)**: Provided as `sklearn.manifold.TSNE`, it is a tool to visualize high-dimensional data by converting similarities between data points to joint probabilities.\n",
        "\n",
        "- **Spectral Embedding**: Can be accessed via `sklearn.manifold.SpectralEmbedding`, using the spectrum of the graph Laplacian for dimensionality reduction.\n",
        "\n",
        "- **Multidimensional Scaling (MDS)**: This technique is implemented in `sklearn.manifold.MDS` and aims to find a low-dimensional representation of the data that preserves the distances between points as much as possible."
      ],
      "metadata": {
        "id": "FYExj-SxI7xq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A (Simplified) Overview of t-SNE\n",
        "\n",
        "t-Distributed Stochastic Neighbor Embedding (t-SNE) is a powerful method for visualizing complex, high-dimensional data in a space of fewer dimensions. It's particularly effective for exploring and analyzing data, as it retains the local structure and reveals patterns and clusters.\n",
        "\n",
        "1. **Calculating Pairwise Similarities**: t-SNE starts by determining how similar each data point is to every other point, typically using Euclidean distance as the measure. This similarity is expressed in terms of probability, with closer points having a higher probability of being 'neighbors'.\n",
        "\n",
        "2. **Dimensional Mapping**: The algorithm then seeks a new, lower-dimensional space where these probabilities are preserved as much as possible. It aims to keep similar points close together and dissimilar points apart, thus maintaining the data's inherent structure.\n",
        "\n",
        "3. **Utilizing Probability Distributions**: t-SNE employs probability distributions to represent similarities. In the high-dimensional space, it uses a Gaussian distribution, while in the lower-dimensional space, it opts for a t-distribution, which helps prevent different points from overlapping too much.\n",
        "\n",
        "4. **Optimizing Representation**: The essence of t-SNE lies in minimizing the difference between the two probability distributions, ensuring that the low-dimensional representation reflects the high-dimensional relationships accurately.\n",
        "\n",
        "This process allows t-SNE to effectively visualize high-dimensional data in a way that highlights the data's natural clusters and patterns."
      ],
      "metadata": {
        "id": "mlso8Ui614Wn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **t-SNE (t-distributed Stochastic Neighbor Embedding)** algorithm is a powerful tool for visualizing high-dimensional data in a lower-dimensional space. Let's break down the hyperparameters of [sklearn.manifold.TSNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html):\n",
        "\n",
        "1. **`n_components`**: This parameter specifies the dimension of the embedded space. By default, it is set to 2, meaning that t-SNE will create a 2D representation of the data.\n",
        "\n",
        "2. **`perplexity`**: Perplexity controls the balance between preserving local and global structures. It determines the number of nearest neighbors used in the algorithm. Larger datasets usually require a larger perplexity. A value between 5 and 50 is recommended. Different values can significantly affect the results.\n",
        "\n",
        "3. **`early_exaggeration`**: This parameter controls the spacing between natural clusters in the original space. Larger values increase the space between clusters in the embedded space.\n",
        "\n",
        "4. **`learning_rate`**: The learning rate influences how the data points move during optimization. It is usually in the range [10.0, 1000.0]. If too high, data points may form a 'ball'; if too low, points may compress into a dense cloud. The 'auto' option sets the learning rate based on the sample size.\n",
        "\n",
        "5. **`n_iter`**: The maximum number of iterations for optimization. The algorithm iteratively refines the embedding.\n",
        "\n",
        "6. **`n_iter_without_progress`**: The maximum number of iterations without progress. If the cost function doesn't improve, the optimization stops.\n",
        "\n",
        "7. **`min_grad_norm`**: A threshold for convergence. If the gradient norm is below this value, the optimization stops.\n",
        "\n",
        "8. **`metric`**: The distance metric used to compute pairwise distances between samples. By default, it's 'euclidean'.\n",
        "\n",
        "9. **`init`**: The initialization method for the embedding. 'pca' is commonly used for dense data.\n",
        "\n",
        "10. **`verbose`**: Controls the verbosity of the output during optimization.\n",
        "\n",
        "11. **`random_state`**: Seed for random number generation.\n",
        "\n",
        "12. **`method`**: The method used for optimization. 'barnes_hut' is efficient for large datasets.\n",
        "\n",
        "13. **`angle`**: The trade-off between speed and accuracy. Smaller values give more accurate results but take longer to compute.\n",
        "\n",
        "Remember that t-SNE is sensitive to hyperparameters, so experimentation and understanding the impact of each parameter are crucial for obtaining meaningful visualizations. If the number of features is very high, consider using another dimensionality reduction method (e.g., PCA) before applying t-SNE."
      ],
      "metadata": {
        "id": "IIplP8sKmH0U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='Blue'><b>Example:</b></font>"
      ],
      "metadata": {
        "id": "q1uZWXI17GqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('https://raw.githubusercontent.com/HatefDastour/ENSF444/main/Files/mystyle.mplstyle')\n",
        "from time import time\n",
        "\n",
        "def plot_tsne(perplexity):\n",
        "    # Load the digits dataset\n",
        "    digits = load_digits()\n",
        "\n",
        "    t0 = time()\n",
        "\n",
        "    # Create a TSNE instance\n",
        "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=0)  # Initialize t-SNE with 2 components\n",
        "\n",
        "    # Fit and transform the data with t-SNE to project from 64 to 2 dimensions\n",
        "    X_tsne = tsne.fit_transform(digits.data)\n",
        "\n",
        "    t1 = time()\n",
        "    t = t1 - t0\n",
        "    # Print shapes before and after t-SNE\n",
        "    print(\"Original data shape:\", digits.data.shape)\n",
        "    print(\"Projected data shape:\", X_tsne.shape)\n",
        "    print(f'Perplexity = {perplexity} in {t:.3f} seconds')\n",
        "\n",
        "    # Create a scatter plot using Seaborn\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(9.5, 9.5))\n",
        "    scatter = ax.scatter(X_tsne[:, 0], X_tsne[:, 1],\n",
        "                         c=digits.target, cmap='Spectral',\n",
        "                         edgecolor='k', linewidth=0.5,\n",
        "                         alpha=0.7, s=40)\n",
        "\n",
        "    # Set labels and title\n",
        "    ax.set(xlabel='Component 1', ylabel='Component 2',\n",
        "           title=f't-SNE Projection of Handwritten Digits (Perplexity = {perplexity})')\n",
        "\n",
        "    # Add legend inside on the top right\n",
        "    legend = ax.legend(*scatter.legend_elements(), title=\"Digits\",\n",
        "                       loc='upper right', fontsize=12)\n",
        "\n",
        "    # Display the plot with tight layout\n",
        "    plt.tight_layout()\n",
        "\n",
        "# Example usage:\n",
        "plot_tsne(30)"
      ],
      "metadata": {
        "id": "7d389rhK6CmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Role of Perplexity\n",
        "1. **Perplexity Definition**:\n",
        "   - Perplexity is a hyperparameter in t-SNE that influences the balance between local and global data structures during the embedding process.\n",
        "   - It approximates the number of effective nearest neighbors, guiding the density of the local manifold of each data point.\n",
        "   - Essentially, perplexity is a measure of the expected density around a point and impacts how t-SNE calculates probabilities for nearby points.\n",
        "\n",
        "2. **Impact on t-SNE Visualizations**:\n",
        "   - **With Higher Perplexity**:\n",
        "     - An increase in perplexity generally allows t-SNE to capture broader data trends, potentially enhancing the visibility of global structures.\n",
        "     - Clusters may appear more separated, aiding in the discernment of distinct data groupings.\n",
        "     - However, this can sometimes lead to an exaggeration of space between clusters, which might not accurately reflect the true data distances.\n",
        "     - High perplexity values can be particularly useful for datasets with intricate structures, like nested clusters.\n",
        "   - **With Lower Perplexity**:\n",
        "     - A lower perplexity value emphasizes the preservation of local data intricacies, often resulting in denser clusters.\n",
        "     - This can be beneficial for highlighting fine-grained patterns but may merge or obscure broader data relationships.\n",
        "     - Overlapping or densely packed clusters are common when the perplexity is set too low, which might complicate the interpretation of the embedding.\n",
        "\n",
        "3. **Selecting an Appropriate Perplexity**:\n",
        "   - The optimal perplexity setting is dependent on the dataset's characteristics and the specific insights you're seeking.\n",
        "   - While the typical range for perplexity lies between 5 and 50, the best value is often found through iterative experimentation.\n",
        "   - Adjusting perplexity in conjunction with other t-SNE parameters, such as the learning rate and the number of iterations, can further refine the resulting visualization.\n",
        "\n",
        "Remember, t-SNE is a non-linear dimensionality reduction technique, and its results can vary significantly with different hyperparameter settings. It's always recommended to try multiple perplexity values and compare the resulting embeddings to ensure the most informative representation of your data."
      ],
      "metadata": {
        "id": "U9r7I0md2Lv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_tsne(50)"
      ],
      "metadata": {
        "id": "dbIJpWfynudR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_tsne(70)"
      ],
      "metadata": {
        "id": "RQCqydVknvf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the given example, varying the perplexity parameter in t-SNE affects the visualization as follows:\n",
        "\n",
        "- **Perplexity 30**: This value is often ideal for maintaining a balance between local and global data structures. It typically results in well-defined clusters that capture both immediate similarities and broader patterns within the data. These clusters should be clearly distinguishable and proximate, accurately representing the natural groupings in the digits dataset.\n",
        "\n",
        "- **Perplexity 50**: At this level, t-SNE shifts its focus slightly more towards global data relationships. Clusters may appear more dispersed, which might lead to a minor reduction in the detail of local structures. However, this dispersion can be beneficial for separating clusters that were previously overlapping, thus enhancing the visualization of the dataset's overall structure.\n",
        "\n",
        "- **Perplexity 70**: A higher perplexity emphasizes the global layout of the data. Clusters tend to be more isolated, but there's a risk that the algorithm may overlook local intricacies. Consequently, the visualization might show clusters that are too spread out, potentially misrepresenting the actual proximities and densities found in the high-dimensional space.\n",
        "\n",
        "It's important to note that the impact of perplexity on t-SNE visualizations is not consistent across all datasets. While the `sklearn` digits dataset tends to produce reliable results, datasets with greater complexity or noise levels may exhibit more pronounced effects when perplexity values are altered."
      ],
      "metadata": {
        "id": "rfTYZPD6p_Du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_tsne_3d(perplexity = 30):\n",
        "    # Load the digits dataset\n",
        "    digits = load_digits()\n",
        "\n",
        "    t0 = time()\n",
        "\n",
        "    # Create a TSNE instance\n",
        "    tsne = TSNE(n_components=3, perplexity = perplexity, random_state=0)  # Initialize t-SNE with 3 components\n",
        "\n",
        "    # Fit and transform the data with t-SNE to project from 64 to 3 dimensions\n",
        "    X_tsne = tsne.fit_transform(digits.data)\n",
        "    t1 = time()\n",
        "    t = t1 - t0\n",
        "    # Print shapes before and after t-SNE\n",
        "    print(\"Original data shape:\", digits.data.shape)\n",
        "    print(\"Projected data shape:\", X_tsne.shape)\n",
        "    print(f'Perplexity = {perplexity} in {t:.3f} seconds')\n",
        "\n",
        "    # Create a 3D scatter plot\n",
        "    fig = plt.figure(figsize=(9, 9))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    scatter = ax.scatter(X_tsne[:, 0], X_tsne[:, 1], X_tsne[:, 2],\n",
        "                         c=digits.target, cmap='Spectral',\n",
        "                         edgecolor='k', linewidth=0.5,\n",
        "                         alpha=0.7, s=40)\n",
        "\n",
        "    # Set axis labels\n",
        "    ax.set_xlabel('Component 1')\n",
        "    ax.set_ylabel('Component 2')\n",
        "    ax.set_zlabel('Component 3')\n",
        "\n",
        "    # Set view angle\n",
        "    ax.view_init(elev=20., azim=30)\n",
        "\n",
        "    # Add legend inside on the top right\n",
        "    legend = ax.legend(*scatter.legend_elements(), title=\"Digits\",\n",
        "                       loc='right', fontsize=12)\n",
        "\n",
        "    # Display the plot with tight layout\n",
        "    plt.tight_layout()\n",
        "\n",
        "# Example usage:\n",
        "plot_tsne_3d(30)"
      ],
      "metadata": {
        "id": "d0190qAa7Yy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_tsne_3d(100)"
      ],
      "metadata": {
        "id": "T-vbtWNwrJmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above example:\n",
        "\n",
        "- **Perplexity 30**: This value is likely to give you a balance between local and global structures. The clusters for each digit should be distinct and may reflect the natural groupings within the dataset. The 3D visualization will likely show clear separation between different digit clusters, with each cluster representing one of the ten digits.\n",
        "\n",
        "- **Perplexity 100**: A higher perplexity value means that t-SNE considers a larger number of nearest neighbors when constructing the low-dimensional embeddings. This can sometimes lead to a more uniform spread of the data points and less distinct clusters. In your case, where the clusters appear less separated, it suggests that the global structure is being emphasized over the local nuances. This might cause different digit clusters to overlap or appear closer together, making it harder to distinguish between them."
      ],
      "metadata": {
        "id": "sv8dJZiCtmFD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example: Examining the Impact of Perplexity on t-SNE Visualizations"
      ],
      "metadata": {
        "id": "npFBMhPY8Lq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "from time import time\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "# Define a custom color map\n",
        "_cmap = ListedColormap([\"#f44336\", \"#295F27\", \"#2986cc\"])\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "species_names = [x.title() for x in iris.target_names]\n",
        "\n",
        "# Set up a 3 by 3 subplot grid\n",
        "fig, axes = plt.subplots(3, 3, figsize=(14, 14))\n",
        "fig.suptitle('t-SNE Visualization with Different Perplexity Values', fontsize=16)\n",
        "\n",
        "# Iterate through different perplexity values\n",
        "perplexities = np.arange(20, 101, 10)\n",
        "\n",
        "for i, perplexity in enumerate(perplexities):\n",
        "    # Apply t-SNE with the current perplexity value\n",
        "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=0)\n",
        "    t0 = time()\n",
        "    X_tsne = tsne.fit_transform(X)\n",
        "    t1 = time()\n",
        "    t = t1 - t0\n",
        "    # Plot the 2D representation\n",
        "    scatter = axes[i // 3, i % 3].scatter(X_tsne[:, 0], X_tsne[:, 1],\n",
        "                                          c=y, cmap=_cmap, edgecolor='k', alpha=0.8)\n",
        "    axes[i // 3, i % 3].set_title(f'Perplexity = {perplexity}\\n in {t:.3f} seconds')\n",
        "\n",
        "# Create a single legend for the entire figure\n",
        "leg = (scatter.legend_elements()[0], species_names)\n",
        "fig.legend(*leg, ncols=3, loc='lower center', fontsize=14, bbox_to_anchor=(0.5, -.03))\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "BR2avGxw-c-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "from time import time\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "# Define a custom color map\n",
        "_cmap = ListedColormap([\"#f44336\", \"#295F27\", \"#2986cc\"])\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "species_names = [x.title() for x in iris.target_names]\n",
        "\n",
        "# Set up a 3 by 3 subplot grid for 3D plots\n",
        "fig = plt.figure(figsize=(14, 14))\n",
        "fig.suptitle('t-SNE 3D Visualization with Different Perplexity Values', fontsize=16)\n",
        "\n",
        "# Iterate through different perplexity values\n",
        "perplexities = np.arange(20, 101, 10)\n",
        "\n",
        "for i, perplexity in enumerate(perplexities):\n",
        "    # Apply t-SNE with the current perplexity value and 3 components\n",
        "    tsne = TSNE(n_components=3, perplexity=perplexity, random_state=0)\n",
        "    t0 = time()\n",
        "    X_tsne = tsne.fit_transform(X)\n",
        "    t1 = time()\n",
        "    t = t1 - t0\n",
        "    # Create a 3D subplot\n",
        "    ax = fig.add_subplot(3, 3, i + 1, projection='3d')\n",
        "    # Plot the 3D representation\n",
        "    scatter = ax.scatter(X_tsne[:, 0], X_tsne[:, 1], X_tsne[:, 2],\n",
        "                         c=y, cmap=_cmap, edgecolor='k', alpha=0.8)\n",
        "    ax.set_title(f'Perplexity = {perplexity}\\n in {t:.3f} seconds')\n",
        "\n",
        "# Create a single legend for the entire figure\n",
        "leg = (scatter.legend_elements()[0], species_names)\n",
        "fig.legend(*leg, ncols=3, loc='lower center', fontsize=14, bbox_to_anchor=(0.5, -.03))\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "a559KW0HDxXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advantages and Disadvantages of t-SNE\n",
        "\n",
        "### Advantages of t-SNE:\n",
        "\n",
        "1. **Preserves Local Structure**: t-SNE excels at keeping the local structure of the data intact. This means that points that are neighbors in the high-dimensional space will remain neighbors in the lower-dimensional representation. This property is particularly beneficial for visualizing clusters and understanding the intrinsic structure of the data.\n",
        "\n",
        "2. **Captures Non-Linear Relationships**: Unlike linear dimensionality reduction methods like PCA, t-SNE can capture complex non-linear relationships between features. This makes it suitable for datasets where the relationship between variables is not linear.\n",
        "\n",
        "3. **Robust to Outliers**: t-SNE is less sensitive to outliers than many other dimensionality reduction techniques. Outliers are less likely to affect the overall structure of the data representation, making t-SNE a robust choice for visualizing data with noise.\n",
        "\n",
        "### Disadvantages of t-SNE:\n",
        "\n",
        "1. **Computationally Intensive**: t-SNE requires significant computational resources, especially for large datasets. This is because it involves calculating pairwise distances between all points, which can be time-consuming and resource-intensive.\n",
        "\n",
        "2. **Not Ideal for Pre-Processing**: t-SNE is not typically used for pre-processing data for predictive modeling. This is because it does not maintain the global structure of the data, which can be important for predictive tasks.\n",
        "\n",
        "3. **Sensitive to Initial Conditions**: The results of t-SNE can vary depending on the initial conditions and random seed used. This means that different runs of the algorithm can produce different visualizations, which can be a challenge when seeking consistent results."
      ],
      "metadata": {
        "id": "RtmXqksl3u3E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applications and Use Cases of t-SNE\n",
        "\n",
        "t-SNE (t-Distributed Stochastic Neighbor Embedding) is a versatile tool for dimensionality reduction and visualization, widely applied across various fields. Here's a refined look at its applications:\n",
        "\n",
        "- **Visualizing Clusters and Patterns**: t-SNE excels in representing high-dimensional data in 2D or 3D plots, making it easier to identify clusters and patterns. It's particularly effective in biological data analysis, where it's used to visualize gene expression patterns, cell types, and DNA sequences, as well as in computer vision for image data, and in natural language processing to discern word clusters.\n",
        "\n",
        "- **Exploring Data Relationships**: By maintaining the proximity of similar data points, t-SNE facilitates the exploration of relationships within the data. It reveals how data points are interconnected, their similarities and differences, and dependencies, which might not be apparent in linear projections. This probabilistic approach uncovers hidden structures, offering a dynamic way to interact with data through various visualization manipulations.\n",
        "\n",
        "- **Anomaly Detection and Complexity Analysis**: t-SNE's ability to handle non-linearities makes it adept at detecting anomalies and outliers that deviate from established patterns. It also aids in analyzing the complexity of data by illustrating the influence of various features or dimensions on the data's overall structure and their interactions.\n",
        "\n",
        "These use cases highlight t-SNE's capacity to provide deep insights into complex datasets, making it a valuable tool for data scientists and researchers."
      ],
      "metadata": {
        "id": "VtQ0212V5UaF"
      }
    }
  ]
}